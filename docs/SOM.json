[
  {
    "sentiment": 0.973,
    "humanLanguage": "en",
    "diffbotUri": "article|3|779171303",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.4.html",
    "html": "<h2><em>7.4</em> problem solving</h2>\n<p>In principle, we can use the <em>generate and test</em> method &mdash; that is, trial and error &mdash; to solve any problem whose solution we can recognize. But in practice, it can take too long for even the most powerful computer to test enough possible solutions. Merely assembling a simple house from a dozen wooden blocks would require searching through more possibilities than a child could try in a lifetime. Here is one way to improve upon blind trial-and-error search.</p>\n<p>The Progress Principle: Any process of exhaustive search can be greatly reduced if we possess some way to detect when <em>progress</em> has been made. Then we can trace a path toward a solution, just as a person can climb an unfamiliar hill in the dark &mdash; by feeling around, at every step, to find the direction of steepest ascent.</p>\n<p>Many easy problems can be solved this way, but for a hard problem, it may be almost as difficult to recognize <em>progress</em> as to solve the problem itself. Without a larger overview, that <em>hill climber</em> may get stuck forever on some minor peak and never find the mountaintop. There is no foolproof way to avoid this.</p>\n<p>Goals and Subgoal: The most powerful way we know for discovering how to solve a hard problem is to find a method that splits it into several simpler ones, each of which can be solved separately.</p>\n<p>Much research in the field called Artificial Intelligence has been concerned with finding methods machines can use for splitting a problem into smaller subproblems and then, if necessary, dividing these into yet smaller ones. In the next few sections we'll see how this can be done by formulating our problems in terms of <em>goals.</em></p>\n<p>Using Knowledge: The most efficient way to solve a problem is to already know how to solve it. Then one can avoid search entirely.</p>\n<p>Accordingly, another branch of Artificial Intelligence research has sought to find ways to embody knowledge in machines. But this problem itself has several parts: we must discover how to acquire the knowledge we need, we must learn how to represent it, and, finally, we must develop processes that can exploit our knowledge effectively. To accomplish all that, our memories must represent, in preference to vast amounts of small details, only those relationships that may help us reach our goals. This research has led to many practical <em>knowledge-based</em> problem-solving systems. Some of these are often called <em>expert systems</em> because they're based on imitating the methods of particular human practitioners.</p>\n<p>A curious phenomenon emerged from this research. It often turned out easier to program machines to solve specialized problems that educated people considered hard &mdash; such as playing chess or proving theorems about logic or geometry &mdash; than to make machines do things that most people considered easy &mdash; such as building toy houses with children's blocks. This is why I've emphasized so many <em>easy</em> problems in this book.</p>",
    "text": "7.4 problem solving\nIn principle, we can use the generate and test method \u2014 that is, trial and error \u2014 to solve any problem whose solution we can recognize. But in practice, it can take too long for even the most powerful computer to test enough possible solutions. Merely assembling a simple house from a dozen wooden blocks would require searching through more possibilities than a child could try in a lifetime. Here is one way to improve upon blind trial-and-error search.\nThe Progress Principle: Any process of exhaustive search can be greatly reduced if we possess some way to detect when progress has been made. Then we can trace a path toward a solution, just as a person can climb an unfamiliar hill in the dark \u2014 by feeling around, at every step, to find the direction of steepest ascent.\nMany easy problems can be solved this way, but for a hard problem, it may be almost as difficult to recognize progress as to solve the problem itself. Without a larger overview, that hill climber may get stuck forever on some minor peak and never find the mountaintop. There is no foolproof way to avoid this.\nGoals and Subgoal: The most powerful way we know for discovering how to solve a hard problem is to find a method that splits it into several simpler ones, each of which can be solved separately.\nMuch research in the field called Artificial Intelligence has been concerned with finding methods machines can use for splitting a problem into smaller subproblems and then, if necessary, dividing these into yet smaller ones. In the next few sections we'll see how this can be done by formulating our problems in terms of goals.\nUsing Knowledge: The most efficient way to solve a problem is to already know how to solve it. Then one can avoid search entirely.\nAccordingly, another branch of Artificial Intelligence research has sought to find ways to embody knowledge in machines. But this problem itself has several parts: we must discover how to acquire the knowledge we need, we must learn how to represent it, and, finally, we must develop processes that can exploit our knowledge effectively. To accomplish all that, our memories must represent, in preference to vast amounts of small details, only those relationships that may help us reach our goals. This research has led to many practical knowledge-based problem-solving systems. Some of these are often called expert systems because they're based on imitating the methods of particular human practitioners.\nA curious phenomenon emerged from this research. It often turned out easier to program machines to solve specialized problems that educated people considered hard \u2014 such as playing chess or proving theorems about logic or geometry \u2014 than to make machines do things that most people considered easy \u2014 such as building toy houses with children's blocks. This is why I've emphasized so many easy problems in this book.",
    "type": "article",
    "title": "7.4 problem solving",
    "tags": [
      {
        "score": 0.6452189683914185,
        "sentiment": 0,
        "count": 2,
        "label": "artificial intelligence",
        "uri": "https://diffbot.com/entity/X_lYDrjmAMlKKwXaDf958zg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 9402352058,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 41393783175,
    "gburl": "http://aurellem.org/society-of-mind/som-7.4.html-diffbotxyz1262140007",
    "lastCrawlTimeUTC": 1588765282,
    "timestamp": "Wed, 06 May 2020 11:41:22 GMT"
  },
  {
    "sentiment": 0.888,
    "images": [
      {
        "naturalHeight": 163,
        "width": 405,
        "diffbotUri": "image|3|1839391710",
        "url": "http://aurellem.org/society-of-mind/illus/ch11/11-2.png",
        "naturalWidth": 405,
        "primary": true,
        "height": 163
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1741332631",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.3.html",
    "html": "<p>The reason our skin can feel is because we're built with myriad nerves that run from every skin spot to the brain. In general, each pair of nearby places on the skin is wired to nearby places in the brain. This is because those nerves tend to run in bundles of parallel fibers &mdash; more or less like this:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch11/11-2.png\"/></figure>\n<p>Each sensory experience involves the activity of many different sensors. In general, the greater the extent to which two stimuli arouse the same sensors, the more nearly alike will be the partial mental states those stimuli produce &mdash; and the more similar those stimuli will <em>seem,</em> simply because they'll tend to lead to similar mental consequences.</p>\n<p>Other things being equal, the apparent similarity of two stimuli will depend on the extent to which they lead to similar activities in other agencies.</p>\n<p>The fact that the nerves from skin to brain tend to run in parallel bundles means that stimulating nearby spots of skin will usually lead to rather similar activities inside the brain. In the next section we'll see how this could enable an agency inside the brain to discover the spatial layout of the skin. For example, as you move a finger along your skin, new nerve endings are stimulated &mdash; and it is safe to assume that the new arrivals represent spots of skin along the advancing edge of your finger.</p>\n<p>Given enough such information, a suitably designed agency could assemble a sort of map to represent which spots are close together on the skin. Because there are many irregularities in the nerve-bundle pathways from skin to brain, the agencies that construct those maps must be able to <em>tidy things up.</em> For example, the mapping agency must learn to correct the sort of crossing-over shown in the diagram. But that is only the beginning of the task. For a child, learning about the spatial world beyond the skin is a journey that stretches over many years.</p>",
    "text": "The reason our skin can feel is because we're built with myriad nerves that run from every skin spot to the brain. In general, each pair of nearby places on the skin is wired to nearby places in the brain. This is because those nerves tend to run in bundles of parallel fibers \u2014 more or less like this:\nEach sensory experience involves the activity of many different sensors. In general, the greater the extent to which two stimuli arouse the same sensors, the more nearly alike will be the partial mental states those stimuli produce \u2014 and the more similar those stimuli will seem, simply because they'll tend to lead to similar mental consequences.\nOther things being equal, the apparent similarity of two stimuli will depend on the extent to which they lead to similar activities in other agencies.\nThe fact that the nerves from skin to brain tend to run in parallel bundles means that stimulating nearby spots of skin will usually lead to rather similar activities inside the brain. In the next section we'll see how this could enable an agency inside the brain to discover the spatial layout of the skin. For example, as you move a finger along your skin, new nerve endings are stimulated \u2014 and it is safe to assume that the new arrivals represent spots of skin along the advancing edge of your finger.\nGiven enough such information, a suitably designed agency could assemble a sort of map to represent which spots are close together on the skin. Because there are many irregularities in the nerve-bundle pathways from skin to brain, the agencies that construct those maps must be able to tidy things up. For example, the mapping agency must learn to correct the sort of crossing-over shown in the diagram. But that is only the beginning of the task. For a child, learning about the spatial world beyond the skin is a journey that stretches over many years.",
    "type": "article",
    "title": "11.3 nearnesses",
    "tags": [
      {
        "score": 0.5790338516235352,
        "sentiment": -0.432,
        "count": 2,
        "label": "product bundling",
        "uri": "https://diffbot.com/entity/X0ToYWK0AMqOVhxgOYkP6qw",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      }
    ],
    "docId": 36576625033,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 191756485052,
    "gburl": "http://aurellem.org/society-of-mind/som-11.3.html-diffbotxyz1616596692",
    "lastCrawlTimeUTC": 1588765273,
    "timestamp": "Wed, 06 May 2020 11:41:13 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1289773689",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-28.7.html",
    "html": "<p>Suppose I had once borrowed your boat and, secretly, replaced each board with a similar but different one. Then, later, when I brought it back, did I return your boat to you? What kind of question is that? It's really not about boats at all, but about what people mean by <em>same.</em> For <em>same</em> is never absolute but always a matter of degree. If I had merely changed one plank, we'd all agree that it's still your boat &mdash; but after all its parts are changed, we're not so sure of its identity. In any case, we do not doubt that the second boat will behave in much the same way &mdash; to the extent that all those substituted boards are suitably equivalent.</p>\n<p>What has this to do with brains? Well, now suppose that we could replace each of your brain cells with a specially designed computer chip that performs the same functions, and then suppose that we interconnect these devices just as your brain cells are connected. If we put it in the same environment, this new machine would reproduce the same processes as those within your brain. Would that new machine be the same as you? Again, the real question is not what we mean by <em>you,</em> but what we mean by <em>same.</em> There isn't any reason to doubt that the substitute machine would think and feel the same kinds of thoughts and feelings that you do &mdash; since it embodies all the same processes and memories. Indeed, it would surely be disposed to declare, with all your own intensity, that it is you. Would that machine be right or wrong? As far as I can see, this, too, is merely a matter of words. A mind is a way in which each state gives rise to the state that follows it. If that new machine had a suitable body and were placed in a similar environment, its sequence of thoughts would be essentially the same as yours &mdash; since its mental states would be equivalent to yours.</p>\n<p>Modifying or replacing the physical parts of a brain will not affect the mind it embodies, unless this alters the successions of states in that brain.</p>\n<p>You might object to this idea about duplicating minds on the grounds that it would never be practical to duplicate enough details. You could argue the same about that borrowed boat: no matter how carefully a carpenter were to copy every board, there would always remain some differences. This plank would be a little too stiff, that one would be a little too weak, and no two of them would bend in exactly the same way. The copied boat would never be precisely the same &mdash; even though you might need a microscope to see the differences. For similar reasons, it would be impractical to duplicate, with absolute fidelity, all the interactions in a brain. For example, our brain cells are all immersed in a liquid that conducts electricity, which means that every cell has at least a small effect on every other cell. If we tried to imitate your brain with a network of computer chips, many of those tiny interactions would be left out.</p>\n<p>Can you then boast that your duplicated brain-machine would not have the same mind as yours because its computer chips don't work exactly like the brain cells they purport to replace? No; you'd get more than you bargained for if you argued that the new machine was not the same as you, merely because of microscopic differences. Consider that as you age, you're never the same as a moment ago. If such small differences matter that much, this would prove that you yourself are not the same as you.</p>",
    "text": "Suppose I had once borrowed your boat and, secretly, replaced each board with a similar but different one. Then, later, when I brought it back, did I return your boat to you? What kind of question is that? It's really not about boats at all, but about what people mean by same. For same is never absolute but always a matter of degree. If I had merely changed one plank, we'd all agree that it's still your boat \u2014 but after all its parts are changed, we're not so sure of its identity. In any case, we do not doubt that the second boat will behave in much the same way \u2014 to the extent that all those substituted boards are suitably equivalent.\nWhat has this to do with brains? Well, now suppose that we could replace each of your brain cells with a specially designed computer chip that performs the same functions, and then suppose that we interconnect these devices just as your brain cells are connected. If we put it in the same environment, this new machine would reproduce the same processes as those within your brain. Would that new machine be the same as you? Again, the real question is not what we mean by you, but what we mean by same. There isn't any reason to doubt that the substitute machine would think and feel the same kinds of thoughts and feelings that you do \u2014 since it embodies all the same processes and memories. Indeed, it would surely be disposed to declare, with all your own intensity, that it is you. Would that machine be right or wrong? As far as I can see, this, too, is merely a matter of words. A mind is a way in which each state gives rise to the state that follows it. If that new machine had a suitable body and were placed in a similar environment, its sequence of thoughts would be essentially the same as yours \u2014 since its mental states would be equivalent to yours.\nModifying or replacing the physical parts of a brain will not affect the mind it embodies, unless this alters the successions of states in that brain.\nYou might object to this idea about duplicating minds on the grounds that it would never be practical to duplicate enough details. You could argue the same about that borrowed boat: no matter how carefully a carpenter were to copy every board, there would always remain some differences. This plank would be a little too stiff, that one would be a little too weak, and no two of them would bend in exactly the same way. The copied boat would never be precisely the same \u2014 even though you might need a microscope to see the differences. For similar reasons, it would be impractical to duplicate, with absolute fidelity, all the interactions in a brain. For example, our brain cells are all immersed in a liquid that conducts electricity, which means that every cell has at least a small effect on every other cell. If we tried to imitate your brain with a network of computer chips, many of those tiny interactions would be left out.\nCan you then boast that your duplicated brain-machine would not have the same mind as yours because its computer chips don't work exactly like the brain cells they purport to replace? No; you'd get more than you bargained for if you argued that the new machine was not the same as you, merely because of microscopic differences. Consider that as you age, you're never the same as a moment ago. If such small differences matter that much, this would prove that you yourself are not the same as you.",
    "type": "article",
    "title": "28.7 individual identities",
    "docId": 66373321101,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 164740284858,
    "gburl": "http://aurellem.org/society-of-mind/som-28.7.html-diffbotxyz301855071",
    "lastCrawlTimeUTC": 1588765205,
    "timestamp": "Wed, 06 May 2020 11:40:05 GMT"
  },
  {
    "sentiment": -0.27,
    "humanLanguage": "en",
    "diffbotUri": "article|3|86614906",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-29.4.html",
    "html": "<p>We often describe the things we like as <em>elevated,</em> <em>lofty,</em> or <em>heavenly.</em> Why do we see such things in terms of altitude in space? We often speak of time itself in spatial terms, as though the future were <em>ahead</em> of us while the past remains behind. We think of problems as <em>obstacles</em> to go around and turn to using diagrams to represent things that don't have shapes at all. What enables us to turn so many skills to so many other purposes? These tendencies reflect the systematic <em>cross-realm correspondences</em> embodied in our families of polynemes and paranomes.</p>\n<p>At each instant, several realms may be engaged in active processing. Each has separate processes but must compete for control of the ascending nemes that lead into the language- agency. Which polyneme will play the role of Origin in the next sentence-frame? Will it be Mary's physical arm or hand, or Mary's social role as party guest? It sometimes seems</p>\n<p>as though the language-agency can focus on only one realm at a time.</p>\n<p>This could be one reason why language scientists find it hard to classify the roles words play in sentence-frames. No sooner does a language-agency assign some polynemes and isonomes to a phrase than various mind divisions proceed to alter how they're used inside each different realm. Every shift of control from one realm to another affects which particular nemes will be next to influence the language-agency. This causes moment-to-moment changes in the apparent meaning of a phrase.</p>\n<p>At one moment, control over language may reside in the realm of thought that is working most successfully; at the next moment, it may be the one experiencing the most difficulty. Each shift in attention affects how the various expressions will be interpreted, and this in turn can affect which realm will next take center stage.</p>\n<p>For example, the sentence <em>Mary gives Jack the kite</em> might start by arousing a listener's concern with Mary's social role as party guest. That would cause the pronomes of a social-frame to represent Mary's obligation to bring a present. But then the listener's possession realm might become concerned with Mary's ownership of that gift or with how she got control of it. This shift from social to possessional concern could then affect the processing of future sentences. For example, it will influence whether a phrase like <em>Jack's kite</em> is interpreted to refer to the kite that Jack happens to be holding or to a different kite that Jack happens to own.</p>\n<p>Every mental realm accumulates its own abilities but also discovers, from time to time, how to exploit the skills of other realms. Thus the mind as a whole can learn to exploit the frames developed in the realm of space both for representing events in time and for thinking about social relationships. Perhaps our chaining skills are the best example of this; no matter which realm or realms they originate in, we eventually learn to apply them to any collection of entities, events, or ideas (in any realm whatever) that we can arrange into sequences. Then chains assume their myriad forms, such as spatial order, psychological causality, or social dominance.</p>",
    "text": "We often describe the things we like as elevated, lofty, or heavenly. Why do we see such things in terms of altitude in space? We often speak of time itself in spatial terms, as though the future were ahead of us while the past remains behind. We think of problems as obstacles to go around and turn to using diagrams to represent things that don't have shapes at all. What enables us to turn so many skills to so many other purposes? These tendencies reflect the systematic cross-realm correspondences embodied in our families of polynemes and paranomes.\nAt each instant, several realms may be engaged in active processing. Each has separate processes but must compete for control of the ascending nemes that lead into the language- agency. Which polyneme will play the role of Origin in the next sentence-frame? Will it be Mary's physical arm or hand, or Mary's social role as party guest? It sometimes seems\nas though the language-agency can focus on only one realm at a time.\nThis could be one reason why language scientists find it hard to classify the roles words play in sentence-frames. No sooner does a language-agency assign some polynemes and isonomes to a phrase than various mind divisions proceed to alter how they're used inside each different realm. Every shift of control from one realm to another affects which particular nemes will be next to influence the language-agency. This causes moment-to-moment changes in the apparent meaning of a phrase.\nAt one moment, control over language may reside in the realm of thought that is working most successfully; at the next moment, it may be the one experiencing the most difficulty. Each shift in attention affects how the various expressions will be interpreted, and this in turn can affect which realm will next take center stage.\nFor example, the sentence Mary gives Jack the kite might start by arousing a listener's concern with Mary's social role as party guest. That would cause the pronomes of a social-frame to represent Mary's obligation to bring a present. But then the listener's possession realm might become concerned with Mary's ownership of that gift or with how she got control of it. This shift from social to possessional concern could then affect the processing of future sentences. For example, it will influence whether a phrase like Jack's kite is interpreted to refer to the kite that Jack happens to be holding or to a different kite that Jack happens to own.\nEvery mental realm accumulates its own abilities but also discovers, from time to time, how to exploit the skills of other realms. Thus the mind as a whole can learn to exploit the frames developed in the realm of space both for representing events in time and for thinking about social relationships. Perhaps our chaining skills are the best example of this; no matter which realm or realms they originate in, we eventually learn to apply them to any collection of entities, events, or ideas (in any realm whatever) that we can arrange into sequences. Then chains assume their myriad forms, such as spatial order, psychological causality, or social dominance.",
    "type": "article",
    "title": "29.4 cross-realm correspondences",
    "tags": [
      {
        "score": 0.7363821864128113,
        "sentiment": 0.471,
        "count": 6,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5973631143569946,
        "sentiment": 0,
        "count": 3,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      }
    ],
    "docId": 74327474604,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 125612212627,
    "gburl": "http://aurellem.org/society-of-mind/som-29.4.html-diffbotxyz691152824",
    "lastCrawlTimeUTC": 1588765224,
    "timestamp": "Wed, 06 May 2020 11:40:24 GMT"
  },
  {
    "sentiment": 0.982,
    "images": [
      {
        "naturalHeight": 129,
        "width": 215,
        "diffbotUri": "image|3|-1759967805",
        "url": "http://aurellem.org/society-of-mind/illus/ch12/12-7.png",
        "naturalWidth": 215,
        "primary": true,
        "height": 129
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|169723219",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.4.html",
    "html": "<p>Suppose an adult watched our child and said, <em>I see you've built an arch.</em> What might the child think this means? To learn new words or new ideas, one must make connections to other structures in the mind. <em>I see you've built an arch</em> should make the child connect the word <em>arch</em> to agencies embodying descriptions of both the Block-Arch and the Hand-Change phenomena &mdash; since those are what is on the child's mind.</p>\n<p>But one can't learn what something means merely by tying things to names. Each word-idea must also be invested with some causes, actions, purposes, and explanations. Consider all the things a word like <em>arch</em> must mean to any real child who understands how arches work and how they're made, and all the ways one can use them! A real child will have noticed, too, that arches are like variants of many other things experienced before, like <em>bridge without a road,</em> <em>wall with door,</em> <em>tablelike,</em> or <em>shaped like an upside-down U.</em> We can use such similarities to help find other things to serve our purposes: to think of an arch as a passage, hole, or tunnel could help someone concerned with a transportation problem; describing an arch as <em>top held up by sides</em> could help a person get to something out of reach. Which kind of description serves us best? That depends upon our purposes.</p>\n<p>Among our most powerful ways of thinking are those that let us bring together things we've learned in different contexts. But how can one think in two different ways at once? By building, somewhere inside the mind, some arches of a different kind:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch12/12-7.png\"/></figure>\n<p>Is that a foolish metaphor &mdash; to talk of building bridges between places in the mind? I'm sure it's not an accident that we so often frame our thoughts in terms of familiar spatial forms. Much of how we think in later life is based on what we learn in early life about the world of space.</p>",
    "text": "Suppose an adult watched our child and said, I see you've built an arch. What might the child think this means? To learn new words or new ideas, one must make connections to other structures in the mind. I see you've built an arch should make the child connect the word arch to agencies embodying descriptions of both the Block-Arch and the Hand-Change phenomena \u2014 since those are what is on the child's mind.\nBut one can't learn what something means merely by tying things to names. Each word-idea must also be invested with some causes, actions, purposes, and explanations. Consider all the things a word like arch must mean to any real child who understands how arches work and how they're made, and all the ways one can use them! A real child will have noticed, too, that arches are like variants of many other things experienced before, like bridge without a road, wall with door, tablelike, or shaped like an upside-down U. We can use such similarities to help find other things to serve our purposes: to think of an arch as a passage, hole, or tunnel could help someone concerned with a transportation problem; describing an arch as top held up by sides could help a person get to something out of reach. Which kind of description serves us best? That depends upon our purposes.\nAmong our most powerful ways of thinking are those that let us bring together things we've learned in different contexts. But how can one think in two different ways at once? By building, somewhere inside the mind, some arches of a different kind:\nIs that a foolish metaphor \u2014 to talk of building bridges between places in the mind? I'm sure it's not an accident that we so often frame our thoughts in terms of familiar spatial forms. Much of how we think in later life is based on what we learn in early life about the world of space.",
    "type": "article",
    "title": "12.4 structure and function",
    "tags": [
      {
        "score": 0.559165358543396,
        "sentiment": 0.489,
        "count": 6,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.536348819732666,
        "sentiment": 0,
        "count": 0,
        "label": "Arch Linux",
        "uri": "https://diffbot.com/entity/X9LbSq79NN5a1lH4Jz54H-Q",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software",
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Sales",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 95838978467,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 84180763031,
    "gburl": "http://aurellem.org/society-of-mind/som-12.4.html-diffbotxyz2060804247",
    "lastCrawlTimeUTC": 1588765131,
    "timestamp": "Wed, 06 May 2020 11:38:51 GMT"
  },
  {
    "sentiment": -0.945,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1579078278",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-24.5.html",
    "html": "<p>Even when you were very young, if someone had told you that most Snarks are green &mdash; and, also, that every Boojum is a Snark &mdash; you would have been able to conclude that most Boojums are green. What could have led you to that conclusion? Presumably, you answer questions about the properties of Boojums by attaching your polyneme for Snark to whichever of your memory-units currently represents a Boojum. Accordingly, you assume that the color property of a Boojum is green by using your usual way of recalling the properties of things you know &mdash; activating their polynemes to set your various agencies into the corresponding states. In other words, we do this kind of reasoning by manipulating our memories to replace particular things by typical things. I mention all this because it is often assumed that adults are better than children are at what is often called abstract or logical reasoning. This idea is unfair both to adult and child because logical thinking is so much simpler &mdash; and less effective &mdash; than common-sense thinking. Actually, what appears to be a matter of <em>logic</em> is usually not logical at all and frequently turns out to be wrong. In the case above, you would have been wrong because Boojums are albino Snarks.</p>\n<p>The situation is different when you happen to know more about a particular example. For example, suppose you first had learned that penguins cannot fly and then learned that penguins are a kind of bird. When you discover that, should you replace all of your penguin properties with those of your <em>generic</em> bird? Clearly not, since then you'd lose your hard-earned penguin facts. To deal with this effectively, children must develop complex skills, not merely to replace one representation with another, but to compare two representations and then move around inside them, making different changes at different levels. These intricate skills involve the use of isonomes that control the level-band of the activities inside our agencies.</p>\n<p>In any case, to reason well, our memory-control agencies must learn to <em>move</em> our memories around as though those memories were building-blocks. Conceivably, those agencies have to learn such skills before we can learn to build with blocks in the outside world of object-things. Unfortunately, we know very little about how such processes work. Indeed, we're virtually unaware that they even exist, because these kinds of <em>commonsense</em> inferences and assumptions come to mind without the slightest conscious effort or activity. Perhaps this unawareness is a consequence of the speed with which those skills employ the very same short-term memory-units that might otherwise be used to record those agents' own recent activities.</p>",
    "text": "Even when you were very young, if someone had told you that most Snarks are green \u2014 and, also, that every Boojum is a Snark \u2014 you would have been able to conclude that most Boojums are green. What could have led you to that conclusion? Presumably, you answer questions about the properties of Boojums by attaching your polyneme for Snark to whichever of your memory-units currently represents a Boojum. Accordingly, you assume that the color property of a Boojum is green by using your usual way of recalling the properties of things you know \u2014 activating their polynemes to set your various agencies into the corresponding states. In other words, we do this kind of reasoning by manipulating our memories to replace particular things by typical things. I mention all this because it is often assumed that adults are better than children are at what is often called abstract or logical reasoning. This idea is unfair both to adult and child because logical thinking is so much simpler \u2014 and less effective \u2014 than common-sense thinking. Actually, what appears to be a matter of logic is usually not logical at all and frequently turns out to be wrong. In the case above, you would have been wrong because Boojums are albino Snarks.\nThe situation is different when you happen to know more about a particular example. For example, suppose you first had learned that penguins cannot fly and then learned that penguins are a kind of bird. When you discover that, should you replace all of your penguin properties with those of your generic bird? Clearly not, since then you'd lose your hard-earned penguin facts. To deal with this effectively, children must develop complex skills, not merely to replace one representation with another, but to compare two representations and then move around inside them, making different changes at different levels. These intricate skills involve the use of isonomes that control the level-band of the activities inside our agencies.\nIn any case, to reason well, our memory-control agencies must learn to move our memories around as though those memories were building-blocks. Conceivably, those agencies have to learn such skills before we can learn to build with blocks in the outside world of object-things. Unfortunately, we know very little about how such processes work. Indeed, we're virtually unaware that they even exist, because these kinds of commonsense inferences and assumptions come to mind without the slightest conscious effort or activity. Perhaps this unawareness is a consequence of the speed with which those skills employ the very same short-term memory-units that might otherwise be used to record those agents' own recent activities.",
    "type": "article",
    "title": "24.5 nonverbal reasoning",
    "tags": [
      {
        "score": 0.7250996828079224,
        "sentiment": 0,
        "count": 3,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      },
      {
        "score": 0.6119866967201233,
        "sentiment": 0,
        "count": 3,
        "label": "Boojum",
        "uri": "https://diffbot.com/entity/CYlWl3pFCNBidFLTbyTcJIA",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      },
      {
        "score": 0.6012743711471558,
        "sentiment": 0,
        "count": 2,
        "label": "Snark",
        "uri": "https://diffbot.com/entity/XX4JS98dDNgy1VLqpkqqPpQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      },
      {
        "score": 0.6004685163497925,
        "sentiment": 0,
        "count": 1,
        "label": "Fly Me to the Moon",
        "uri": "https://diffbot.com/entity/XRZrQ-OahMnaptNVys6VVhw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 146576703878,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 206235599262,
    "gburl": "http://aurellem.org/society-of-mind/som-24.5.html-diffbotxyz4119827958",
    "lastCrawlTimeUTC": 1588765183,
    "timestamp": "Wed, 06 May 2020 11:39:43 GMT"
  },
  {
    "sentiment": 0.637,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1782215649",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-3.3.html",
    "html": "<blockquote> <b>bu&bull;reauc&prime;ra&bull;cy</b> <i>n.</i> the administration of government through departments and subdivisions managed by sets of officials following an inflexible routine. &mdash;Webster's Unabridged Dictionary </blockquote>\n<p>As an agent, Builder does no physical work but merely turns on Begin, Add, and End. Similarly, Add just orders Find, Put, and Get to do their jobs. Then these divide into agents like Move and Grasp. It seems that it will never stop &mdash; this breaking-down to smaller things.</p>\n<p>Eventually, it all must end with agents that do actual work, but there are many steps before we get to all the little muscle-motor agents that actually move the arms and hands and finger joints. Thus Builder is like a high-level executive, far removed from those subordinates who actually produce the final product.</p>\n<p>Does this mean that Builder's administrative work is unimportant? Not at all. Those lower-level agents need to be controlled. It's much the same in human affairs. When any enterprise becomes too complex and large for one person to do, we construct organizations in which certain agents are concerned, not with the final result, but only with what some other agents do. Designing any society, be it human or mechanical, involves decisions like these:</p>\n<p>Which agents choose which others to do what jobs? Who will decide which jobs are done at all? Who decides what efforts to expend? How will conflicts be settled?</p>\n<p>How much of ordinary human thought has Builder' s character? The Builder we described is not much like a human supervisor. It doesn't decide which agents to assign to which jobs, because that has already been arranged. It doesn't plan its future work but simply carries out fixed steps until End says the job is done. Nor has it any repertoire of ways to deal with unexpected accidents.</p>\n<p>Because our little mental agents are so limited, we should not try to extend very far the analogy between them and human supervisors and workers. Furthermore, as we'll shortly see, the relations between mental agents are not always strictly hierarchical. And in any case, such roles are always relative. To Builder, Add is a subordinate, but to Find, Add is a boss. As for yourself, it all depends on how you live. Which sorts of thoughts concern you most &mdash; the orders you are made to take or those you're being forced to give?</p>",
    "text": "bu\u2022reauc\u2032ra\u2022cy n. the administration of government through departments and subdivisions managed by sets of officials following an inflexible routine. \u2014Webster's Unabridged Dictionary\nAs an agent, Builder does no physical work but merely turns on Begin, Add, and End. Similarly, Add just orders Find, Put, and Get to do their jobs. Then these divide into agents like Move and Grasp. It seems that it will never stop \u2014 this breaking-down to smaller things.\nEventually, it all must end with agents that do actual work, but there are many steps before we get to all the little muscle-motor agents that actually move the arms and hands and finger joints. Thus Builder is like a high-level executive, far removed from those subordinates who actually produce the final product.\nDoes this mean that Builder's administrative work is unimportant? Not at all. Those lower-level agents need to be controlled. It's much the same in human affairs. When any enterprise becomes too complex and large for one person to do, we construct organizations in which certain agents are concerned, not with the final result, but only with what some other agents do. Designing any society, be it human or mechanical, involves decisions like these:\nWhich agents choose which others to do what jobs? Who will decide which jobs are done at all? Who decides what efforts to expend? How will conflicts be settled?\nHow much of ordinary human thought has Builder' s character? The Builder we described is not much like a human supervisor. It doesn't decide which agents to assign to which jobs, because that has already been arranged. It doesn't plan its future work but simply carries out fixed steps until End says the job is done. Nor has it any repertoire of ways to deal with unexpected accidents.\nBecause our little mental agents are so limited, we should not try to extend very far the analogy between them and human supervisors and workers. Furthermore, as we'll shortly see, the relations between mental agents are not always strictly hierarchical. And in any case, such roles are always relative. To Builder, Add is a subordinate, but to Find, Add is a boss. As for yourself, it all depends on how you live. Which sorts of thoughts concern you most \u2014 the orders you are made to take or those you're being forced to give?",
    "type": "article",
    "title": "3.3 hierarchies",
    "tags": [
      {
        "score": 0.8032744526863098,
        "sentiment": 0.429,
        "count": 1,
        "label": "Webster's Dictionary",
        "uri": "https://diffbot.com/entity/XlIprARBHP8OHeVdOXY7bFA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.6036543250083923,
        "sentiment": 0.172,
        "count": 9,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 203328061840,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 156680094112,
    "gburl": "http://aurellem.org/society-of-mind/som-3.3.html-diffbotxyz2776976766",
    "lastCrawlTimeUTC": 1588765246,
    "timestamp": "Wed, 06 May 2020 11:40:46 GMT"
  },
  {
    "sentiment": 0.452,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1636551536",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-16.3.html",
    "html": "<p>Suppose you had to build an artificial animal. First you'd make a list of everything you want your animal to do. Then you'd ask your engineers to find a way to meet each need.</p>\n<p>This diagram depicts a separate agency for each of several <em>basic needs.</em> Let's call them <em>proto-specialists.</em> Each has a separate mini-mind to do its job and is equipped with special sensors and effectors designed to suit its specific needs. For example, the proto-specialist for Thirst might have a set of parts like these:</p>\n<p>It would not usually be practical to make an animal that way. With all those separate specialists, we'd end up with a dozen different sets of heads and hands and feet. Not only would it cost too much to carry and feed all those organs; they'd also get in one another's way! Despite that inconvenience, there actually do exist some animals that work this way and thus can do many things at once. Genetically, the swarms of social ants and bees are really multibodied individuals whose different organs move around freely. But most animals economize by having all their proto-specialists share common sets of organs for their interactions with the outer world.</p>\n<p>Another kind of economy comes from allowing the proto-specialists to share what they learn. Whether you seek warmth, safety, nutrition, or companionship &mdash; eventually you'll have to be able to recognize and act in order to acquire the objects you need. So even though their initial goals are entirely different, all those different proto-specialists will end up needing to solve the same sorts of <em>subproblems</em> &mdash; such as finding ways around obstacles and deciding how to conserve limited resources. Whenever we try to solve problems of increasing complexity, whatever particular techniques we already know become correspondingly less adequate, and it becomes more important to be able to acquire new kinds of knowledge and skills. In the end, most of the mechanisms we need for any highly ambitious goal can be shared with most of our other goals.</p>\n<p>When a dog runs, it moves its legs. When a sea urchin runs, it is moved by its legs. &mdash; JAKOB VON UEXKLL</p>",
    "text": "Suppose you had to build an artificial animal. First you'd make a list of everything you want your animal to do. Then you'd ask your engineers to find a way to meet each need.\nThis diagram depicts a separate agency for each of several basic needs. Let's call them proto-specialists. Each has a separate mini-mind to do its job and is equipped with special sensors and effectors designed to suit its specific needs. For example, the proto-specialist for Thirst might have a set of parts like these:\nIt would not usually be practical to make an animal that way. With all those separate specialists, we'd end up with a dozen different sets of heads and hands and feet. Not only would it cost too much to carry and feed all those organs; they'd also get in one another's way! Despite that inconvenience, there actually do exist some animals that work this way and thus can do many things at once. Genetically, the swarms of social ants and bees are really multibodied individuals whose different organs move around freely. But most animals economize by having all their proto-specialists share common sets of organs for their interactions with the outer world.\nAnother kind of economy comes from allowing the proto-specialists to share what they learn. Whether you seek warmth, safety, nutrition, or companionship \u2014 eventually you'll have to be able to recognize and act in order to acquire the objects you need. So even though their initial goals are entirely different, all those different proto-specialists will end up needing to solve the same sorts of subproblems \u2014 such as finding ways around obstacles and deciding how to conserve limited resources. Whenever we try to solve problems of increasing complexity, whatever particular techniques we already know become correspondingly less adequate, and it becomes more important to be able to acquire new kinds of knowledge and skills. In the end, most of the mechanisms we need for any highly ambitious goal can be shared with most of our other goals.\nWhen a dog runs, it moves its legs. When a sea urchin runs, it is moved by its legs. \u2014 JAKOB VON UEXKLL",
    "type": "article",
    "title": "16.3 mental proto-specialists",
    "tags": [
      {
        "score": 0.5721720457077026,
        "sentiment": 0,
        "count": 0,
        "label": "medical specialty",
        "uri": "https://diffbot.com/entity/XlzwYE_VkMFaZ37ZqRbn2DQ"
      },
      {
        "score": 0.5306705236434937,
        "sentiment": 0,
        "count": 1,
        "label": "Thirst",
        "uri": "https://diffbot.com/entity/X0lItTEtCNHaCPw6VMDZDeg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      }
    ],
    "docId": 205178323377,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 87775068560,
    "gburl": "http://aurellem.org/society-of-mind/som-16.3.html-diffbotxyz368826828",
    "lastCrawlTimeUTC": 1588765153,
    "timestamp": "Wed, 06 May 2020 11:39:13 GMT"
  },
  {
    "sentiment": 0.24,
    "images": [
      {
        "naturalHeight": 163,
        "width": 355,
        "diffbotUri": "image|3|1051387083",
        "url": "http://aurellem.org/society-of-mind/illus/ch13/13-12.png",
        "naturalWidth": 355,
        "primary": true,
        "height": 163
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-73980062",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-13.5.html",
    "html": "<p>What will our portrait-drawing child try next? Some children keep working to improve their person pictures. But most of them go on to put their newfound skills to work at drawing more ambitious scenes in which two or more picture-people interact. This involves wonderful problems about how to depict social interactions and relationships &mdash; and these more ambitious projects lead the child away from being concerned with making the pictures of the individual more elaborate and realistic. When this happens, the parent may feel disappointed at what seems to be a lack of progress. But we should try to appreciate the changing character of our children's ambitions and recognize that their new problems may be even more challenging.</p>\n<p>This doesn't mean that drawing learning stops. Even as those children cease to make their person pictures more elaborate, the speed at which they draw them keeps increasing, and with seemingly less effort. How and why does this happen? In everyday life, we take it for granted that <em>practice makes perfect,</em> and that repeating and rehearsing a skill will, somehow, automatically cause it to become faster and more dependable. But when you come to think of it, this really is quite curious. You might expect, instead, that the more you learned, the slower you would get &mdash; from having more knowledge from which to choose! How does practice speed things up?</p>\n<p>Perhaps, when we practice skills we can already perform, we engage a special kind of learning, in the course of which the original performance process is replaced or <em>bridged-across</em> by new and simpler processes. The <em>program</em> to the left below shows the many steps our novice portrait drawer had to take in order to draw each childish body-face. The <em>script</em> to the right shows only those steps that actually produce the lines of the drawing; this script has only half as many steps.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch13/13-12.png\"/></figure>\n<p>The people we call <em>experts</em> seem to exercise their special skills with scarcely any thought at all &mdash; as though they were simply reading preassembled scripts. Perhaps when we <em>practice</em> to improve our skills, we're mainly building simpler scripts that don't engage so many agencies. This lets us do old things with much less <em>thought</em> and gives us more time to think of other things. The less the child has to think of where to put each arm and leg, the more time remains to represent what that picture-person is actually doing.</p>",
    "text": "What will our portrait-drawing child try next? Some children keep working to improve their person pictures. But most of them go on to put their newfound skills to work at drawing more ambitious scenes in which two or more picture-people interact. This involves wonderful problems about how to depict social interactions and relationships \u2014 and these more ambitious projects lead the child away from being concerned with making the pictures of the individual more elaborate and realistic. When this happens, the parent may feel disappointed at what seems to be a lack of progress. But we should try to appreciate the changing character of our children's ambitions and recognize that their new problems may be even more challenging.\nThis doesn't mean that drawing learning stops. Even as those children cease to make their person pictures more elaborate, the speed at which they draw them keeps increasing, and with seemingly less effort. How and why does this happen? In everyday life, we take it for granted that practice makes perfect, and that repeating and rehearsing a skill will, somehow, automatically cause it to become faster and more dependable. But when you come to think of it, this really is quite curious. You might expect, instead, that the more you learned, the slower you would get \u2014 from having more knowledge from which to choose! How does practice speed things up?\nPerhaps, when we practice skills we can already perform, we engage a special kind of learning, in the course of which the original performance process is replaced or bridged-across by new and simpler processes. The program to the left below shows the many steps our novice portrait drawer had to take in order to draw each childish body-face. The script to the right shows only those steps that actually produce the lines of the drawing; this script has only half as many steps.\nThe people we call experts seem to exercise their special skills with scarcely any thought at all \u2014 as though they were simply reading preassembled scripts. Perhaps when we practice to improve our skills, we're mainly building simpler scripts that don't engage so many agencies. This lets us do old things with much less thought and gives us more time to think of other things. The less the child has to think of where to put each arm and leg, the more time remains to represent what that picture-person is actually doing.",
    "type": "article",
    "title": "13.5 learning a script",
    "tags": [
      {
        "score": 0.6403053998947144,
        "sentiment": 0.615,
        "count": 2,
        "label": "screenplay",
        "uri": "https://diffbot.com/entity/X8x_vrN0xMLW135RLD8gi7A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.609623372554779,
        "sentiment": 0,
        "count": 1,
        "label": "portrait",
        "uri": "https://diffbot.com/entity/XKHPpJy85Md60ehNIa77pig",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5463880300521851,
        "sentiment": 0.444,
        "count": 5,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 41621815710,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 131550396819,
    "gburl": "http://aurellem.org/society-of-mind/som-13.5.html-diffbotxyz3096821021",
    "lastCrawlTimeUTC": 1588765046,
    "timestamp": "Wed, 06 May 2020 11:37:26 GMT"
  },
  {
    "sentiment": -0.732,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1567614441",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.1.html",
    "html": "<p>What's wrong with the old arguments that lead us to believe that if machines could ever think at all, they'd have to think with perfect logic? We're told that by their nature, all machines must work according to rules. We're also told that they can only do exactly what they're told to do. Besides that, we also hear that machines can only handle quantities and therefore cannot deal with qualities or anything like analogies.</p>\n<p>Most such arguments are based upon a mistake that is like confusing an agent with an agency. When we design and build a machine, we know a good deal about how it works. When our design is based on neat, logical principles, we are likely to make the mistake of expecting the machine to behave in a similarly neat and logical fashion. But that confuses what the machine does inside itself &mdash; that is, how it <em>works</em> &mdash; with our expectations of how it will appear to behave in the outer world. Being able to explain in logical terms how a machine's parts work does not automatically enable us to explain its subsequent activities in simple, logical terms. Edgar Allan Poe once argued that a certain chess-playing <em>machine</em> had to be fraudulent because it did not always win. If it were really a machine, he argued, it would be perfectly logical &mdash; and therefore could never make any mistakes! What is the fallacy in this? Simply that there is nothing to prevent us from using logical language to describe illogical reasoning. To a certain extent it's true that machines can do only what they are designed to do. But this does not preclude us, when once we know how thinking works, from designing machines that think.</p>\n<p>When do we actually use logic in real life? We use it to simplify and summarize our thoughts. We use it to explain arguments to other people and to persuade them that those arguments are right. We use it to reformulate our own ideas. But I doubt that we often use logic actually to solve problems or to <em>get</em> new ideas. Instead, we formulate our arguments and conclusions in logical terms after we have constructed or discovered them in other ways; only then do we use verbal and other kinds of formal reasoning to <em>clean things up,</em> to separate the essential parts from the spaghettilike tangles of thoughts and ideas in which they first occurred.</p>\n<p>To see why logic must come afterward, recall the idea of solving problems by using the generate and test method. In any such process, logic can be only a fraction of the reasoning; it can serve as a test to keep us from coming to invalid conclusions, but it cannot tell us which ideas to generate, or which processes and memories to use. Logic no more explains how we think than grammar explains how we speak; both can tell us whether our sentences are properly formed, but they cannot tell us which sentences to make. Without an intimate connection between our knowledge and our intentions, logic leads to madness, not intelligence. A logical system without a goal will merely generate an endless host of pointless truths like these:</p>\n<p>A implies A. P or not P. A implies A or A or A. If 4 is 5, then pigs can fly.</p>",
    "text": "What's wrong with the old arguments that lead us to believe that if machines could ever think at all, they'd have to think with perfect logic? We're told that by their nature, all machines must work according to rules. We're also told that they can only do exactly what they're told to do. Besides that, we also hear that machines can only handle quantities and therefore cannot deal with qualities or anything like analogies.\nMost such arguments are based upon a mistake that is like confusing an agent with an agency. When we design and build a machine, we know a good deal about how it works. When our design is based on neat, logical principles, we are likely to make the mistake of expecting the machine to behave in a similarly neat and logical fashion. But that confuses what the machine does inside itself \u2014 that is, how it works \u2014 with our expectations of how it will appear to behave in the outer world. Being able to explain in logical terms how a machine's parts work does not automatically enable us to explain its subsequent activities in simple, logical terms. Edgar Allan Poe once argued that a certain chess-playing machine had to be fraudulent because it did not always win. If it were really a machine, he argued, it would be perfectly logical \u2014 and therefore could never make any mistakes! What is the fallacy in this? Simply that there is nothing to prevent us from using logical language to describe illogical reasoning. To a certain extent it's true that machines can do only what they are designed to do. But this does not preclude us, when once we know how thinking works, from designing machines that think.\nWhen do we actually use logic in real life? We use it to simplify and summarize our thoughts. We use it to explain arguments to other people and to persuade them that those arguments are right. We use it to reformulate our own ideas. But I doubt that we often use logic actually to solve problems or to get new ideas. Instead, we formulate our arguments and conclusions in logical terms after we have constructed or discovered them in other ways; only then do we use verbal and other kinds of formal reasoning to clean things up, to separate the essential parts from the spaghettilike tangles of thoughts and ideas in which they first occurred.\nTo see why logic must come afterward, recall the idea of solving problems by using the generate and test method. In any such process, logic can be only a fraction of the reasoning; it can serve as a test to keep us from coming to invalid conclusions, but it cannot tell us which ideas to generate, or which processes and memories to use. Logic no more explains how we think than grammar explains how we speak; both can tell us whether our sentences are properly formed, but they cannot tell us which sentences to make. Without an intimate connection between our knowledge and our intentions, logic leads to madness, not intelligence. A logical system without a goal will merely generate an endless host of pointless truths like these:\nA implies A. P or not P. A implies A or A or A. If 4 is 5, then pigs can fly.",
    "type": "article",
    "title": "18.1 must machines be logical?",
    "tags": [
      {
        "score": 0.8268522024154663,
        "sentiment": 0,
        "count": 7,
        "label": "logic",
        "uri": "https://diffbot.com/entity/X2FR1oallObSTFA7yvuGb3g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5552462935447693,
        "sentiment": 0,
        "count": 1,
        "label": "Edgar Allan Poe",
        "uri": "https://diffbot.com/entity/PvybJnC9oM2iIZ6B_ipZgWQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 63819383188,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 3544039846,
    "gburl": "http://aurellem.org/society-of-mind/som-18.1.html-diffbotxyz1535378366",
    "lastCrawlTimeUTC": 1588765066,
    "timestamp": "Wed, 06 May 2020 11:37:46 GMT"
  },
  {
    "sentiment": 0.917,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1063236710",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-28.2.html",
    "html": "<p>How can a hurt be canceled by a kiss? How can an insult <em>add</em> to injury? Why do we so often speak as though our wishes and desires were like forces, which increase one another's effects when they're aligned but cancel out when they're opposed? I'll argue that this is because, at every moment of our lives, we're forced to choose between alternatives we can't compare. Suppose, for example, that you must choose between two homes: one of them offers a mountain view; the other is closer to where you work. It really is a strange idea that two such unrelated things as nearness to work and beautiful scenery could be compared at all. But a person could instead assess that pleasant, restful view as worth a certain amount of travel time. Instead of comparing the items themselves, you could simply compare how much time they seem to be worth.</p>\n<p>We turn to using quantities when we can't compare the qualities of things.</p>\n<p>This way, for better or for worse, we often assign some magnitude or price to each alternative. That tactic helps to simplify our lives so much that virtually every social community works out its own communal measure-schemes &mdash; let's call them currencies &mdash; that let its people work and trade in harmony, even though each individual has somewhat different personal goals. The establishment of a currency can foster both competition and cooperation by providing us with peaceful ways to divide and apportion the things we have to share.</p>\n<p>But who can set prices on things like time or measure the values of comfort and love? What makes our mental marketplaces work so well when emotional states seem so hard to compare? One reason is that no matter how different those mental conditions seem, they must all compete for certain limited resources &mdash; such as space, time, and energy &mdash; and these, to a rather large extent, are virtually interchangeable. For example, you'd end up with essentially the same result whether you measure things in terms of food or time &mdash; because it takes time to find food, and each amount of food helps you survive for some amount of time. Thus the value we place on each commodity constrains, to some extent, the values we'll assign to many other kinds of goods. Because there are so many such constraints, once a community sets up a currency, that currency takes on a life of its own, and soon we start to treat our <em>wealth</em> as though it were a genuine commodity, a real substance that we can use, save, lend, or waste.</p>\n<p>In a similar way, a group of agencies inside the brain could exploit some <em>amount</em> to keep account of their transactions with one another. Indeed agencies need such techniques even more than people do, because they are less able to appreciate each other's concerns. But if agents had to <em>pay their way,</em> what might they use for currency? One family of agents might evolve ways to exploit their common access to some chemical that is available in limited quantities; another family of agents might contrive to use a quantity that doesn't actually exist at all, but whose amount is simply <em>computed.</em> I suspect that what we call the pleasure of success may be, in effect, the currency of some such scheme. To the extent that success is interchangeable with time or food or energy, it's useful to treat pleasure as equivalent to wealth.</p>",
    "text": "How can a hurt be canceled by a kiss? How can an insult add to injury? Why do we so often speak as though our wishes and desires were like forces, which increase one another's effects when they're aligned but cancel out when they're opposed? I'll argue that this is because, at every moment of our lives, we're forced to choose between alternatives we can't compare. Suppose, for example, that you must choose between two homes: one of them offers a mountain view; the other is closer to where you work. It really is a strange idea that two such unrelated things as nearness to work and beautiful scenery could be compared at all. But a person could instead assess that pleasant, restful view as worth a certain amount of travel time. Instead of comparing the items themselves, you could simply compare how much time they seem to be worth.\nWe turn to using quantities when we can't compare the qualities of things.\nThis way, for better or for worse, we often assign some magnitude or price to each alternative. That tactic helps to simplify our lives so much that virtually every social community works out its own communal measure-schemes \u2014 let's call them currencies \u2014 that let its people work and trade in harmony, even though each individual has somewhat different personal goals. The establishment of a currency can foster both competition and cooperation by providing us with peaceful ways to divide and apportion the things we have to share.\nBut who can set prices on things like time or measure the values of comfort and love? What makes our mental marketplaces work so well when emotional states seem so hard to compare? One reason is that no matter how different those mental conditions seem, they must all compete for certain limited resources \u2014 such as space, time, and energy \u2014 and these, to a rather large extent, are virtually interchangeable. For example, you'd end up with essentially the same result whether you measure things in terms of food or time \u2014 because it takes time to find food, and each amount of food helps you survive for some amount of time. Thus the value we place on each commodity constrains, to some extent, the values we'll assign to many other kinds of goods. Because there are so many such constraints, once a community sets up a currency, that currency takes on a life of its own, and soon we start to treat our wealth as though it were a genuine commodity, a real substance that we can use, save, lend, or waste.\nIn a similar way, a group of agencies inside the brain could exploit some amount to keep account of their transactions with one another. Indeed agencies need such techniques even more than people do, because they are less able to appreciate each other's concerns. But if agents had to pay their way, what might they use for currency? One family of agents might evolve ways to exploit their common access to some chemical that is available in limited quantities; another family of agents might contrive to use a quantity that doesn't actually exist at all, but whose amount is simply computed. I suspect that what we call the pleasure of success may be, in effect, the currency of some such scheme. To the extent that success is interchangeable with time or food or energy, it's useful to treat pleasure as equivalent to wealth.",
    "type": "article",
    "title": "28.2 magnitude and marketplace",
    "docId": 197061558695,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 175300264374,
    "gburl": "http://aurellem.org/society-of-mind/som-28.2.html-diffbotxyz3755829009",
    "lastCrawlTimeUTC": 1588765022,
    "timestamp": "Wed, 06 May 2020 11:37:02 GMT"
  },
  {
    "sentiment": 0.764,
    "images": [
      {
        "naturalHeight": 133,
        "width": 393,
        "diffbotUri": "image|3|1152095618",
        "url": "http://aurellem.org/society-of-mind/illus/ch10/10-6.png",
        "naturalWidth": 393,
        "primary": true,
        "height": 133
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|697886370",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-10.4.html",
    "html": "<p>What should one do when different kinds of knowledge don't agree? It sometimes helps to place them in some order of priority, but as we've seen, that can still lead to mistakes. How can we make our system sensitive to different circumstances? The secret is to use the principle of noncompromise and look for help from other agencies! For help with comparing quantities, we'll need to add new <em>administrative agents</em> to our Society-of-More.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch10/10-6.png\"/></figure>\n<p>The new Appearance administrator is designed to say <em>more</em> when the agent Tall is active, to say <em>less</em> when the agent Thin is active,</p>\n<p>and to say nothing at all when something appears both taller and thinner. Then the other new administrator, History, makes the decision on the basis of what Confined says.</p>\n<p>This explanation of the difference between the older and younger children was first proposed by Seymour Papert in the 1960s, when we first started to explore society of mind ideas. Most previous theories had tried to explain Piaget's experiments by suggesting that children develop different kinds of reasoning as time goes by. That certainly is true, but the importance of Papert's conception is in emphasizing not merely the ingredients of reasoning, but how they're organized: a mind cannot really grow very much merely by accumulating knowledge. It must also develop better ways to use what it already knows. That principle deserves a name.</p>\n<p>Papert's Principle: Some of the most crucial steps in mental growth are based not simply on acquiring new skills, but on acquiring new administrative ways to use what one already knows.</p>\n<p>Our two new middle-level managers illustrate this idea: Appearance and History form a new, intermediate layer that groups together certain sets of lower-level skills. The choice of agents for those groups is absolutely critical. The system will work quite well if we group Tall and Thin together, so that Confined can take control when they conflict. But it would only make things worse if we were to group Tall and Confined together! Then what decides which groups to form? Papert's principle suggests that the processes which assemble agents into groups must somehow exploit relationships among the skills of those agents. For example, because Tall and Thin are more similar in character to one another than to Confined, it makes sense to group them more closely together in the administrative hierarchy.</p>",
    "text": "What should one do when different kinds of knowledge don't agree? It sometimes helps to place them in some order of priority, but as we've seen, that can still lead to mistakes. How can we make our system sensitive to different circumstances? The secret is to use the principle of noncompromise and look for help from other agencies! For help with comparing quantities, we'll need to add new administrative agents to our Society-of-More.\nThe new Appearance administrator is designed to say more when the agent Tall is active, to say less when the agent Thin is active,\nand to say nothing at all when something appears both taller and thinner. Then the other new administrator, History, makes the decision on the basis of what Confined says.\nThis explanation of the difference between the older and younger children was first proposed by Seymour Papert in the 1960s, when we first started to explore society of mind ideas. Most previous theories had tried to explain Piaget's experiments by suggesting that children develop different kinds of reasoning as time goes by. That certainly is true, but the importance of Papert's conception is in emphasizing not merely the ingredients of reasoning, but how they're organized: a mind cannot really grow very much merely by accumulating knowledge. It must also develop better ways to use what it already knows. That principle deserves a name.\nPapert's Principle: Some of the most crucial steps in mental growth are based not simply on acquiring new skills, but on acquiring new administrative ways to use what one already knows.\nOur two new middle-level managers illustrate this idea: Appearance and History form a new, intermediate layer that groups together certain sets of lower-level skills. The choice of agents for those groups is absolutely critical. The system will work quite well if we group Tall and Thin together, so that Confined can take control when they conflict. But it would only make things worse if we were to group Tall and Confined together! Then what decides which groups to form? Papert's principle suggests that the processes which assemble agents into groups must somehow exploit relationships among the skills of those agents. For example, because Tall and Thin are more similar in character to one another than to Confined, it makes sense to group them more closely together in the administrative hierarchy.",
    "type": "article",
    "title": "10.4 papert's principle",
    "tags": [
      {
        "score": 0.8667823076248169,
        "sentiment": 0,
        "count": 4,
        "label": "Seymour Papert",
        "uri": "https://diffbot.com/entity/P8Ue3ofn7MP-xCyDxpkXe0A",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5673168301582336,
        "sentiment": 0,
        "count": 2,
        "label": "History",
        "uri": "https://diffbot.com/entity/CScu9JMKOPmK102LS4ozGvg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Broadcaster",
          "http://dbpedia.org/ontology/BroadcastNetwork"
        ]
      },
      {
        "score": 0.5053979754447937,
        "sentiment": 0.749,
        "count": 3,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 239257551282,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 44523176321,
    "gburl": "http://aurellem.org/society-of-mind/som-10.4.html-diffbotxyz3768011102",
    "lastCrawlTimeUTC": 1588765099,
    "timestamp": "Wed, 06 May 2020 11:38:19 GMT"
  },
  {
    "sentiment": -0.894,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-993731402",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-10.8.html",
    "html": "<p>Such lessons just don't seem to work very well. Given enough explanation and encouragement, and enough drill and practice, we can make children appear to understand &mdash; yet even then they don't often apply what they've <em>learned</em> to real-life situations. Thus it seems that even when we lead them along these paths, they remain unable to use much of what we show to them until they develop inner signposts of their own.</p>\n<p>Here's my guess about what goes wrong. Presumably the child senses that the spaced-out eggs are <em>more</em> because they stretch across a longer span. Eventually, we want that sense of greater length to be canceled out by the sense that there's more empty space between the eggs. In the more mature Papert hierarchy, this would happen automatically &mdash; but for now, the child could learn this only as a special, isolated rule. Many other problems could also be solved by making special rules for them. But to <em>simulate</em> that multilayer society, complete with middle-level agents like Appearance and History, would involve so many special rules, and so many exceptions to them, that the younger child would be unable to manage so much complexity. The result is that educational programs allegedly designed <em>according to Piaget</em> often appear to succeed from one moment to the next, but the structures that result from this are so fragile and specialized that children can apply them only to contexts almost exactly like those in which they were learned.</p>\n<p>All this reminds me of a visit to my home from my friend Gilbert Voyat, who was then a student of Papert and Piaget and later became a distinguished child psychologist. On meeting our five-year-old twins, his eyes sparkled, and he quickly improvised some experiments in the kitchen. Gilbert engaged Julie first, planning to ask her about whether a potato would balance best on one, two, three, or four toothpicks. First, in order to assess her general development, he began by performing the water jar experiment. The conversation went like this:</p>\n<p>Gilbert: <em>Is there more water in this jar or in that jar?</em> Julie: <em>It looks like there's more in that one. But you should ask my brother, Henry. He has conservation already.</em></p>\n<p>Gilbert paled and fled. I always wondered what Henry would have said. In any case, this anecdote illustrates how a young child may possess many of the ingredients of perception, knowledge, and ability needed for this kind of judgment &mdash; yet still not have suitably organized those components.</p>\n<p>Parent: Why are all the agents in your societies so competitive? They're always attacking each other. Instead of making Tall and Thin cancel each other out, why can't they cooperate?</p>\n<p>The first part of this book has given this impression because we had to begin with relatively simple mechanisms. It is fairly easy to resolve conflicts by switching among alternatives. It is much harder to develop mechanisms that can use cooperation and compromise &mdash; because that requires more complex ways for agencies to interact. In later sections of this book we'll see how higher-level systems could make more reasonable negotiations and compromises.</p>",
    "text": "Such lessons just don't seem to work very well. Given enough explanation and encouragement, and enough drill and practice, we can make children appear to understand \u2014 yet even then they don't often apply what they've learned to real-life situations. Thus it seems that even when we lead them along these paths, they remain unable to use much of what we show to them until they develop inner signposts of their own.\nHere's my guess about what goes wrong. Presumably the child senses that the spaced-out eggs are more because they stretch across a longer span. Eventually, we want that sense of greater length to be canceled out by the sense that there's more empty space between the eggs. In the more mature Papert hierarchy, this would happen automatically \u2014 but for now, the child could learn this only as a special, isolated rule. Many other problems could also be solved by making special rules for them. But to simulate that multilayer society, complete with middle-level agents like Appearance and History, would involve so many special rules, and so many exceptions to them, that the younger child would be unable to manage so much complexity. The result is that educational programs allegedly designed according to Piaget often appear to succeed from one moment to the next, but the structures that result from this are so fragile and specialized that children can apply them only to contexts almost exactly like those in which they were learned.\nAll this reminds me of a visit to my home from my friend Gilbert Voyat, who was then a student of Papert and Piaget and later became a distinguished child psychologist. On meeting our five-year-old twins, his eyes sparkled, and he quickly improvised some experiments in the kitchen. Gilbert engaged Julie first, planning to ask her about whether a potato would balance best on one, two, three, or four toothpicks. First, in order to assess her general development, he began by performing the water jar experiment. The conversation went like this:\nGilbert: Is there more water in this jar or in that jar? Julie: It looks like there's more in that one. But you should ask my brother, Henry. He has conservation already.\nGilbert paled and fled. I always wondered what Henry would have said. In any case, this anecdote illustrates how a young child may possess many of the ingredients of perception, knowledge, and ability needed for this kind of judgment \u2014 yet still not have suitably organized those components.\nParent: Why are all the agents in your societies so competitive? They're always attacking each other. Instead of making Tall and Thin cancel each other out, why can't they cooperate?\nThe first part of this book has given this impression because we had to begin with relatively simple mechanisms. It is fairly easy to resolve conflicts by switching among alternatives. It is much harder to develop mechanisms that can use cooperation and compromise \u2014 because that requires more complex ways for agencies to interact. In later sections of this book we'll see how higher-level systems could make more reasonable negotiations and compromises.",
    "type": "article",
    "title": "10.8 education and development",
    "tags": [
      {
        "score": 0.749894917011261,
        "sentiment": 0.514,
        "count": 4,
        "label": "W. S. Gilbert",
        "uri": "https://diffbot.com/entity/PXbn0mLAkM8ygtJ3XIzMCfg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.6912551522254944,
        "sentiment": 0,
        "count": 1,
        "label": "education",
        "uri": "https://diffbot.com/entity/XXcR_kk4bOyWJaNHalWfncw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5677456259727478,
        "sentiment": 0.255,
        "count": 2,
        "label": "Seymour Papert",
        "uri": "https://diffbot.com/entity/P8Ue3ofn7MP-xCyDxpkXe0A",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5674644112586975,
        "sentiment": 0,
        "count": 2,
        "label": "Jean Piaget",
        "uri": "https://diffbot.com/entity/PDLtioHPGOfSddrmNWrXIYg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5600479245185852,
        "sentiment": 0,
        "count": 2,
        "label": "Julie Martin",
        "uri": "https://diffbot.com/entity/X1pyG2-J5NBGwfctBd6rEsQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5139866471290588,
        "sentiment": -0.647,
        "count": 5,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 31838192002,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 134805651895,
    "gburl": "http://aurellem.org/society-of-mind/som-10.8.html-diffbotxyz3471441918",
    "lastCrawlTimeUTC": 1588764893,
    "timestamp": "Wed, 06 May 2020 11:34:53 GMT"
  },
  {
    "sentiment": 0,
    "images": [
      {
        "naturalHeight": 129,
        "width": 377,
        "diffbotUri": "image|3|1105528107",
        "url": "http://aurellem.org/society-of-mind/illus/ch12/12-12.png",
        "naturalWidth": 377,
        "primary": true,
        "height": 129
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1459324934",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.8.html",
    "html": "<p>When should you accumulate, and when should you make uniframes? The choice depends upon your purposes. Sometimes it is useful to regard things as similar because they have similar forms, but sometimes it makes more sense to group together things with similar uses. At one moment you may wish to emphasize a similarity; at the next moment, you may want to emphasize a distinction. Often, we have to use both uniframes and accumulations in combination. In Block-Arch, for example, we found that there could be two different kinds of arch tops &mdash; the block and the wedge. Accordingly, when we used the phrase <em>block or wedge,</em> we actually inserted a <em>subaccumulation</em> into our uniframe.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch12/12-12.png\"/></figure>\n<p>Accumulations rarely seem quite satisfactory because we feel ideas should have more unity. We wouldn't have a word for chair or arch or currency if they meant nothing more than lists of unrelated things. If each did not involve some unifying thought, we'd never think to make those lists in the first place! Why is it so hard to describe their essences? In the next few sections we'll discover a number of reasons for this. Here is one of them:</p>\n<p>Many good ideas are really two ideas in one &mdash; which form a bridge between two realms of thought or different points of view.</p>\n<p>Whenever we build a bridge between structure and function, one end of that bridge may represent a goal or use, while the other end describes what we might use to gain those ends. But it is rare for those structures to correspond neatly to those functions. The problem is that we usually find many different ways to achieve any goal. This means that we'll find an accumulation on the structural side of the bridge. For example, if you want to reach something high up, you can stand on a chair, reach with a stick, or ask someone taller to get it for you. Similarly, an accumulation of functions or goals can be found for any structure. My colleague Oliver Selfridge once wrote an entire book entitled Things to Do with a Stick.</p>\n<p>Our different worlds of ends and means don't usually match up very well. So when we find a useful, compact uniframe in one such world, it often corresponds to an accumulation in our other worlds.</p>\n<p>We encountered this problem earlier. When we classified birds as animals while classifying airplanes as machines, we thereby forced disunity upon the class of things that fly. Later, when we come to theories about metaphors, we'll see that such problems are almost inevitable because we know only a very few &mdash; and, therefore, very precious &mdash; schemes whose unifying powers cross many realms.</p>",
    "text": "When should you accumulate, and when should you make uniframes? The choice depends upon your purposes. Sometimes it is useful to regard things as similar because they have similar forms, but sometimes it makes more sense to group together things with similar uses. At one moment you may wish to emphasize a similarity; at the next moment, you may want to emphasize a distinction. Often, we have to use both uniframes and accumulations in combination. In Block-Arch, for example, we found that there could be two different kinds of arch tops \u2014 the block and the wedge. Accordingly, when we used the phrase block or wedge, we actually inserted a subaccumulation into our uniframe.\nAccumulations rarely seem quite satisfactory because we feel ideas should have more unity. We wouldn't have a word for chair or arch or currency if they meant nothing more than lists of unrelated things. If each did not involve some unifying thought, we'd never think to make those lists in the first place! Why is it so hard to describe their essences? In the next few sections we'll discover a number of reasons for this. Here is one of them:\nMany good ideas are really two ideas in one \u2014 which form a bridge between two realms of thought or different points of view.\nWhenever we build a bridge between structure and function, one end of that bridge may represent a goal or use, while the other end describes what we might use to gain those ends. But it is rare for those structures to correspond neatly to those functions. The problem is that we usually find many different ways to achieve any goal. This means that we'll find an accumulation on the structural side of the bridge. For example, if you want to reach something high up, you can stand on a chair, reach with a stick, or ask someone taller to get it for you. Similarly, an accumulation of functions or goals can be found for any structure. My colleague Oliver Selfridge once wrote an entire book entitled Things to Do with a Stick.\nOur different worlds of ends and means don't usually match up very well. So when we find a useful, compact uniframe in one such world, it often corresponds to an accumulation in our other worlds.\nWe encountered this problem earlier. When we classified birds as animals while classifying airplanes as machines, we thereby forced disunity upon the class of things that fly. Later, when we come to theories about metaphors, we'll see that such problems are almost inevitable because we know only a very few \u2014 and, therefore, very precious \u2014 schemes whose unifying powers cross many realms.",
    "type": "article",
    "title": "12.8 problems of disunity",
    "tags": [
      {
        "score": 0.5277542471885681,
        "sentiment": 0,
        "count": 0,
        "label": "Gateway Arch",
        "uri": "https://diffbot.com/entity/Li4ey-yQzN56wwgpdmAbFoA",
        "rdfTypes": ["http://dbpedia.org/ontology/Monument"]
      }
    ],
    "docId": 41360245182,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 28576989601,
    "gburl": "http://aurellem.org/society-of-mind/som-12.8.html-diffbotxyz741538532",
    "lastCrawlTimeUTC": 1588764872,
    "timestamp": "Wed, 06 May 2020 11:34:32 GMT"
  },
  {
    "sentiment": -0.27,
    "images": [
      {
        "naturalHeight": 118,
        "width": 395,
        "diffbotUri": "image|3|1840315231",
        "url": "http://aurellem.org/society-of-mind/illus/ch11/11-3.png",
        "naturalWidth": 395,
        "primary": true,
        "height": 118
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-645682365",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.8.html",
    "html": "<p>Let's do one more experiment: touch one ear and then touch your nose. They don't feel very similar. Now touch one ear and then the other. These touches seem more similar, although they're twice as far apart. This may be in part because they are represented in related agencies. In fact, our brains have many pairs of agencies, arranged like mirror- images, with huge bundles of nerves running between them.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch11/11-3.png\"/></figure>\n<p>The two hemispheres of the brain look so alike that they were long assumed to be identical. Then it was found that after those cross-connections are destroyed, usually only the left brain can recognize or speak words, and only the right brain can draw pictures. More recently, when modern methods found other differences between those sides, it seems to me that some psychologists went mad &mdash; and tried to match those differences to every mentalistic two-part theory that ever was conceived. Our culture soon became entranced by this revival of an old idea in modern guise: that our minds are meeting grounds for pairs of antiprinciples. On one side stands the Logical, across from Analogical. The left-side brain is Rational; the right side is Emotional. No wonder so many seized upon this pseudoscientific scheme: it gave new life to nearly every dead idea of how to cleave the mental world into two halves as nicely as a peach.</p>\n<p>What's wrong with this is that each brain has many parts, not only two. And though there are many differences, we also ought to ask about why those left-right brain halves are actually so similar. What functions might this serve? For one thing, we know that when a major brain area is damaged in a young person, the mirror region can sometimes take over its function. Probably even when there is no injury, an agency that has consumed all the space available in its neighborhood can expand into the mirror region across the way. Another theory: a pair of mirrored agencies could be useful for making comparisons and for recognizing differences, since if one side could make a copy of its state on the other side then, after doing some work, it could compare those initial and final states to see what progress had been made.</p>\n<p>My own theory of what happens when the cross-connections between those brain halves are destroyed is that, in early life, we start with mostly similar agencies on either side. Later, as we grow more complex, a combination of genetic and circumstantial effects lead one of each pair to take control of both. Otherwise, we might become paralyzed by conflicts, because many agents would have to serve two masters. Eventually, the adult managers for many skills would tend to develop on the side of the brain most concerned with language because those agencies connect to an unusually large number of other agencies. The less dominant side of the brain will continue to develop, but with fewer administrative functions &mdash; and end up with more of our lower-level skills, but with less involvement in plans and higher-level goals that engage many agencies at once. Then if, by accident, that brain half is abandoned to itself, it will seem more childish and less mature because it lags so far behind in administrative growth.</p>",
    "text": "Let's do one more experiment: touch one ear and then touch your nose. They don't feel very similar. Now touch one ear and then the other. These touches seem more similar, although they're twice as far apart. This may be in part because they are represented in related agencies. In fact, our brains have many pairs of agencies, arranged like mirror- images, with huge bundles of nerves running between them.\nThe two hemispheres of the brain look so alike that they were long assumed to be identical. Then it was found that after those cross-connections are destroyed, usually only the left brain can recognize or speak words, and only the right brain can draw pictures. More recently, when modern methods found other differences between those sides, it seems to me that some psychologists went mad \u2014 and tried to match those differences to every mentalistic two-part theory that ever was conceived. Our culture soon became entranced by this revival of an old idea in modern guise: that our minds are meeting grounds for pairs of antiprinciples. On one side stands the Logical, across from Analogical. The left-side brain is Rational; the right side is Emotional. No wonder so many seized upon this pseudoscientific scheme: it gave new life to nearly every dead idea of how to cleave the mental world into two halves as nicely as a peach.\nWhat's wrong with this is that each brain has many parts, not only two. And though there are many differences, we also ought to ask about why those left-right brain halves are actually so similar. What functions might this serve? For one thing, we know that when a major brain area is damaged in a young person, the mirror region can sometimes take over its function. Probably even when there is no injury, an agency that has consumed all the space available in its neighborhood can expand into the mirror region across the way. Another theory: a pair of mirrored agencies could be useful for making comparisons and for recognizing differences, since if one side could make a copy of its state on the other side then, after doing some work, it could compare those initial and final states to see what progress had been made.\nMy own theory of what happens when the cross-connections between those brain halves are destroyed is that, in early life, we start with mostly similar agencies on either side. Later, as we grow more complex, a combination of genetic and circumstantial effects lead one of each pair to take control of both. Otherwise, we might become paralyzed by conflicts, because many agents would have to serve two masters. Eventually, the adult managers for many skills would tend to develop on the side of the brain most concerned with language because those agencies connect to an unusually large number of other agencies. The less dominant side of the brain will continue to develop, but with fewer administrative functions \u2014 and end up with more of our lower-level skills, but with less involvement in plans and higher-level goals that engage many agencies at once. Then if, by accident, that brain half is abandoned to itself, it will seem more childish and less mature because it lags so far behind in administrative growth.",
    "type": "article",
    "title": "11.8 half-brains",
    "tags": [
      {
        "score": 0.7147341966629028,
        "sentiment": -0.415,
        "count": 2,
        "label": "brain",
        "uri": "https://diffbot.com/entity/X_EH2MANnPh60z9fSfbgdHw"
      },
      {
        "score": 0.5266916751861572,
        "sentiment": 0,
        "count": 1,
        "label": "Our Culture, What's Left of It",
        "uri": "https://diffbot.com/entity/Xab8dv2XTNgu0zljCjl6fGg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.5006165504455566,
        "sentiment": 0,
        "count": 1,
        "label": "Let's",
        "uri": "https://diffbot.com/entity/CRLcl-YyyOXa6DYMRB1qTdA",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      }
    ],
    "docId": 45041549739,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 239155757485,
    "gburl": "http://aurellem.org/society-of-mind/som-11.8.html-diffbotxyz2763579021",
    "lastCrawlTimeUTC": 1588764955,
    "timestamp": "Wed, 06 May 2020 11:35:55 GMT"
  },
  {
    "sentiment": 0.12,
    "humanLanguage": "en",
    "diffbotUri": "article|3|73479106",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-3.2.html",
    "html": "<h2><em>3.2</em> Noncompromise</h2>\n<p>To settle arguments, nations develop legal systems, corporations establish policies, and individuals may argue, fight, or compromise &mdash; or turn for help to mediators that lie outside themselves. What happens when there are conflicts inside minds?</p>\n<p>Whenever several agents have to compete for the same resources, they are likely to get into conflicts. If those agents were left to themselves, the conflicts might persist indefinitely, and this would leave those agents paralyzed, unable to accomplish any goal. What happens then? We'll assume that those agents' supervisors, too, are under competitive pressure and likely to grow weak themselves whenever their subordinates are slow in achieving their goals, no matter whether because of conflicts between them or because of individual incompetence.</p>\n<p>The Principle of Noncompromise: The longer an internal conflict persists among an agent's subordinates, the weaker becomes that agent's status among its own competitors. If such internal problems aren't settled soon, other agents will take control and the agents formerly involved will be <em>dismissed.</em></p>\n<p>So long as playing with blocks goes well, Play can maintain its strength and keep control. In the meantime, though, the child may also be growing hungry and sleepy, because other processes are arousing the agents Eat and Sleep. So long as Eat and Sleep are not yet strongly activated, Play can hold them both at bay. However, any conflict inside Play will weaken it and make it easier for Eat or Sleep to take over. Of course, Eat or Sleep must conquer in the end, since the longer they wait, the stronger they get.</p>\n<p>We see this in our own experience. We all know how easy it is to fight off small distractions when things are going well. But once some trouble starts inside our work, we become increasingly impatient and irritable. Eventually we find it so hard to concentrate that the least disturbance can allow another, different, interest to take control.</p>\n<p>Now, when any of our agencies loses the power to control what other systems do, that doesn't mean it has to cease its own internal activity. An agency that has lost control can continue to work inside itself &mdash; and thus become prepared to seize a later opportunity. However, we're normally unaware of all those other activities proceeding deep inside our minds.</p>\n<p>Where does it stop, this process of yielding control to other agencies? Must every mind contain some topmost center of control? Not necessarily. We sometimes settle conflicts by appealing to superiors, but other conflicts never end and never cease to trouble us.</p>\n<p>At first, our principle of noncompromise may seem too extreme. After all, good human supervisors plan ahead to avoid conflicts in the first place, and &mdash; when they can't &mdash; they try to settle quarrels locally before appealing to superiors. But we should not try to find a close analogy between the low-level agents of a single mind and the members of a human community. Those tiny mental agents simply cannot know enough to be able to negotiate with one another or to find effective ways to adjust to each other's interference. Only larger agencies could be resourceful enough to do such things. Inside an actual child, the agencies responsible for Building and Wrecking might indeed become versatile enough to negotiate by offering support for one another's goals. <em>Please, Wrecker, wait a moment more till Builder adds just one more block: it's worth it for a louder crash!</em></p>",
    "text": "3.2 Noncompromise\nTo settle arguments, nations develop legal systems, corporations establish policies, and individuals may argue, fight, or compromise \u2014 or turn for help to mediators that lie outside themselves. What happens when there are conflicts inside minds?\nWhenever several agents have to compete for the same resources, they are likely to get into conflicts. If those agents were left to themselves, the conflicts might persist indefinitely, and this would leave those agents paralyzed, unable to accomplish any goal. What happens then? We'll assume that those agents' supervisors, too, are under competitive pressure and likely to grow weak themselves whenever their subordinates are slow in achieving their goals, no matter whether because of conflicts between them or because of individual incompetence.\nThe Principle of Noncompromise: The longer an internal conflict persists among an agent's subordinates, the weaker becomes that agent's status among its own competitors. If such internal problems aren't settled soon, other agents will take control and the agents formerly involved will be dismissed.\nSo long as playing with blocks goes well, Play can maintain its strength and keep control. In the meantime, though, the child may also be growing hungry and sleepy, because other processes are arousing the agents Eat and Sleep. So long as Eat and Sleep are not yet strongly activated, Play can hold them both at bay. However, any conflict inside Play will weaken it and make it easier for Eat or Sleep to take over. Of course, Eat or Sleep must conquer in the end, since the longer they wait, the stronger they get.\nWe see this in our own experience. We all know how easy it is to fight off small distractions when things are going well. But once some trouble starts inside our work, we become increasingly impatient and irritable. Eventually we find it so hard to concentrate that the least disturbance can allow another, different, interest to take control.\nNow, when any of our agencies loses the power to control what other systems do, that doesn't mean it has to cease its own internal activity. An agency that has lost control can continue to work inside itself \u2014 and thus become prepared to seize a later opportunity. However, we're normally unaware of all those other activities proceeding deep inside our minds.\nWhere does it stop, this process of yielding control to other agencies? Must every mind contain some topmost center of control? Not necessarily. We sometimes settle conflicts by appealing to superiors, but other conflicts never end and never cease to trouble us.\nAt first, our principle of noncompromise may seem too extreme. After all, good human supervisors plan ahead to avoid conflicts in the first place, and \u2014 when they can't \u2014 they try to settle quarrels locally before appealing to superiors. But we should not try to find a close analogy between the low-level agents of a single mind and the members of a human community. Those tiny mental agents simply cannot know enough to be able to negotiate with one another or to find effective ways to adjust to each other's interference. Only larger agencies could be resourceful enough to do such things. Inside an actual child, the agencies responsible for Building and Wrecking might indeed become versatile enough to negotiate by offering support for one another's goals. Please, Wrecker, wait a moment more till Builder adds just one more block: it's worth it for a louder crash!",
    "type": "article",
    "title": "3.2 Noncompromise",
    "tags": [
      {
        "score": 0.5631970763206482,
        "sentiment": -0.907,
        "count": 10,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5582239627838135,
        "sentiment": 0.655,
        "count": 2,
        "label": "Sleep",
        "uri": "https://diffbot.com/entity/OHbQ7lGIKP_yOL41orRkOKg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      }
    ],
    "docId": 172304957883,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 114837143957,
    "gburl": "http://aurellem.org/society-of-mind/som-3.2.html-diffbotxyz3174769601",
    "lastCrawlTimeUTC": 1588764914,
    "timestamp": "Wed, 06 May 2020 11:35:14 GMT"
  },
  {
    "sentiment": 0.253,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1163155188",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.10.html",
    "html": "<p>We naturally admire our Einsteins, Shakespeares, and Beethovens &mdash; and we wonder if machines could ever create such wondrous theories, plays, and symphonies. Most people think that accomplishments like these require <em>talents</em> or <em>gifts</em> that cannot be explained. If so, then it follows that computers can't create such things &mdash; since anything machines do can be explained. But why assume that what our greatest artists do is very different from what ordinary people do &mdash; when we know so little about what ordinary people do! Surely it is premature to ask how great composers write great symphonies before we know how ordinary people think of ordinary tunes. I don't believe there is much difference between normal and <em>creative</em> thought. Right now, if asked which seems the more mysterious, I'd have to say the ordinary kind.</p>\n<p>We shouldn't let our envy of distinguished masters of the arts distract us from the wonder of how each of us gets new ideas. Perhaps we hold on to our superstitions about creativity in order to make our own deficiencies seem more excusable. For when we tell ourselves that masterful abilities are simply unexplainable, we're also comforting ourselves by saying that those super-heroes come endowed with all the qualities we don't possess. Our failures are therefore no fault of our own, nor are those heroes' virtues to their credit, either. If it isn't learned, it isn't earned.</p>\n<p>When we actually meet the heroes whom our culture views as great, we don't find any singular propensities &mdash; only combinations of ingredients quite common in themselves. Most of these heroes are intensely motivated, but so are many other people. They're usually very proficient in some field &mdash; but in itself we simply call this craftsmanship or expertise. They often have enough self-confidence to stand up to the scorn of peers &mdash; but in itself, we might just call that stubbornness. They surely think of things in some novel ways, but so does everyone from time to time. And as for what we call <em>intelligence,</em> my view is that each person who can speak coherently already has the better part of what our heroes have. Then what makes genius appear to stand apart, if we each have most of what it takes?</p>\n<p>I suspect that genius needs one thing more: in order to accumulate outstanding qualities, one needs unusually effective ways to learn. It's not enough to learn a lot; one also has to manage what one learns. Those masters have, beneath the surface of their mastery, some special knacks of <em>higher-order</em> expertise, which help them organize and apply the things they learn. It is those hidden tricks of mental management that produce the systems that create those works of genius. Why do certain people learn so many more and better skills? These all-important differences could begin with early accidents. One child works out clever ways to arrange some blocks in rows and stacks; a second child plays at rearranging how it thinks. Everyone can praise the first child's castles and towers, but no one can see what the second child has done, and one may even get the false impression of a lack of industry. But if the second child persists in seeking better ways to learn, this can lead to silent growth in which some better ways to learn may lead to better ways to learn to learn. Then, later, well observe an awesome, qualitative change, with no apparent cause &mdash; and give to it some empty name like talent, aptitude, or gift.</p>\n<p>Finally, an awful thought: perhaps what we call genius is rare because our evolution works without respect for individuals. Could any tribe or culture endure in which each individual discovered novel ways to think? If not, how sad, since the genes for genius might then lead not to nurturing, but only to frequent weeding-out.</p>",
    "text": "We naturally admire our Einsteins, Shakespeares, and Beethovens \u2014 and we wonder if machines could ever create such wondrous theories, plays, and symphonies. Most people think that accomplishments like these require talents or gifts that cannot be explained. If so, then it follows that computers can't create such things \u2014 since anything machines do can be explained. But why assume that what our greatest artists do is very different from what ordinary people do \u2014 when we know so little about what ordinary people do! Surely it is premature to ask how great composers write great symphonies before we know how ordinary people think of ordinary tunes. I don't believe there is much difference between normal and creative thought. Right now, if asked which seems the more mysterious, I'd have to say the ordinary kind.\nWe shouldn't let our envy of distinguished masters of the arts distract us from the wonder of how each of us gets new ideas. Perhaps we hold on to our superstitions about creativity in order to make our own deficiencies seem more excusable. For when we tell ourselves that masterful abilities are simply unexplainable, we're also comforting ourselves by saying that those super-heroes come endowed with all the qualities we don't possess. Our failures are therefore no fault of our own, nor are those heroes' virtues to their credit, either. If it isn't learned, it isn't earned.\nWhen we actually meet the heroes whom our culture views as great, we don't find any singular propensities \u2014 only combinations of ingredients quite common in themselves. Most of these heroes are intensely motivated, but so are many other people. They're usually very proficient in some field \u2014 but in itself we simply call this craftsmanship or expertise. They often have enough self-confidence to stand up to the scorn of peers \u2014 but in itself, we might just call that stubbornness. They surely think of things in some novel ways, but so does everyone from time to time. And as for what we call intelligence, my view is that each person who can speak coherently already has the better part of what our heroes have. Then what makes genius appear to stand apart, if we each have most of what it takes?\nI suspect that genius needs one thing more: in order to accumulate outstanding qualities, one needs unusually effective ways to learn. It's not enough to learn a lot; one also has to manage what one learns. Those masters have, beneath the surface of their mastery, some special knacks of higher-order expertise, which help them organize and apply the things they learn. It is those hidden tricks of mental management that produce the systems that create those works of genius. Why do certain people learn so many more and better skills? These all-important differences could begin with early accidents. One child works out clever ways to arrange some blocks in rows and stacks; a second child plays at rearranging how it thinks. Everyone can praise the first child's castles and towers, but no one can see what the second child has done, and one may even get the false impression of a lack of industry. But if the second child persists in seeking better ways to learn, this can lead to silent growth in which some better ways to learn may lead to better ways to learn to learn. Then, later, well observe an awesome, qualitative change, with no apparent cause \u2014 and give to it some empty name like talent, aptitude, or gift.\nFinally, an awful thought: perhaps what we call genius is rare because our evolution works without respect for individuals. Could any tribe or culture endure in which each individual discovered novel ways to think? If not, how sad, since the genes for genius might then lead not to nurturing, but only to frequent weeding-out.",
    "type": "article",
    "title": "7.10 genius",
    "tags": [
      {
        "score": 0.5819314122200012,
        "sentiment": 0,
        "count": 2,
        "label": "ordinary",
        "uri": "https://diffbot.com/entity/X8P6bf9EwNmSiRmBqr3HzoA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession"
        ]
      },
      {
        "score": 0.5696398019790649,
        "sentiment": 0.914,
        "count": 1,
        "label": "William Shakespeare",
        "uri": "https://diffbot.com/entity/PjjLkNTA0O1KDotckPhZqlg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5353769659996033,
        "sentiment": 0.208,
        "count": 1,
        "label": "information technology",
        "uri": "https://diffbot.com/entity/X3ffe7fPvOfKnzD0cOCsEaQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5325972437858582,
        "sentiment": 0.86,
        "count": 2,
        "label": "symphony",
        "uri": "https://diffbot.com/entity/X_ahBkw7DOiapzWmCPc5bVQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Genre",
          "http://dbpedia.org/ontology/MusicGenre",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5282543301582336,
        "sentiment": 0,
        "count": 2,
        "label": "It Isn't, It Wasn't, It Ain't Never Gonna Be",
        "uri": "https://diffbot.com/entity/XsSLOTFkBNOG90uq_6lcMaw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Single"
        ]
      },
      {
        "score": 0.5098539590835571,
        "sentiment": 0.958,
        "count": 1,
        "label": "Ludwig van Beethoven",
        "uri": "https://diffbot.com/entity/PBcSxaL5vOI-6QoryqE-6tA",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 184023548329,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 58937786755,
    "gburl": "http://aurellem.org/society-of-mind/som-7.10.html-diffbotxyz3527904906",
    "lastCrawlTimeUTC": 1588764988,
    "timestamp": "Wed, 06 May 2020 11:36:28 GMT"
  },
  {
    "sentiment": -0.762,
    "humanLanguage": "en",
    "diffbotUri": "article|3|610392538",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-25.4.html",
    "html": "<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<p>Imagine what these frame-arrays can do! They let us <em>visualize</em> imaginary scenes, such as what might happen when we move, because the frames for what we can expect to see are filled in automatically. Not only that, but by using other processes to fill in all those terminals, we can <em>imagine</em> scenes and views of things we've never seen before. Still, many people find it hard to consider the thought that mental images could be based on anything as crude as frame-arrays. The world of our experience seems so perfectly continuous. Could such smooth thoughts emerge from sudden frame-to-frame jumps? If the mind kept jerking from one frame to another, wouldn't what we experience seem equally abrupt? Yet we rarely feel our minds change frames, any more than we perceive a visual scene as composed of disconnected spots of light. Why do we have the sense that things proceed in smooth, continuous ways? Is it because, as some mystics think, our minds are part of some flowing stream? I think it's just the opposite: our sense of constant, steady change emerges from the parts of mind that manage to insulate themselves against the continuous flow of time!</p>\n<p>In other words, our sense of smooth progression from one mental state to another emerges not from the nature of that progression itself, but from the descriptions we use to represent it. Nothing can seem jerky except what is represented as jerky. Paradoxically, our sense of continuity comes from our marvelous insensitivity to most kinds of changes rather than from any genuine perceptiveness. Existence seems continuous to us not because we continually experience what is happening in the present, but because we hold to our memories of how things were in the recent past. Without those short-term memories, all would seem entirely new at every instant, and we would have no sense at all of continuity or, for that matter, of existence.</p>\n<p>One might suppose that it would be wonderful to possess a faculty of <em>continual awareness.</em> But such an affliction would be worse than useless, because the more frequently our higher-level agencies change their representations of reality, the harder it is for them to find significance in what they sense. The power of consciousness comes not from ceaseless change of state, but from having enough stability to discern significant changes in our surroundings. To <em>notice</em> change requires the ability to resist it. In order to sense what persists through time, one must be able to examine and compare descriptions from the recent past. We notice change in spite of change, not because of it.</p>\n<p>Our sense of constant contact with the world is not a genuine experience; instead, it is a form of immanence illusion. We have the sense of actuality when every question asked of our visual-systems is answered so swiftly that it seems as though those answers were already there. And that's what frame-arrays provide us with: once any frame fills its terminals, the terminals of the other frames in its array are also filled. When every change of view engages frames whose terminals are already filled, albeit only by default, then sight seems instantaneous.</p>",
    "text": "Your browser does not support the video tag.\nImagine what these frame-arrays can do! They let us visualize imaginary scenes, such as what might happen when we move, because the frames for what we can expect to see are filled in automatically. Not only that, but by using other processes to fill in all those terminals, we can imagine scenes and views of things we've never seen before. Still, many people find it hard to consider the thought that mental images could be based on anything as crude as frame-arrays. The world of our experience seems so perfectly continuous. Could such smooth thoughts emerge from sudden frame-to-frame jumps? If the mind kept jerking from one frame to another, wouldn't what we experience seem equally abrupt? Yet we rarely feel our minds change frames, any more than we perceive a visual scene as composed of disconnected spots of light. Why do we have the sense that things proceed in smooth, continuous ways? Is it because, as some mystics think, our minds are part of some flowing stream? I think it's just the opposite: our sense of constant, steady change emerges from the parts of mind that manage to insulate themselves against the continuous flow of time!\nIn other words, our sense of smooth progression from one mental state to another emerges not from the nature of that progression itself, but from the descriptions we use to represent it. Nothing can seem jerky except what is represented as jerky. Paradoxically, our sense of continuity comes from our marvelous insensitivity to most kinds of changes rather than from any genuine perceptiveness. Existence seems continuous to us not because we continually experience what is happening in the present, but because we hold to our memories of how things were in the recent past. Without those short-term memories, all would seem entirely new at every instant, and we would have no sense at all of continuity or, for that matter, of existence.\nOne might suppose that it would be wonderful to possess a faculty of continual awareness. But such an affliction would be worse than useless, because the more frequently our higher-level agencies change their representations of reality, the harder it is for them to find significance in what they sense. The power of consciousness comes not from ceaseless change of state, but from having enough stability to discern significant changes in our surroundings. To notice change requires the ability to resist it. In order to sense what persists through time, one must be able to examine and compare descriptions from the recent past. We notice change in spite of change, not because of it.\nOur sense of constant contact with the world is not a genuine experience; instead, it is a form of immanence illusion. We have the sense of actuality when every question asked of our visual-systems is answered so swiftly that it seems as though those answers were already there. And that's what frame-arrays provide us with: once any frame fills its terminals, the terminals of the other frames in its array are also filled. When every change of view engages frames whose terminals are already filled, albeit only by default, then sight seems instantaneous.",
    "type": "article",
    "title": "25.4 the sense of continuity",
    "tags": [
      {
        "score": 0.6830323934555054,
        "sentiment": 0.676,
        "count": 3,
        "label": "continuity editing",
        "uri": "https://diffbot.com/entity/XSmJSycZKPsqvecZQwPz39w"
      },
      {
        "score": 0.5034672617912292,
        "sentiment": 0,
        "count": 1,
        "label": "Fly Me to the Moon",
        "uri": "https://diffbot.com/entity/XRZrQ-OahMnaptNVys6VVhw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 237883228572,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 17700929920,
    "gburl": "http://aurellem.org/society-of-mind/som-25.4.html-diffbotxyz3294501941",
    "lastCrawlTimeUTC": 1588764934,
    "timestamp": "Wed, 06 May 2020 11:35:34 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-685943917",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.4.html",
    "html": "<h2><em>11.4</em> innate geography</h2>\n<p>We've seen that touching nearby spots of skin will usually give rise to similar sensations: this is because the corresponding nerves run in parallel courses and thus cause similar activities inside the brain. The reverse is also usually true: the more similar two sensations are, the closer their origins in the skin. This has an important consequence:</p>\n<p>The nerve pathways that preserve the physical nearness relations of our skin-sensors can make it easy for inner agencies to discover corresponding nearnesses about the outer world of space.</p>\n<p>Moving your hand across an object tells you something about that object's shape. Imagine what must happen when a very young infant moves its hand across some object: each continuous motion produces a sequence of skin-sensor signals. Over time, various mapping agents can first use this information to learn, simply, which skin spots are nearest one another. Later, further layers of mapping agents could learn which skin spots lie between which others; this should be easy, too, because most small-scale motions tend to go in nearly straight lines. But then, since space itself is just a society of nearness relations between places, this is all the information we need to <em>reconstruct</em> the spatial structure of the skin. All this is in accord with a basic principle of mathematics:</p>\n<p>Suppose you were lost in some unknown space &mdash; and could only tell which pairs of points were close to one another. That would be enough for you to figure a great deal about the space. From that alone, you could deduce if you were in a world of two dimensions or three. You could tell where there were obstacles and boundaries, holes and tunnels and bridges, and so on. You could figure out the global layout of that world from just those local bits of information about nearnesses.</p>\n<p>It is a wonderful fact that, in principle, one can deduce the global geography of a space from nothing more than hints about which pairs of points lie near one another! But it is another matter to actually make such maps, and no one yet knows how the brain does this. To design a machine to accomplish such tasks, one could begin with a layer of <em>correlation agents,</em> one for each tiny patch of skin, each engineered to detect which other skin spots are most often aroused at nearly the same times; those will then be mapped as the nearest ones. A second layer of similar agents could then begin to make maps of larger regions, and several such layers would eventually assemble a sequence of maps on various scales, for representing several levels of detail.</p>\n<p>If brains do something of this sort, it might illuminate a problem that has troubled some philosophers: <em>Why do we all agree on what the outer world of space is like?</em> Why don't different people interpret space in different, alien ways? In principle it is mathematically possible for each person to conclude, for example, that the world is three-dimensional &mdash; rather than two-or four-dimensional &mdash; just from enough experience with nearby pairs of points. However, if the wires from the skin to the brain were shuffled and scrambled around too much, we would probably never get them straightened out because the actual calculations for doing such things would be beyond our capabilities.</p>",
    "text": "11.4 innate geography\nWe've seen that touching nearby spots of skin will usually give rise to similar sensations: this is because the corresponding nerves run in parallel courses and thus cause similar activities inside the brain. The reverse is also usually true: the more similar two sensations are, the closer their origins in the skin. This has an important consequence:\nThe nerve pathways that preserve the physical nearness relations of our skin-sensors can make it easy for inner agencies to discover corresponding nearnesses about the outer world of space.\nMoving your hand across an object tells you something about that object's shape. Imagine what must happen when a very young infant moves its hand across some object: each continuous motion produces a sequence of skin-sensor signals. Over time, various mapping agents can first use this information to learn, simply, which skin spots are nearest one another. Later, further layers of mapping agents could learn which skin spots lie between which others; this should be easy, too, because most small-scale motions tend to go in nearly straight lines. But then, since space itself is just a society of nearness relations between places, this is all the information we need to reconstruct the spatial structure of the skin. All this is in accord with a basic principle of mathematics:\nSuppose you were lost in some unknown space \u2014 and could only tell which pairs of points were close to one another. That would be enough for you to figure a great deal about the space. From that alone, you could deduce if you were in a world of two dimensions or three. You could tell where there were obstacles and boundaries, holes and tunnels and bridges, and so on. You could figure out the global layout of that world from just those local bits of information about nearnesses.\nIt is a wonderful fact that, in principle, one can deduce the global geography of a space from nothing more than hints about which pairs of points lie near one another! But it is another matter to actually make such maps, and no one yet knows how the brain does this. To design a machine to accomplish such tasks, one could begin with a layer of correlation agents, one for each tiny patch of skin, each engineered to detect which other skin spots are most often aroused at nearly the same times; those will then be mapped as the nearest ones. A second layer of similar agents could then begin to make maps of larger regions, and several such layers would eventually assemble a sequence of maps on various scales, for representing several levels of detail.\nIf brains do something of this sort, it might illuminate a problem that has troubled some philosophers: Why do we all agree on what the outer world of space is like? Why don't different people interpret space in different, alien ways? In principle it is mathematically possible for each person to conclude, for example, that the world is three-dimensional \u2014 rather than two-or four-dimensional \u2014 just from enough experience with nearby pairs of points. However, if the wires from the skin to the brain were shuffled and scrambled around too much, we would probably never get them straightened out because the actual calculations for doing such things would be beyond our capabilities.",
    "type": "article",
    "title": "11.4 innate geography",
    "docId": 118988505499,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 148445430171,
    "gburl": "http://aurellem.org/society-of-mind/som-11.4.html-diffbotxyz3656345520",
    "lastCrawlTimeUTC": 1588764779,
    "timestamp": "Wed, 06 May 2020 11:32:59 GMT"
  },
  {
    "sentiment": -0.553,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-293233560",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-27.2.html",
    "html": "<p>It would be wonderful never to make mistakes. One way would be to always have such perfect thoughts that none of them is ever wrong. But such perfection can't be reached. Instead we try, as best we can, to recognize our bad ideas before they do much harm. We can thus imagine two poles of self-improvement. On one side we try to stretch the range of the ideas we generate: this leads to more ideas, but also to more mistakes. On the other side, we try to learn not to repeat mistakes we've made before. All communities evolve some prohibitions and taboos to tell their members what they shouldn't do. That, too, must happen in our minds: we accumulate memories to tell ourselves what we shouldn't think.</p>\n<p>But how could we make an agent to prevent us from doing something that, in the past, has led to bad or ineffectual results? Ideally, that agent would keep us from even thinking that bad idea again. But that seems almost paradoxical, like telling someone, <em>Don't think about a monkey!</em> Yet there is a way to accomplish this. To see how it works,</p>\n<p>imagine the sequence of mental states that led to a certain mistake:</p>\n<p>We could prevent the undesired action from taking place by introducing an agent that recognizes the state which, in the past, preceded the undesired action.</p>\n<p>Suppressor-agents wait until you get a certain <em>bad idea.</em> Then they prevent your taking the corresponding action, and make you wait until you think of some alternative. If a suppressor could speak, it would say, <em>Stop thinking that!</em></p>\n<p>Suppressors could indeed prevent us from repeating actions that we've learned are bad. But it is inefficient to wait until we actually reach undesirable states, then have to <em>backtrack.</em> It would be more efficient to anticipate such lines of thought so that we never reach those states at all. In the next section we'll see how to do this by using agents called censors.</p>\n<p>Censor-agents need not wait until a certain bad idea occurs; instead, they intercept the states of mind that usually precede that thought. If a censor could speak, it would say, <em>Don't even begin to think that!</em></p>\n<p>Though censors were conceived of long ago by Sigmund Freud, they're scarcely mentioned in present-day psychology. I suspect that this is a serious oversight and that censors play fundamental roles in how we learn and how we think. Perhaps the trouble is that our censors work too well. For, naturally, it is easier for psychologists to study only what someone does &mdash; instead of what someone doesn't do.</p>",
    "text": "It would be wonderful never to make mistakes. One way would be to always have such perfect thoughts that none of them is ever wrong. But such perfection can't be reached. Instead we try, as best we can, to recognize our bad ideas before they do much harm. We can thus imagine two poles of self-improvement. On one side we try to stretch the range of the ideas we generate: this leads to more ideas, but also to more mistakes. On the other side, we try to learn not to repeat mistakes we've made before. All communities evolve some prohibitions and taboos to tell their members what they shouldn't do. That, too, must happen in our minds: we accumulate memories to tell ourselves what we shouldn't think.\nBut how could we make an agent to prevent us from doing something that, in the past, has led to bad or ineffectual results? Ideally, that agent would keep us from even thinking that bad idea again. But that seems almost paradoxical, like telling someone, Don't think about a monkey! Yet there is a way to accomplish this. To see how it works,\nimagine the sequence of mental states that led to a certain mistake:\nWe could prevent the undesired action from taking place by introducing an agent that recognizes the state which, in the past, preceded the undesired action.\nSuppressor-agents wait until you get a certain bad idea. Then they prevent your taking the corresponding action, and make you wait until you think of some alternative. If a suppressor could speak, it would say, Stop thinking that!\nSuppressors could indeed prevent us from repeating actions that we've learned are bad. But it is inefficient to wait until we actually reach undesirable states, then have to backtrack. It would be more efficient to anticipate such lines of thought so that we never reach those states at all. In the next section we'll see how to do this by using agents called censors.\nCensor-agents need not wait until a certain bad idea occurs; instead, they intercept the states of mind that usually precede that thought. If a censor could speak, it would say, Don't even begin to think that!\nThough censors were conceived of long ago by Sigmund Freud, they're scarcely mentioned in present-day psychology. I suspect that this is a serious oversight and that censors play fundamental roles in how we learn and how we think. Perhaps the trouble is that our censors work too well. For, naturally, it is easier for psychologists to study only what someone does \u2014 instead of what someone doesn't do.",
    "type": "article",
    "title": "27.2 suppressors",
    "tags": [
      {
        "score": 0.5152504444122314,
        "sentiment": 0,
        "count": 4,
        "label": "Roman censor",
        "uri": "https://diffbot.com/entity/rKf0jC588Pi2Duwn_6D0WtQ"
      }
    ],
    "docId": 211255968166,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 213525791132,
    "gburl": "http://aurellem.org/society-of-mind/som-27.2.html-diffbotxyz556194456",
    "lastCrawlTimeUTC": 1588764753,
    "timestamp": "Wed, 06 May 2020 11:32:33 GMT"
  },
  {
    "sentiment": 0.337,
    "images": [
      {
        "naturalHeight": 118,
        "width": 379,
        "diffbotUri": "image|3|1690365470",
        "url": "http://aurellem.org/society-of-mind/illus/ch17/17-2.png",
        "naturalWidth": 379,
        "primary": true,
        "height": 118
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-557444664",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.1.html",
    "html": "<p>Up to this point we've portrayed the mind as made of scattered fragments of machinery. But we adults rarely see ourselves that way; we have more sense of unity. In the next few sections we'll speculate that this coherency is acquired over many <em>stages of development.</em> Each new stage first works under the guidance of previous stages, to acquire some knowledge, values, and goals. Then it proceeds to change its role and becomes a teacher to subsequent stages.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch17/17-2.png\"/></figure>\n<p>How could an early stage teach anything to a later one when it knows less than its student does? As every teacher knows, this is not as hard as it might seem. For one thing, it is usually easier to recognize a solution to a problem than to discover a solution; this is what we called the <em>puzzle principle.</em> A teacher need not know how to solve a problem to be able to reward a student for doing so or to help the student search for solutions by imparting ways to sense when progress has been made. Even better is for the teacher to impart new goals to the student.</p>\n<p>How could an early stage of development affect the goals of a later stage? One simple way would be to give each later stage some access to the goals of earlier stages; however, those early goals would then remain infantile. How could later stages develop more advanced goals? Shortly we'll see an astonishing answer: it is not necessary to formulate more advanced goals at <em>higher levels</em> of organization because they are likely to develop spontaneously, as subgoals of relatively simple goals.</p>\n<p>In any case, it wouldn't be safe to send the student into the world equipped with systems that have not yet been tried and tested. A safer strategy would be to keep each new stage suppressed &mdash; that is, incapable of controlling the child's actual behavior &mdash; until it passes tests to verify that it is at least as capable as its predecessor. This could explain some of those apparently sudden <em>spurts</em> in our children's development &mdash; for example, in episodes of rapid growth of language skills. That apparent speed could be illusory if it were merely the end result of longer, hidden projects carried out silently inside the mind.</p>\n<p>Returning to our sense of Self, how could so many steps and stages lead to any sense of unity? Why wouldn't they lead us, instead, to feel increasingly fragmentary and dispersed? I suspect the secret is that after each old stage's work is done, its structure still remains available for further use. These remnants of our prior selves provide us with a powerful resource: whenever one's present mind gets confused, it can exploit what once was used by earlier minds. Even though we weren't as smart then as we are now, we can be sure that every stage once had, in its turn, some workable ways to manage things.</p>\n<p>One's present personality cannot share many of the thoughts of all one's older personalities &mdash; and yet it has some sense that they exist. This is one reason why we feel that we possess an inner Self &mdash; a sort of ever-present person-friend, inside the mind, whom we can always ask for help.</p>",
    "text": "Up to this point we've portrayed the mind as made of scattered fragments of machinery. But we adults rarely see ourselves that way; we have more sense of unity. In the next few sections we'll speculate that this coherency is acquired over many stages of development. Each new stage first works under the guidance of previous stages, to acquire some knowledge, values, and goals. Then it proceeds to change its role and becomes a teacher to subsequent stages.\nHow could an early stage teach anything to a later one when it knows less than its student does? As every teacher knows, this is not as hard as it might seem. For one thing, it is usually easier to recognize a solution to a problem than to discover a solution; this is what we called the puzzle principle. A teacher need not know how to solve a problem to be able to reward a student for doing so or to help the student search for solutions by imparting ways to sense when progress has been made. Even better is for the teacher to impart new goals to the student.\nHow could an early stage of development affect the goals of a later stage? One simple way would be to give each later stage some access to the goals of earlier stages; however, those early goals would then remain infantile. How could later stages develop more advanced goals? Shortly we'll see an astonishing answer: it is not necessary to formulate more advanced goals at higher levels of organization because they are likely to develop spontaneously, as subgoals of relatively simple goals.\nIn any case, it wouldn't be safe to send the student into the world equipped with systems that have not yet been tried and tested. A safer strategy would be to keep each new stage suppressed \u2014 that is, incapable of controlling the child's actual behavior \u2014 until it passes tests to verify that it is at least as capable as its predecessor. This could explain some of those apparently sudden spurts in our children's development \u2014 for example, in episodes of rapid growth of language skills. That apparent speed could be illusory if it were merely the end result of longer, hidden projects carried out silently inside the mind.\nReturning to our sense of Self, how could so many steps and stages lead to any sense of unity? Why wouldn't they lead us, instead, to feel increasingly fragmentary and dispersed? I suspect the secret is that after each old stage's work is done, its structure still remains available for further use. These remnants of our prior selves provide us with a powerful resource: whenever one's present mind gets confused, it can exploit what once was used by earlier minds. Even though we weren't as smart then as we are now, we can be sure that every stage once had, in its turn, some workable ways to manage things.\nOne's present personality cannot share many of the thoughts of all one's older personalities \u2014 and yet it has some sense that they exist. This is one reason why we feel that we possess an inner Self \u2014 a sort of ever-present person-friend, inside the mind, whom we can always ask for help.",
    "type": "article",
    "title": "17.1 sequences of teaching-selves",
    "tags": [
      {
        "score": 0.6945921778678894,
        "sentiment": 0,
        "count": 1,
        "label": "sequence",
        "uri": "https://diffbot.com/entity/XOrzabgzIMHCInWn8sTi8Ww",
        "rdfTypes": ["http://dbpedia.org/ontology/Activity"]
      },
      {
        "score": 0.6089882850646973,
        "sentiment": 0.616,
        "count": 5,
        "label": "student",
        "uri": "https://diffbot.com/entity/XukqhMkrmO3yJfe0nEfa2pA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6013939380645752,
        "sentiment": 0.928,
        "count": 4,
        "label": "teacher",
        "uri": "https://diffbot.com/entity/X68Svpe4cMVuxINSKHJg2-g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5687013864517212,
        "sentiment": 0.936,
        "count": 11,
        "label": "stage",
        "uri": "https://diffbot.com/entity/X4hRmk0AAM2i5tG48Cjnsxg"
      }
    ],
    "docId": 229853528485,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 110405157260,
    "gburl": "http://aurellem.org/society-of-mind/som-17.1.html-diffbotxyz4116463092",
    "lastCrawlTimeUTC": 1588764848,
    "timestamp": "Wed, 06 May 2020 11:34:08 GMT"
  },
  {
    "sentiment": 0.029,
    "humanLanguage": "fr",
    "diffbotUri": "article|3|804505476",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-3.html",
    "text": "",
    "type": "article",
    "title": "3 conflict and compromise",
    "docId": 256040763805,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 53610021289,
    "gburl": "http://aurellem.org/society-of-mind/som-3.html-diffbotxyz3132671226",
    "lastCrawlTimeUTC": 1588764816,
    "timestamp": "Wed, 06 May 2020 11:33:36 GMT"
  },
  {
    "sentiment": 0.986,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1634511958",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-4.6.html",
    "html": "<blockquote> Those who really seek the path to Enlightenment dicate terms to their mind. Then they proceed with strong determination. &mdash;Buddha </blockquote>\n<p>The episode of Professor Challenger showed just one way we can control ourselves: by exploiting an emotional aversion in order to accomplish an intellectual purpose. Consider all the other kinds of tricks we use to try to force ourselves to work when we're tired or distracted.</p>\n<p>WILLPOWER: Tell yourself, <em>Don't give in to that,</em> or, <em>Keep on trying.</em></p>\n<p>Such self-injunctions can work at first &mdash; but finally they always fail, as though some engine in the mind runs out of fuel. Another style of self-control involves more physical activity:</p>\n<p>ACTIVITY: Move around. Exercise. Inhale. Shout.</p>\n<p>Certain physical acts are peculiarly effective, especially the facial expressions involved in social communication: they affect the sender as much as the recipient.</p>\n<p>EXPRESSION: Set jaw. Stiffen upper lip. Furrow brow.</p>\n<p>Another kind of stimulating act is moving to a stimulating place. And we often perform actions that directly change the brain's chemical environment.</p>\n<p>CHEMISTRY: Take coffee, amphetamines, or other brain-affecting drugs.</p>\n<p>Then there are actions in the mind with which we set up thoughts and fantasies that move our own emotions, arousing hopes and fears through self-directed offers, bribes, and even threats.</p>\n<p>EMOTION: <em>If I win, there's much to gain, but more to lose if I fail!</em></p>\n<p>Perhaps most powerful of all are those actions that promise gain or loss of the regard of certain special persons.</p>\n<p>ATTACHMENT: Imagine admiration if you succeed &mdash; or disapproval if you fail &mdash; especially from those to whom you are attached.</p>\n<p>So many schemes for self-control! How do we choose which ones to use? There isn't any easy way. Self-discipline takes years to learn; it grows inside us stage by stage.</p>",
    "text": "Those who really seek the path to Enlightenment dicate terms to their mind. Then they proceed with strong determination. \u2014Buddha\nThe episode of Professor Challenger showed just one way we can control ourselves: by exploiting an emotional aversion in order to accomplish an intellectual purpose. Consider all the other kinds of tricks we use to try to force ourselves to work when we're tired or distracted.\nWILLPOWER: Tell yourself, Don't give in to that, or, Keep on trying.\nSuch self-injunctions can work at first \u2014 but finally they always fail, as though some engine in the mind runs out of fuel. Another style of self-control involves more physical activity:\nACTIVITY: Move around. Exercise. Inhale. Shout.\nCertain physical acts are peculiarly effective, especially the facial expressions involved in social communication: they affect the sender as much as the recipient.\nEXPRESSION: Set jaw. Stiffen upper lip. Furrow brow.\nAnother kind of stimulating act is moving to a stimulating place. And we often perform actions that directly change the brain's chemical environment.\nCHEMISTRY: Take coffee, amphetamines, or other brain-affecting drugs.\nThen there are actions in the mind with which we set up thoughts and fantasies that move our own emotions, arousing hopes and fears through self-directed offers, bribes, and even threats.\nEMOTION: If I win, there's much to gain, but more to lose if I fail!\nPerhaps most powerful of all are those actions that promise gain or loss of the regard of certain special persons.\nATTACHMENT: Imagine admiration if you succeed \u2014 or disapproval if you fail \u2014 especially from those to whom you are attached.\nSo many schemes for self-control! How do we choose which ones to use? There isn't any easy way. Self-discipline takes years to learn; it grows inside us stage by stage.",
    "type": "article",
    "title": "4.6 self-control",
    "tags": [
      {
        "score": 0.780267596244812,
        "sentiment": 0,
        "count": 1,
        "label": "Professor Challenger",
        "uri": "https://diffbot.com/entity/XobmEtwweOk688vIQ90PUWw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6086299419403076,
        "sentiment": 0,
        "count": 0,
        "label": "Self",
        "uri": "https://diffbot.com/entity/XVTcSMEEgNYi4-xRQmuUv2g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Language",
          "http://dbpedia.org/ontology/ProgrammingLanguage"
        ]
      },
      {
        "score": 0.6013587117195129,
        "sentiment": 0.358,
        "count": 1,
        "label": "Enlightenment",
        "uri": "https://diffbot.com/entity/X9K8a24JmNsSR-_9MQEr6Rw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/TelevisionShow"
        ]
      }
    ],
    "docId": 53002715581,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 116997833128,
    "gburl": "http://aurellem.org/society-of-mind/som-4.6.html-diffbotxyz883832234",
    "lastCrawlTimeUTC": 1588764732,
    "timestamp": "Wed, 06 May 2020 11:32:12 GMT"
  },
  {
    "sentiment": 0.37,
    "images": [
      {
        "naturalHeight": 184,
        "width": 417,
        "diffbotUri": "image|3|-1273068414",
        "url": "http://aurellem.org/society-of-mind/illus/ch29/29-6.png",
        "naturalWidth": 417,
        "primary": true,
        "height": 184
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1505255491",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-29.3.html",
    "html": "<p>What enables us to comprehend <em>Mary gives Jack the kite</em> in so many ways at once? Different meanings don't conflict when they apply to separate realms &mdash; but that can't be quite what's happening here, since the physical, social, and mental realms are closely linked in many ways. So now I'll argue just the opposite, that these meanings are so similar they don't conflict! Here is my hypothesis about what holds together all these aspects of our thoughts:</p>\n<p>Many of our higher level conceptual-frames are really parallel arrays of analogous frames, each active in a different realm.</p>\n<p>Consider all the different roles played by the Actor pronome of our sentence. In the physical realm, the Origin of give is Mary's hand. In the possessional realm of <em>give and take,</em> that Origin is in Mary's estate &mdash; since Mary can only give Jack what she owns. Similarly, in the physical realm, it is the kite itself that moves from Mary's hand to Jack's; however, in the realm of estates, the kite's ownership is what <em>changes hands.</em></p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch29/29-6.png\"/></figure>\n<p>This suggests that certain pronomes can operate in several different realms at once. Let's call them <em>paranomes</em> to emphasize their parallel activities. When the language-agency activates some polynemes and paranomes, these agents run crosswise through the agencies of various realms to arouse several processes and frames at the same time; these correspond to different interpretations, in different realms, of the same phrase or sentence. Then, because each major agency contains its own memory-control system, the agencies within each realm can simultaneously apply their own methods for dealing with the corresponding aspect of the common topic of concern. In this way, a single language-phrase can at the same time evoke different processes involved with social dispositions, spatial images, poetical fancies, musical themes, mathematical structures &mdash; or any other assortment of types of thought that don't interfere too much with one another.</p>\n<p>This is not to say that all these different modes of thought will proceed independently of one another. Whenever any process gains momentary control over a paranome, many other processes can be affected. For example, one agency's memory-control process might thus cause the agencies in several other realms simultaneously to <em>blink</em> on and off their Origin and Destination paranomes. This would force the agencies active in each of those realms to focus upon whichever types of differences they then discern; then, in between such episodes, each agency can apply its own way of thinking to the corresponding topic, difference, or relationship. By using these cross-connecting polynemes and paranomes, the activity in each realm can proceed sometimes independently, yet at other times influence and be affected by what happens in the other realms.</p>",
    "text": "What enables us to comprehend Mary gives Jack the kite in so many ways at once? Different meanings don't conflict when they apply to separate realms \u2014 but that can't be quite what's happening here, since the physical, social, and mental realms are closely linked in many ways. So now I'll argue just the opposite, that these meanings are so similar they don't conflict! Here is my hypothesis about what holds together all these aspects of our thoughts:\nMany of our higher level conceptual-frames are really parallel arrays of analogous frames, each active in a different realm.\nConsider all the different roles played by the Actor pronome of our sentence. In the physical realm, the Origin of give is Mary's hand. In the possessional realm of give and take, that Origin is in Mary's estate \u2014 since Mary can only give Jack what she owns. Similarly, in the physical realm, it is the kite itself that moves from Mary's hand to Jack's; however, in the realm of estates, the kite's ownership is what changes hands.\nThis suggests that certain pronomes can operate in several different realms at once. Let's call them paranomes to emphasize their parallel activities. When the language-agency activates some polynemes and paranomes, these agents run crosswise through the agencies of various realms to arouse several processes and frames at the same time; these correspond to different interpretations, in different realms, of the same phrase or sentence. Then, because each major agency contains its own memory-control system, the agencies within each realm can simultaneously apply their own methods for dealing with the corresponding aspect of the common topic of concern. In this way, a single language-phrase can at the same time evoke different processes involved with social dispositions, spatial images, poetical fancies, musical themes, mathematical structures \u2014 or any other assortment of types of thought that don't interfere too much with one another.\nThis is not to say that all these different modes of thought will proceed independently of one another. Whenever any process gains momentary control over a paranome, many other processes can be affected. For example, one agency's memory-control process might thus cause the agencies in several other realms simultaneously to blink on and off their Origin and Destination paranomes. This would force the agencies active in each of those realms to focus upon whichever types of differences they then discern; then, in between such episodes, each agency can apply its own way of thinking to the corresponding topic, difference, or relationship. By using these cross-connecting polynemes and paranomes, the activity in each realm can proceed sometimes independently, yet at other times influence and be affected by what happens in the other realms.",
    "type": "article",
    "title": "29.3 paranomes",
    "docId": 77633798586,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 81781064116,
    "gburl": "http://aurellem.org/society-of-mind/som-29.3.html-diffbotxyz4256814966",
    "lastCrawlTimeUTC": 1588764612,
    "timestamp": "Wed, 06 May 2020 11:30:12 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1367569927",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-postscript.html",
    "html": "<blockquote> Never speak more clearly than you think. &mdash;Jeremy Bernstein </blockquote>\n<p>This book assumes that any brain, machine, or other thing that has a mind must be composed of smaller things that cannot think at all. The structure of the book itself reflects this view: each page explores a theory or idea that exploits what other pages do. Some readers might prefer a more usual form of story plot. I tried to do that several times, but it never seemed to work; each way I tried to line things up left too many thoughts that would not fit. A mind is too complex to fit the mold of narratives that start out here and end up there; a human intellect depends upon the connections in a tangled web &mdash; which simply wouldn't work at all if it were neatly straightened out.</p>\n<p>Many psychologists dream of describing minds so economically that psychology would become as simple and precise as physics. But one must not confuse reality with dreams. It was not the ambitions of the physicists that made it possible to describe so much of the world in terms of so few and simple principles; that was because of the nature of our universe. But the operations of our minds do not depend on similarly few and simple laws, because our brains have accumulated many different mechanisms over aeons of evolution. This means that psychology can never be as simple as physics, and any simple theory of mind would be bound to miss most of the <em>big picture.</em> The science of psychology will be handicapped until we develop an overview with room for a great many smaller theories.</p>\n<p>To assemble the overview suggested in this book, I had to make literally hundreds of assumptions. Some scientists might object to this on the ground that successful sciences like physics and chemistry have found it more productive to develop theories that make the fewest assumptions, eliminating everything that does not seem absolutely essential. But until we have a more coherent framework for psychology, it will remain too early for the task of weeding out unproved hypotheses or for trying to show that one theory is better than another &mdash; since none of our present-day theories seem likely to survive very long in any case. Before we can have an image of the forest of psychology, we'll have to imagine more of its trees and restrain ourselves from simplifying them to death. Instead, we have to make ourselves complicated enough to deal with what is actually there.</p>\n<p>It is scarcely a century since people started to think effectively about the natures of the brainmachines that manufacture thoughts. Before that, those who tried to speculate about this were handicapped on one side by their failure to do experiments, particularly with young children, and on the other side by their lack of concepts for describing complicated machinery. Now, for the first time, mankind has accumulated enough conceptual tools to begin comprehending machines with thousands of parts. However, we are only beginning to deal with machines that have millions of parts and we have barely started to acquire the concepts that we'll need to understand the billion-part machines that constitute our brains. New kinds of problems always arise when one encounters systems built on larger, less familiar scales.</p>\n<p>Since most of the statements in this book are speculations, it would have been too tedious to mention this on every page. Instead, I did the opposite &mdash; by taking out all words like <em>possibly</em> and deleting every reference to scientific evidence. Accordingly, this book should be read less as a text of scientific scholarship and more as an adventure story for the imagination. Each idea should be seen not as a firm hypothesis about the mind, but as another implement to keep inside one's toolbox for making theories of the mind. Indeed, there is a sense in which that can be the only realistic way to think about psychology &mdash; since every particular person's mind develops as a huge machine that grows in a somewhat different way. Are minds machines? Of that, I've raised no doubt at all but have only asked, what kind of machines? And though most people still consider it degrading to be regarded as machines, I hope this book will make them entertain, instead, the thought of how wonderful it is to be machines with such marvelous powers.</p>\n<p>Scientists like to credit those who first discovered each idea. But the central concept of this book, that the mind is a society of many smaller mechanisms, involved so many years of work to bring it to its present form that I can mention only a few of the people who had the most influence on it. In this research I shared the greatest privilege a human mind can have: to work on new ideas together with the foremost intellects of one's time. As a student at Harvard, I immersed myself in mathematics and psychology and attached myself to two great young scientists, the mathematician Andrew Gleason and the psychologist George A. Miller. This was the era of the scientific movement that was later called cybernetics, and I was especially entranced with the works of Nicholas Rashevsky and of Warren McCulloch, who were making the first theories of how assemblies of simple cell-machines could do such things as recognize objects and remember what they'd seen. By the time I started graduate school in mathematics at Princeton in 1950, I had a clear enough idea about how to make a multi-agent learning machine. George Miller obtained funds for building it; this was the Snarc machine of chapter 7. Constructed with the help of a fellow student, Dean Edmonds, it managed to learn in certain ways, but its limitations convinced me that a more versatile <em>thinking machine</em> would have to exploit many other principles.</p>\n<p>My teachers in the golden age of mathematics at Princeton were not particularly interested in psychology, but the ways of thought are more important than the subject matter, and I learned new mental strategies from Albert Tucker, Ralph Fox, Solomon Lefshetz, John Tukey, Salomon Bochner, and John von Neumann. I learned even more from my own generation of students at Princeton: particularly from John Nash, Lloyd Shapley, Martin Shubik, and John McCarthy. In 1954 I returned to Harvard as a Junior Fellow of the Harvard Society of Fellows, with no obligation but to pursue whatever goal seemed most important. There seemed no way to get around the apparent limitations of low-level, distributed-connection learning machines, so I turned toward a new theory being pioneered by Ray Solomonoff, about generalizing from experience. I attached myself to Warren McCulloch and Oliver Selfridge, with whom I worked most closely of all until becoming a professor of mathematics at MIT. It was from them that I derived my image of how to make a laboratory work.</p>\n<p>In 1959, John McCarthy came to MIT from Dartmouth, and we started the MIT Artificial Intelligence Project. We agreed that the most critical problem was of how minds do common-sense reasoning. McCarthy was more concerned with establishing logical and mathematical foundations for reasoning, while I was more involved with theories of how we actually reason using pattern recognition and analogy. This combination of theoretical and practical research attracted students of great ability, and our laboratory had an atmosphere that combined mathematical power with engineering adventure; this led not only to new theories of computation, but also to developing some of the very first automatic robots. In 1963, McCarthy left to start a new AI laboratory at Stanford, and now there were three principal centers of research in Artificial Intelligence, including the one that Allen Newell and Herbert Simon had started earlier at Carnegie-Mellon University. A fourth center soon emerged at Stanford Research Laboratory, and we all worked closely together.</p>\n<p>The money to support the people and equipment for this work came mainly from an office of the Advanced Research Projects Agency concerned with information processing technology. This office was directed, in effect, by the scientists themselves, initially by Dr. J.C.R.</p>\n<p>Licklider, who had been my teacher and friend when I was a student at Harvard. Licklider had already organized a research center at the Bolt, Beranek, and Newman company in Cambridge, Massachusetts, and McCarthy and I and several of our students had worked closely with that group for several years. Later, when Licklider returned to become a professor at MIT, the Information Processing Technology Office was taken over successively by Lawrence G. Roberts and Ivan Sutherland (who had been students of ours at MIT) and then by Robert Taylor and Robert Kahn &mdash; all of whom made important intellectual contributions. The actual details of all these research contracts were managed in the Office of Naval Research by Marvin Denicoff, whose vision of the future had a substantial influence on the entire field. My own research was supported by the ONR over an even longer period, since it had previously financed my graduate studies in topology at Princeton, and, subsequently, Denicoff's successor, Alan Meyrowitz, supported my research during the completion of this book.</p>\n<p>Jerome Wiesner and Philip Morse of MIT obtained the resources for our first laboratory. Our development at MIT was encouraged by William Ted Martin, Norman Levinson, Witold Hurewicz, Norbert Weiner, Claude Shannon, Peter Elias, and Robert Fano. I was given the privilege of sharing with Shannon the endowed chair of Donner Professor of Science at MIT and enjoyed the support of many other people and organizations over the years: John Williams, Paul Armer, and Merril Flood enabled me to work with Newell, Shaw, and Simon at the Rand Corporation; Oliver Selfridge and Gerald Dinneen encouraged research at MIT's Lincoln Laboratory; Michel Gouilloud supported my work from the Schlumberger Corporation; Edward David provided support from Exxon; and Alan Kay supported many of our students with funding from (successively) the Xerox, Atari, and Apple corporations. For several years, the Thinking Machines Corporation has supported both this research and the development of a new type of computer called the Connection Machine &mdash; designed by my student Danny Hillis for embodying societies of mind.</p>\n<p>Most of all, I want to acknowledge the contributions to this book of Seymour Papert, who came to MIT in 1963 after five years of studying child development with Jean Piaget in Geneva. Papert and I worked so well together that for a decade we supervised the laboratory jointly, each able at any time to leave the other to decide what should be done. Together we evolved new mathematical techniques, designed laboratory experiments, built computer hardware and software, and supervised the same students. Such a partnership could not have worked so well had we not both developed in similar intellectual directions before we met; we were both involved with the same areas of mathematics, with similar concerns about machinery, and with similar attitudes about psychology. One of our projects was to build a machine that could see well enough to use mechanical hands for solving real-world problems; this was the origin of Builder and the insights that emerged from it. In trying to make that robot see, we found that no single method ever worked well by itself. For example, the robot could rarely discern an object's shape by using vision alone; it also had to exploit other types of knowledge about which kinds of objects were likely to be seen. This experience impressed on us the idea that only a society of different kinds of processes could possibly suffice. Papert and I worked together not only on robotic machines, but in many other areas; for example, we spent several years developing a new mathematical theory for the then mysterious Perceptron type of learning machine. In the middle 1970s Papert and I tried together to write a book about societies of mind but abandoned the attempt when it became clear that the ideas were not mature enough. The results of that collaboration shaped many earlier sections of this book.</p>\n<p>Eventually Papert and I both turned away from large-scale scientific enterprises toward somewhat different individual goals, and we imposed the directorship of our laboratory upon one of our most original and productive students, Patrick Winston &mdash; who first worked out the idea of making uniframes. Papert went on to develop a host of new theories about mental development and education; these led to the computer language LOGO and to many other concepts that started to enter the mainstream of educational thought over the next decade. I focused my concern on searching for better theories about the little world of how a child might learn to build with blocks. The parts of the puzzle that form this book began to fit together in my mind in the mid-1970s, around the concept of frame-array, and this eventually led to the theories about communication-lines, K-lines, and level-bands, and then, during the book's final stages, to the ideas about pronomes, polynemes, and cross-realm correspondences.</p>\n<p>As for the manuscript itself, Bradley Marx read through every draft, comparing each version with earlier ones, helping to maintain clarity, stylistic coherency, and especially protecting good ideas from destructive revisionary impulses. This was hard because the early manuscript was more than twice its present length. Robin Lakoff suggested neutering the English; this seemed at first impossible but soon became quite natural. Theodore Sturgeon reviewed an early draft; I wish he had lived to see it now. Kenneth Haase, Betty Dexter, and Tom Beckman made innumerable suggestions and corrections. Successive drafts were reviewed by Danny Hillis, Steve Bagley, Marvin Denicoff, Charlotte Minsky, Michel Gouilloud, Justin Lieber, Philip Agre, David Wallace, Ben Kuipers, Peter de Jong, and Sona Vogel. Richard Feynman contributed a variety of insights about memory and parallel processing. David Yarmush helped to organize the book into sections, to smooth out the transitions, and to establish the gradient wherein the words begin with commonsense meanings and gradually become more technical. Bob Whittinghill made many suggestions about language as well as about psychology. Douglas Hofstadter evaluated the entire theory, forcing me to make several substantial changes. Michael Crichton made many technical suggestions and helped me to refine the early chapters.</p>\n<p>Russell Noftsker and Tom Callahan made substantial engineering contributions to our work. Hosts of ideas came from students at MIT, notably Howard Austin, Manuel Blum, Danny Bobrow, Eugene Charniak, Henry Ernst, Tom Evans, Scott Fahlman, Ira Goldstein, William Gosper, Richard Greenblatt, Adolfo Guzman, Kenneth Haase, William Henneman, Carl Hewitt, Danny Hillis, Jack Holloway, Tom Knight, William Martin, Joel Moses, Bertram Raphael, Larry Roberts, James Slagle, Jerry Sussman, Ivan Sutherland, David Waltz, Terry Winograd, Patrick Winston, and many others. Countless other thoughts came from working at various times with Maryann Amacher, Gregory Benford, Terry Beyer, Woodrow Bledsoe, Mortimer Casson, Edward Feigenbaum, Edward Fredkin, Arnold Griffith, Louis Hodes, Berthold Horn, Joel Isaacson, Russell Kirsch, David Kirsh, Robert Lawler, Justin Leiber, Douglas Lenat, Jerome Lettvin, David MacDonald, Curtis Marx, Hans Moravec, Stewart Nelson, Nils Nillsson, Donald Norman, Walter Pitts, Jerry Pournelle, Charles Rosen, Carl Sagan, Roger Schank,</p>\n<p>Robert Sheckley, Stephen Smoliar, Cynthia Solomon, Ray Solomonoff, Luc Steels, Warren Teitelman, and Graziella Tonfoni. I wish I could acknowledge the inspirations of all the friends of earlier years, particularly W. Ross Ashby, Thomas Etter, Ned Feder, Heinz von Foerster, Donald Hebb, John Hollander, Arnold Honig, Gordon Pask, Roland Silver, Jan Syrjala, Carroll Williams, Bertram Wolfe, David Yarmush &mdash; and of all the teachers of my youth, particularly Dudley Fitts, Ruth Gordon, Alexander Joseph, Edward Lepowsky, and Herbert Zim. My development was also strongly influenced first by the writing and later by the friendship of Arthur C. Clarke, Robert Heinlein, Frederick Pohl, and most of all by Isaac Asimov.</p>\n<p>Of course, the deepest influence on my style of thought came from my parents, Henry Minsky and Fannie Reiser. My wife, Gloria Rudisch, our children Margaret, Henry, and Juliana (who drew the illustrations and sometimes changed the text to make them fit), and my sister Ruth all helped to shape this book. My sister Charlotte also lives between these lines, for even in our childhood, she was a powerful artist and critic, and her dreams became the meanings of my ordinary words.</p>",
    "text": "Never speak more clearly than you think. \u2014Jeremy Bernstein\nThis book assumes that any brain, machine, or other thing that has a mind must be composed of smaller things that cannot think at all. The structure of the book itself reflects this view: each page explores a theory or idea that exploits what other pages do. Some readers might prefer a more usual form of story plot. I tried to do that several times, but it never seemed to work; each way I tried to line things up left too many thoughts that would not fit. A mind is too complex to fit the mold of narratives that start out here and end up there; a human intellect depends upon the connections in a tangled web \u2014 which simply wouldn't work at all if it were neatly straightened out.\nMany psychologists dream of describing minds so economically that psychology would become as simple and precise as physics. But one must not confuse reality with dreams. It was not the ambitions of the physicists that made it possible to describe so much of the world in terms of so few and simple principles; that was because of the nature of our universe. But the operations of our minds do not depend on similarly few and simple laws, because our brains have accumulated many different mechanisms over aeons of evolution. This means that psychology can never be as simple as physics, and any simple theory of mind would be bound to miss most of the big picture. The science of psychology will be handicapped until we develop an overview with room for a great many smaller theories.\nTo assemble the overview suggested in this book, I had to make literally hundreds of assumptions. Some scientists might object to this on the ground that successful sciences like physics and chemistry have found it more productive to develop theories that make the fewest assumptions, eliminating everything that does not seem absolutely essential. But until we have a more coherent framework for psychology, it will remain too early for the task of weeding out unproved hypotheses or for trying to show that one theory is better than another \u2014 since none of our present-day theories seem likely to survive very long in any case. Before we can have an image of the forest of psychology, we'll have to imagine more of its trees and restrain ourselves from simplifying them to death. Instead, we have to make ourselves complicated enough to deal with what is actually there.\nIt is scarcely a century since people started to think effectively about the natures of the brainmachines that manufacture thoughts. Before that, those who tried to speculate about this were handicapped on one side by their failure to do experiments, particularly with young children, and on the other side by their lack of concepts for describing complicated machinery. Now, for the first time, mankind has accumulated enough conceptual tools to begin comprehending machines with thousands of parts. However, we are only beginning to deal with machines that have millions of parts and we have barely started to acquire the concepts that we'll need to understand the billion-part machines that constitute our brains. New kinds of problems always arise when one encounters systems built on larger, less familiar scales.\nSince most of the statements in this book are speculations, it would have been too tedious to mention this on every page. Instead, I did the opposite \u2014 by taking out all words like possibly and deleting every reference to scientific evidence. Accordingly, this book should be read less as a text of scientific scholarship and more as an adventure story for the imagination. Each idea should be seen not as a firm hypothesis about the mind, but as another implement to keep inside one's toolbox for making theories of the mind. Indeed, there is a sense in which that can be the only realistic way to think about psychology \u2014 since every particular person's mind develops as a huge machine that grows in a somewhat different way. Are minds machines? Of that, I've raised no doubt at all but have only asked, what kind of machines? And though most people still consider it degrading to be regarded as machines, I hope this book will make them entertain, instead, the thought of how wonderful it is to be machines with such marvelous powers.\nScientists like to credit those who first discovered each idea. But the central concept of this book, that the mind is a society of many smaller mechanisms, involved so many years of work to bring it to its present form that I can mention only a few of the people who had the most influence on it. In this research I shared the greatest privilege a human mind can have: to work on new ideas together with the foremost intellects of one's time. As a student at Harvard, I immersed myself in mathematics and psychology and attached myself to two great young scientists, the mathematician Andrew Gleason and the psychologist George A. Miller. This was the era of the scientific movement that was later called cybernetics, and I was especially entranced with the works of Nicholas Rashevsky and of Warren McCulloch, who were making the first theories of how assemblies of simple cell-machines could do such things as recognize objects and remember what they'd seen. By the time I started graduate school in mathematics at Princeton in 1950, I had a clear enough idea about how to make a multi-agent learning machine. George Miller obtained funds for building it; this was the Snarc machine of chapter 7. Constructed with the help of a fellow student, Dean Edmonds, it managed to learn in certain ways, but its limitations convinced me that a more versatile thinking machine would have to exploit many other principles.\nMy teachers in the golden age of mathematics at Princeton were not particularly interested in psychology, but the ways of thought are more important than the subject matter, and I learned new mental strategies from Albert Tucker, Ralph Fox, Solomon Lefshetz, John Tukey, Salomon Bochner, and John von Neumann. I learned even more from my own generation of students at Princeton: particularly from John Nash, Lloyd Shapley, Martin Shubik, and John McCarthy. In 1954 I returned to Harvard as a Junior Fellow of the Harvard Society of Fellows, with no obligation but to pursue whatever goal seemed most important. There seemed no way to get around the apparent limitations of low-level, distributed-connection learning machines, so I turned toward a new theory being pioneered by Ray Solomonoff, about generalizing from experience. I attached myself to Warren McCulloch and Oliver Selfridge, with whom I worked most closely of all until becoming a professor of mathematics at MIT. It was from them that I derived my image of how to make a laboratory work.\nIn 1959, John McCarthy came to MIT from Dartmouth, and we started the MIT Artificial Intelligence Project. We agreed that the most critical problem was of how minds do common-sense reasoning. McCarthy was more concerned with establishing logical and mathematical foundations for reasoning, while I was more involved with theories of how we actually reason using pattern recognition and analogy. This combination of theoretical and practical research attracted students of great ability, and our laboratory had an atmosphere that combined mathematical power with engineering adventure; this led not only to new theories of computation, but also to developing some of the very first automatic robots. In 1963, McCarthy left to start a new AI laboratory at Stanford, and now there were three principal centers of research in Artificial Intelligence, including the one that Allen Newell and Herbert Simon had started earlier at Carnegie-Mellon University. A fourth center soon emerged at Stanford Research Laboratory, and we all worked closely together.\nThe money to support the people and equipment for this work came mainly from an office of the Advanced Research Projects Agency concerned with information processing technology. This office was directed, in effect, by the scientists themselves, initially by Dr. J.C.R.\nLicklider, who had been my teacher and friend when I was a student at Harvard. Licklider had already organized a research center at the Bolt, Beranek, and Newman company in Cambridge, Massachusetts, and McCarthy and I and several of our students had worked closely with that group for several years. Later, when Licklider returned to become a professor at MIT, the Information Processing Technology Office was taken over successively by Lawrence G. Roberts and Ivan Sutherland (who had been students of ours at MIT) and then by Robert Taylor and Robert Kahn \u2014 all of whom made important intellectual contributions. The actual details of all these research contracts were managed in the Office of Naval Research by Marvin Denicoff, whose vision of the future had a substantial influence on the entire field. My own research was supported by the ONR over an even longer period, since it had previously financed my graduate studies in topology at Princeton, and, subsequently, Denicoff's successor, Alan Meyrowitz, supported my research during the completion of this book.\nJerome Wiesner and Philip Morse of MIT obtained the resources for our first laboratory. Our development at MIT was encouraged by William Ted Martin, Norman Levinson, Witold Hurewicz, Norbert Weiner, Claude Shannon, Peter Elias, and Robert Fano. I was given the privilege of sharing with Shannon the endowed chair of Donner Professor of Science at MIT and enjoyed the support of many other people and organizations over the years: John Williams, Paul Armer, and Merril Flood enabled me to work with Newell, Shaw, and Simon at the Rand Corporation; Oliver Selfridge and Gerald Dinneen encouraged research at MIT's Lincoln Laboratory; Michel Gouilloud supported my work from the Schlumberger Corporation; Edward David provided support from Exxon; and Alan Kay supported many of our students with funding from (successively) the Xerox, Atari, and Apple corporations. For several years, the Thinking Machines Corporation has supported both this research and the development of a new type of computer called the Connection Machine \u2014 designed by my student Danny Hillis for embodying societies of mind.\nMost of all, I want to acknowledge the contributions to this book of Seymour Papert, who came to MIT in 1963 after five years of studying child development with Jean Piaget in Geneva. Papert and I worked so well together that for a decade we supervised the laboratory jointly, each able at any time to leave the other to decide what should be done. Together we evolved new mathematical techniques, designed laboratory experiments, built computer hardware and software, and supervised the same students. Such a partnership could not have worked so well had we not both developed in similar intellectual directions before we met; we were both involved with the same areas of mathematics, with similar concerns about machinery, and with similar attitudes about psychology. One of our projects was to build a machine that could see well enough to use mechanical hands for solving real-world problems; this was the origin of Builder and the insights that emerged from it. In trying to make that robot see, we found that no single method ever worked well by itself. For example, the robot could rarely discern an object's shape by using vision alone; it also had to exploit other types of knowledge about which kinds of objects were likely to be seen. This experience impressed on us the idea that only a society of different kinds of processes could possibly suffice. Papert and I worked together not only on robotic machines, but in many other areas; for example, we spent several years developing a new mathematical theory for the then mysterious Perceptron type of learning machine. In the middle 1970s Papert and I tried together to write a book about societies of mind but abandoned the attempt when it became clear that the ideas were not mature enough. The results of that collaboration shaped many earlier sections of this book.\nEventually Papert and I both turned away from large-scale scientific enterprises toward somewhat different individual goals, and we imposed the directorship of our laboratory upon one of our most original and productive students, Patrick Winston \u2014 who first worked out the idea of making uniframes. Papert went on to develop a host of new theories about mental development and education; these led to the computer language LOGO and to many other concepts that started to enter the mainstream of educational thought over the next decade. I focused my concern on searching for better theories about the little world of how a child might learn to build with blocks. The parts of the puzzle that form this book began to fit together in my mind in the mid-1970s, around the concept of frame-array, and this eventually led to the theories about communication-lines, K-lines, and level-bands, and then, during the book's final stages, to the ideas about pronomes, polynemes, and cross-realm correspondences.\nAs for the manuscript itself, Bradley Marx read through every draft, comparing each version with earlier ones, helping to maintain clarity, stylistic coherency, and especially protecting good ideas from destructive revisionary impulses. This was hard because the early manuscript was more than twice its present length. Robin Lakoff suggested neutering the English; this seemed at first impossible but soon became quite natural. Theodore Sturgeon reviewed an early draft; I wish he had lived to see it now. Kenneth Haase, Betty Dexter, and Tom Beckman made innumerable suggestions and corrections. Successive drafts were reviewed by Danny Hillis, Steve Bagley, Marvin Denicoff, Charlotte Minsky, Michel Gouilloud, Justin Lieber, Philip Agre, David Wallace, Ben Kuipers, Peter de Jong, and Sona Vogel. Richard Feynman contributed a variety of insights about memory and parallel processing. David Yarmush helped to organize the book into sections, to smooth out the transitions, and to establish the gradient wherein the words begin with commonsense meanings and gradually become more technical. Bob Whittinghill made many suggestions about language as well as about psychology. Douglas Hofstadter evaluated the entire theory, forcing me to make several substantial changes. Michael Crichton made many technical suggestions and helped me to refine the early chapters.\nRussell Noftsker and Tom Callahan made substantial engineering contributions to our work. Hosts of ideas came from students at MIT, notably Howard Austin, Manuel Blum, Danny Bobrow, Eugene Charniak, Henry Ernst, Tom Evans, Scott Fahlman, Ira Goldstein, William Gosper, Richard Greenblatt, Adolfo Guzman, Kenneth Haase, William Henneman, Carl Hewitt, Danny Hillis, Jack Holloway, Tom Knight, William Martin, Joel Moses, Bertram Raphael, Larry Roberts, James Slagle, Jerry Sussman, Ivan Sutherland, David Waltz, Terry Winograd, Patrick Winston, and many others. Countless other thoughts came from working at various times with Maryann Amacher, Gregory Benford, Terry Beyer, Woodrow Bledsoe, Mortimer Casson, Edward Feigenbaum, Edward Fredkin, Arnold Griffith, Louis Hodes, Berthold Horn, Joel Isaacson, Russell Kirsch, David Kirsh, Robert Lawler, Justin Leiber, Douglas Lenat, Jerome Lettvin, David MacDonald, Curtis Marx, Hans Moravec, Stewart Nelson, Nils Nillsson, Donald Norman, Walter Pitts, Jerry Pournelle, Charles Rosen, Carl Sagan, Roger Schank,\nRobert Sheckley, Stephen Smoliar, Cynthia Solomon, Ray Solomonoff, Luc Steels, Warren Teitelman, and Graziella Tonfoni. I wish I could acknowledge the inspirations of all the friends of earlier years, particularly W. Ross Ashby, Thomas Etter, Ned Feder, Heinz von Foerster, Donald Hebb, John Hollander, Arnold Honig, Gordon Pask, Roland Silver, Jan Syrjala, Carroll Williams, Bertram Wolfe, David Yarmush \u2014 and of all the teachers of my youth, particularly Dudley Fitts, Ruth Gordon, Alexander Joseph, Edward Lepowsky, and Herbert Zim. My development was also strongly influenced first by the writing and later by the friendship of Arthur C. Clarke, Robert Heinlein, Frederick Pohl, and most of all by Isaac Asimov.\nOf course, the deepest influence on my style of thought came from my parents, Henry Minsky and Fannie Reiser. My wife, Gloria Rudisch, our children Margaret, Henry, and Juliana (who drew the illustrations and sometimes changed the text to make them fit), and my sister Ruth all helped to shape this book. My sister Charlotte also lives between these lines, for even in our childhood, she was a powerful artist and critic, and her dreams became the meanings of my ordinary words.",
    "type": "article",
    "title": "postscript and acknowledgement",
    "docId": 133688459681,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 155072708994,
    "gburl": "http://aurellem.org/society-of-mind/som-postscript.html-diffbotxyz1764732136",
    "lastCrawlTimeUTC": 1588764642,
    "timestamp": "Wed, 06 May 2020 11:30:42 GMT"
  },
  {
    "sentiment": -0.38,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1149958386",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.1.html",
    "html": "<p>In real life, you often have to deal with things you don't completely understand. You drive a car, not knowing how its engine works. You ride as passenger in someone else's car, not knowing how that driver works. Most strange of all, you drive your body and your mind, not knowing how your own self works. Isn't it amazing that we can think, not knowing what it means to think? Isn't it remarkable that we can get ideas, yet not explain what ideas are?</p>\n<p>In every normal person's mind there seem to be some processes that we call consciousness. We usually regard them as enabling us to know what's happening inside our minds. But this reputation of self-awareness is not so well deserved, because our conscious thoughts reveal to us so little of what gives rise to them.</p>\n<p>Consider how a driver guides the immense momentum of a motorcar, not knowing how its engine works or how its steering wheel directs it to the left or right. Yet when one comes to think of it, we drive our bodies in much the same way. So far as conscious thought is concerned, you turn yourself to walk in a certain direction in much the way you steer a car; you are aware only of some general intention, and all the rest takes care of itself. To change your direction of motion is actually quite complicated. If you simply took a larger or smaller step on one side, the way you would turn a rowboat, you would fall toward the outside of the turn. Instead, you start to turn by making yourself fall toward the inside &mdash; and then use centrifugal force to right yourself on the next step. This incredible process involves a huge society of muscles, bones, and joints, all controlled by hundreds of interacting programs that even specialists don't yet understand. Yet all you think is, Turn that way,</p>\n<p>and your wish is automatically fulfilled.</p>\n<p>We give the name <em>signals</em> to acts whose consequences are not inherent in their own character but have merely been assigned to them. When you accelerate your car by pressing on the gas pedal, this is not what does the work; it is merely a signal to make the engine push the car. Similarly, rotating the steering wheel is merely a signal that makes the steering mechanism turn the car. The car's designer could easily have assigned the pedal to steer the car or made the steering wheel control its speed. But practical designers try to exploit the use of signals that already have acquired some significance.</p>\n<p>Our conscious thoughts use signal-signs to steer the engines in our minds, controlling countless processes of which we're never much aware. Not understanding how it's done, we learn to gain our ends by sending signals to those great machines, much as the sorcerers of older times used rituals to cast their spells.</p>",
    "text": "In real life, you often have to deal with things you don't completely understand. You drive a car, not knowing how its engine works. You ride as passenger in someone else's car, not knowing how that driver works. Most strange of all, you drive your body and your mind, not knowing how your own self works. Isn't it amazing that we can think, not knowing what it means to think? Isn't it remarkable that we can get ideas, yet not explain what ideas are?\nIn every normal person's mind there seem to be some processes that we call consciousness. We usually regard them as enabling us to know what's happening inside our minds. But this reputation of self-awareness is not so well deserved, because our conscious thoughts reveal to us so little of what gives rise to them.\nConsider how a driver guides the immense momentum of a motorcar, not knowing how its engine works or how its steering wheel directs it to the left or right. Yet when one comes to think of it, we drive our bodies in much the same way. So far as conscious thought is concerned, you turn yourself to walk in a certain direction in much the way you steer a car; you are aware only of some general intention, and all the rest takes care of itself. To change your direction of motion is actually quite complicated. If you simply took a larger or smaller step on one side, the way you would turn a rowboat, you would fall toward the outside of the turn. Instead, you start to turn by making yourself fall toward the inside \u2014 and then use centrifugal force to right yourself on the next step. This incredible process involves a huge society of muscles, bones, and joints, all controlled by hundreds of interacting programs that even specialists don't yet understand. Yet all you think is, Turn that way,\nand your wish is automatically fulfilled.\nWe give the name signals to acts whose consequences are not inherent in their own character but have merely been assigned to them. When you accelerate your car by pressing on the gas pedal, this is not what does the work; it is merely a signal to make the engine push the car. Similarly, rotating the steering wheel is merely a signal that makes the steering mechanism turn the car. The car's designer could easily have assigned the pedal to steer the car or made the steering wheel control its speed. But practical designers try to exploit the use of signals that already have acquired some significance.\nOur conscious thoughts use signal-signs to steer the engines in our minds, controlling countless processes of which we're never much aware. Not understanding how it's done, we learn to gain our ends by sending signals to those great machines, much as the sorcerers of older times used rituals to cast their spells.",
    "type": "article",
    "title": "6.1 consciousness",
    "tags": [
      {
        "score": 0.5272706151008606,
        "sentiment": 0.923,
        "count": 1,
        "label": "What's Going On",
        "uri": "https://diffbot.com/entity/XsuJv6OJPNxumjrik0fQ8Fg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 164862804367,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 201523675540,
    "gburl": "http://aurellem.org/society-of-mind/som-6.1.html-diffbotxyz1289021407",
    "lastCrawlTimeUTC": 1588764703,
    "timestamp": "Wed, 06 May 2020 11:31:43 GMT"
  },
  {
    "sentiment": -0.41,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1077248140",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-5.1.html",
    "html": "<p>Whenever we can, we like to explain things in terms of simple cause and effect. We explained the case of Professor Challenger by assuming that my wish to Work came first, then Work exploited Anger's aptitude for fighting Sleep. But in real life the causal relations between feelings and thoughts are rarely so simple. My desire to work and my annoyance with Challenger were probably so intermingled, all along, that it is inappropriate to ask which came first, Anger or Work. Most likely, both agencies exploited one another simultaneously, thus combining both into a single fiendish synthesis that accomplished two goals at once; Work thus got to do its work &mdash; and, thereby, injured Challenger! (In an academic rivalry, a technical accomplishment can hurt more than a fist.) Two goals can support each other.</p>\n<p>A causes B <em>John wanted to go home because he felt tired of work.</em> B causes A <em>John felt tired of work because he wanted to go home.</em></p>\n<p>There need be no <em>first cause</em> since John could start out with both distaste for work and inclination to go home. Then a loop of circular causality ensues, in which each goal gains support from the other until their combined urge becomes irresistible. We're always enmeshed in causal loops. Suppose you had borrowed past your means and later had to borrow more in order to pay the interest on your loan. If you were asked what the difficulty was, it would not be enough to say simply, <em>Because I have to pay the interest,</em> or to say only, <em>Because I have to pay the principal.</em> Neither alone is the actual cause, and you'd have to explain that you're caught in a loop.</p>\n<p>We often speak of <em>straightening things out</em> when we're involved in situations that seem too complicated. It seems to me that this metaphor reflects how hard it is to find one's way through a maze that has complicated loops in it. In such a situation, we always try to find a <em>path</em> through it by seeking <em>causal</em> explanations that go in only one direction. There's a good reason for doing this.</p>\n<p>There are countless different types of networks that contain loops. But all networks that contain no loops are basically the</p>\n<p>same: each has the form of a simple chain.</p>\n<p>Because of this, we can apply the very same types of reasoning to everything we can represent in terms of chains of causes and effects. Whenever we accomplish that, we can proceed from start to end without any need for a novel thought; that's what we mean by <em>straightening out.</em> But frequently, to construct such a path, we have to ignore important interactions and dependencies that run in other directions.</p>",
    "text": "Whenever we can, we like to explain things in terms of simple cause and effect. We explained the case of Professor Challenger by assuming that my wish to Work came first, then Work exploited Anger's aptitude for fighting Sleep. But in real life the causal relations between feelings and thoughts are rarely so simple. My desire to work and my annoyance with Challenger were probably so intermingled, all along, that it is inappropriate to ask which came first, Anger or Work. Most likely, both agencies exploited one another simultaneously, thus combining both into a single fiendish synthesis that accomplished two goals at once; Work thus got to do its work \u2014 and, thereby, injured Challenger! (In an academic rivalry, a technical accomplishment can hurt more than a fist.) Two goals can support each other.\nA causes B John wanted to go home because he felt tired of work. B causes A John felt tired of work because he wanted to go home.\nThere need be no first cause since John could start out with both distaste for work and inclination to go home. Then a loop of circular causality ensues, in which each goal gains support from the other until their combined urge becomes irresistible. We're always enmeshed in causal loops. Suppose you had borrowed past your means and later had to borrow more in order to pay the interest on your loan. If you were asked what the difficulty was, it would not be enough to say simply, Because I have to pay the interest, or to say only, Because I have to pay the principal. Neither alone is the actual cause, and you'd have to explain that you're caught in a loop.\nWe often speak of straightening things out when we're involved in situations that seem too complicated. It seems to me that this metaphor reflects how hard it is to find one's way through a maze that has complicated loops in it. In such a situation, we always try to find a path through it by seeking causal explanations that go in only one direction. There's a good reason for doing this.\nThere are countless different types of networks that contain loops. But all networks that contain no loops are basically the\nsame: each has the form of a simple chain.\nBecause of this, we can apply the very same types of reasoning to everything we can represent in terms of chains of causes and effects. Whenever we accomplish that, we can proceed from start to end without any need for a novel thought; that's what we mean by straightening out. But frequently, to construct such a path, we have to ignore important interactions and dependencies that run in other directions.",
    "type": "article",
    "title": "5.1 circular causality",
    "tags": [
      {
        "score": 0.774469256401062,
        "sentiment": 0,
        "count": 1,
        "label": "Professor Challenger",
        "uri": "https://diffbot.com/entity/XobmEtwweOk688vIQ90PUWw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6817042827606201,
        "sentiment": -0.211,
        "count": 2,
        "label": "Challenger",
        "uri": "https://diffbot.com/entity/XkzzHeCalMbWxeZ0vxplZLg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/MeanOfTransportation",
          "http://dbpedia.org/ontology/Rocket"
        ]
      },
      {
        "score": 0.5691531300544739,
        "sentiment": 0,
        "count": 1,
        "label": "Sleep",
        "uri": "https://diffbot.com/entity/OHbQ7lGIKP_yOL41orRkOKg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      }
    ],
    "docId": 181802484154,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 159234982280,
    "gburl": "http://aurellem.org/society-of-mind/som-5.1.html-diffbotxyz4228728796",
    "lastCrawlTimeUTC": 1588764673,
    "timestamp": "Wed, 06 May 2020 11:31:13 GMT"
  },
  {
    "sentiment": 0.582,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-2136442138",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-30.2.html",
    "html": "<p>We often speak as though we classify our thoughts into different types called facts, opinions, and beliefs.</p>\n<ul> <li>The red object is on the table.</li> <li>I think the red block is on the table.</li> <li>I believe that the red block is on the table.</li> </ul>\n<p>How do these statements differ from one another? Some philosophers have argued that <em>knowing</em> must mean <em>true and justified belief.</em> However, no one has ever found a test to prove what's justified or true. For example, we all know that the sun rises in the morning. Once, long ago, some people thought this was due to godlike agents in the sky, and that the sun's trajectory was where Apollo steered his chariot. Today our scientists tell us that the sun doesn't really rise at all, because <em>sunrise</em> is simply what we each experience when the planet Earth's rotation moves us into the sun's unchanging light. This means we all <em>know</em> something that isn't true.</p>\n<p>To comprehend what knowing is, we have to guard ourselves against that single-agent fallacy of thinking that the <em>I</em> in <em>I believe</em> is actually a single, stable thing. The truth is that a person's mind holds different views in different realms. Thus, one part of an astronomer's mind can apply the common view of sunrise to down-to-earth affairs, regarding the sun as like a lamp that wakes us up and lights our way. But at the same time, that same astronomer can apply the modern physical view to technical problems in astronomy. We each use many different views, and which we choose to use depends, from one moment to the next, upon the changing balance of power among our agencies.</p>\n<p>Then if what we <em>believe</em> is so conditional, what makes us feel that our beliefs are much more definite than that? It is because whenever we commit ourselves to speak or act, we thereby have to force ourselves into clear-cut, action-oriented states of mind in which most of our questions are suppressed. As far as everyday life is concerned, decisiveness is indispensable; otherwise we'd have to act so cautiously that nothing would get done. And here lies much of what we express with words like <em>guess,</em> <em>believe,</em> and <em>know.</em> In the course of making practical decisions (and thereby turning off most agencies), we use such words to summarize our various varieties of certainty.</p>\n<p>The notion that only certain of a person's beliefs are <em>genuine</em> plays vital roles in all our moral and legal schemes. Whenever we censure or applaud what other people do, we're taught to be more concerned with what those other people <em>genuinely</em> expected or intended to happen than with what actually happened. This doctrine underlies how we distinguish thoughtlessness and forgetfulness from lies, deceit, and treachery. I do not mean that such distinctions are not important, only that they do not justify the simplistic assumption that, among all the mind's activities, certain special kinds of thoughts are essentially more <em>genuine</em> than others. All such distinctions seem less absolute when every deeper probe into beliefs reveals more ambiguities.</p>",
    "text": "We often speak as though we classify our thoughts into different types called facts, opinions, and beliefs.\nThe red object is on the table.\nI think the red block is on the table.\nI believe that the red block is on the table.\nHow do these statements differ from one another? Some philosophers have argued that knowing must mean true and justified belief. However, no one has ever found a test to prove what's justified or true. For example, we all know that the sun rises in the morning. Once, long ago, some people thought this was due to godlike agents in the sky, and that the sun's trajectory was where Apollo steered his chariot. Today our scientists tell us that the sun doesn't really rise at all, because sunrise is simply what we each experience when the planet Earth's rotation moves us into the sun's unchanging light. This means we all know something that isn't true.\nTo comprehend what knowing is, we have to guard ourselves against that single-agent fallacy of thinking that the I in I believe is actually a single, stable thing. The truth is that a person's mind holds different views in different realms. Thus, one part of an astronomer's mind can apply the common view of sunrise to down-to-earth affairs, regarding the sun as like a lamp that wakes us up and lights our way. But at the same time, that same astronomer can apply the modern physical view to technical problems in astronomy. We each use many different views, and which we choose to use depends, from one moment to the next, upon the changing balance of power among our agencies.\nThen if what we believe is so conditional, what makes us feel that our beliefs are much more definite than that? It is because whenever we commit ourselves to speak or act, we thereby have to force ourselves into clear-cut, action-oriented states of mind in which most of our questions are suppressed. As far as everyday life is concerned, decisiveness is indispensable; otherwise we'd have to act so cautiously that nothing would get done. And here lies much of what we express with words like guess, believe, and know. In the course of making practical decisions (and thereby turning off most agencies), we use such words to summarize our various varieties of certainty.\nThe notion that only certain of a person's beliefs are genuine plays vital roles in all our moral and legal schemes. Whenever we censure or applaud what other people do, we're taught to be more concerned with what those other people genuinely expected or intended to happen than with what actually happened. This doctrine underlies how we distinguish thoughtlessness and forgetfulness from lies, deceit, and treachery. I do not mean that such distinctions are not important, only that they do not justify the simplistic assumption that, among all the mind's activities, certain special kinds of thoughts are essentially more genuine than others. All such distinctions seem less absolute when every deeper probe into beliefs reveals more ambiguities.",
    "type": "article",
    "title": "30.2 knowing and believing",
    "tags": [
      {
        "score": 0.5966249108314514,
        "sentiment": 0.43,
        "count": 1,
        "label": "Apollo",
        "uri": "https://diffbot.com/entity/X_Y_UFQ_hNSeTdZWlOq4rbw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/Deity"
        ]
      },
      {
        "score": 0.5236136317253113,
        "sentiment": 0.766,
        "count": 2,
        "label": "astronomer",
        "uri": "https://diffbot.com/entity/XT3kF9aSQMVqkzILKL1JgZw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession"
        ]
      }
    ],
    "docId": 60467544500,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 79375598005,
    "gburl": "http://aurellem.org/society-of-mind/som-30.2.html-diffbotxyz1241054380",
    "lastCrawlTimeUTC": 1588764560,
    "timestamp": "Wed, 06 May 2020 11:29:20 GMT"
  },
  {
    "sentiment": -0.969,
    "humanLanguage": "en",
    "diffbotUri": "article|3|284685580",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.2.html",
    "html": "<p>Here's a rule that's part of ordinary common sense: If A depends on B, and B depends on C, then &mdash; clearly &mdash; A depends on C. But what do such expressions mean? And why do we make the same kinds of inferences not only for dependency but also for implication and causality?</p>\n<p>If A depends on B, and, also, B depends on C, then A depends on C. If A implies B, and, also, B implies C, then A implies C. If A causes B, and, also, B causes C, then A causes C.</p>\n<p>What do all these different ideas have in common? All lend themselves to being linked into chainlike strings. Whenever we discover such sequences &mdash; however long they may be &mdash; we regard it as completely natural to compress them into single links, by deleting all but the beginning and the end. This lets us <em>conclude,</em> for example, that A depends on, implies, or causes C. We do this even with imaginary paths through time and space.</p>\n<p>Floor holds Table holds Saucer holds Cup holds Tea Wheel turns Shaft turns Gear turns Shaft turns Gear Sometimes we even chain together different kinds of links:</p>\n<p>House walk to Garage drive to Airport fly to Airport Owls are Birds, and Birds can Fly. So, Owls can Fly.</p>\n<p>The chain containing <em>walk,</em> <em>drive,</em> and <em>fly</em> may appear to use several different kinds of links. But although they differ in regard to vehicles, they all refer to paths through space. And in the Owl-Bird example, <em>are</em> and <em>can</em> seem more different at first, but we can translate them both into a more uniform language by changing <em>Owls are Birds</em> into <em>An Owl is a Typical-Bird</em> and <em>Birds can Fly</em> into <em>A Typical-Bird is a thing-which-can-Fly.</em> Both sentences then share the same type of <em>is a</em> link, and this allows us to chain them together more easily.</p>\n<p>For generations, scientists and philosophers have tried to explain ordinary reasoning in terms of logical principles &mdash; with virtually no success. I suspect this enterprise failed because it was looking in the wrong direction: common sense works so well not because it is an approximation of logic; logic is only a small part of our great accumulation of different, useful ways to chain things together. Many thinkers have assumed that logical necessity lies at the heart of our reasoning. But for the purposes of psychology, we'd do better to set aside the dubious ideal of faultless deduction and try, instead, to understand how people actually deal with what is usual or typical. To do this, we often think in terms of causes, similarities, and dependencies. What do all these forms of thinking share? They all use different ways to make chains.</p>",
    "text": "Here's a rule that's part of ordinary common sense: If A depends on B, and B depends on C, then \u2014 clearly \u2014 A depends on C. But what do such expressions mean? And why do we make the same kinds of inferences not only for dependency but also for implication and causality?\nIf A depends on B, and, also, B depends on C, then A depends on C. If A implies B, and, also, B implies C, then A implies C. If A causes B, and, also, B causes C, then A causes C.\nWhat do all these different ideas have in common? All lend themselves to being linked into chainlike strings. Whenever we discover such sequences \u2014 however long they may be \u2014 we regard it as completely natural to compress them into single links, by deleting all but the beginning and the end. This lets us conclude, for example, that A depends on, implies, or causes C. We do this even with imaginary paths through time and space.\nFloor holds Table holds Saucer holds Cup holds Tea Wheel turns Shaft turns Gear turns Shaft turns Gear Sometimes we even chain together different kinds of links:\nHouse walk to Garage drive to Airport fly to Airport Owls are Birds, and Birds can Fly. So, Owls can Fly.\nThe chain containing walk, drive, and fly may appear to use several different kinds of links. But although they differ in regard to vehicles, they all refer to paths through space. And in the Owl-Bird example, are and can seem more different at first, but we can translate them both into a more uniform language by changing Owls are Birds into An Owl is a Typical-Bird and Birds can Fly into A Typical-Bird is a thing-which-can-Fly. Both sentences then share the same type of is a link, and this allows us to chain them together more easily.\nFor generations, scientists and philosophers have tried to explain ordinary reasoning in terms of logical principles \u2014 with virtually no success. I suspect this enterprise failed because it was looking in the wrong direction: common sense works so well not because it is an approximation of logic; logic is only a small part of our great accumulation of different, useful ways to chain things together. Many thinkers have assumed that logical necessity lies at the heart of our reasoning. But for the purposes of psychology, we'd do better to set aside the dubious ideal of faultless deduction and try, instead, to understand how people actually deal with what is usual or typical. To do this, we often think in terms of causes, similarities, and dependencies. What do all these forms of thinking share? They all use different ways to make chains.",
    "type": "article",
    "title": "18.2 chains of reasoning",
    "tags": [
      {
        "score": 0.7913476228713989,
        "sentiment": 0.751,
        "count": 8,
        "label": "C",
        "uri": "https://diffbot.com/entity/X4K2_qY_jNBGAZG6xOsO29A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Language",
          "http://dbpedia.org/ontology/ProgrammingLanguage",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6824572682380676,
        "sentiment": 0,
        "count": 3,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      },
      {
        "score": 0.6229912042617798,
        "sentiment": 0,
        "count": 2,
        "label": "Hong Kong International Airport",
        "uri": "https://diffbot.com/entity/BVeuJ5FY4NkOekrWCB2j73Q",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/ArchitecturalStructure",
          "http://dbpedia.org/ontology/Infrastructure",
          "http://dbpedia.org/ontology/Airport",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5927725434303284,
        "sentiment": 0.97,
        "count": 3,
        "label": "Owls",
        "uri": "https://diffbot.com/entity/OQB1c6vFuPmmQOzCsruePjw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      },
      {
        "score": 0.553627610206604,
        "sentiment": 0.591,
        "count": 1,
        "label": "Web Ontology Language",
        "uri": "https://diffbot.com/entity/XgPerHgJeO9OmwgONmiw2dA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.5459325313568115,
        "sentiment": 0,
        "count": 0,
        "label": "Larry Bird",
        "uri": "https://diffbot.com/entity/PNVMF33eTNZOKSiKjPQPJMA",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.52032470703125,
        "sentiment": 0.32,
        "count": 1,
        "label": "Benjamin West",
        "uri": "https://diffbot.com/entity/PsV9kQP68PK60AfZ9ReFMkg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5140985250473022,
        "sentiment": 0.973,
        "count": 2,
        "label": "FLY.",
        "uri": "https://diffbot.com/entity/Oi6MWTRW2OzWuJx9C3Doqqw",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      }
    ],
    "docId": 102989906360,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 252427534732,
    "gburl": "http://aurellem.org/society-of-mind/som-18.2.html-diffbotxyz690499139",
    "lastCrawlTimeUTC": 1588764500,
    "timestamp": "Wed, 06 May 2020 11:28:20 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1877866314",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.html",
    "text": "",
    "type": "article",
    "title": "8 a theory of memory",
    "docId": 228132897165,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 222521033132,
    "gburl": "http://aurellem.org/society-of-mind/som-8.html-diffbotxyz1597751303",
    "lastCrawlTimeUTC": 1588764528,
    "timestamp": "Wed, 06 May 2020 11:28:48 GMT"
  },
  {
    "sentiment": 0.587,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1941178128",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.5.html",
    "html": "<p>&ldquo;Jack is flying his kite.&rdquo; What knowledge do you need to understand this? It helps to know that you can't fly kites without any wind. It helps to know how to fly a kite. You would understand it better if you knew how kites are made, or where they're found, or what they cost. Understanding never ends. It is remarkable how much we can imagine about Jack's activity. Neither you nor I have ever seen Jack's kite, nor do we know its color, shape, or size, and yet our minds supply details from memories of other kites we've seen before. That sentence may have made you think of string, yet string wasn't mentioned. How does your mind arouse so many memories so quickly? And how does your mind know not to arouse too many memories &mdash; something that could also lead to serious problems? To explain this, I'll introduce what I call the level-band theory.</p>\n<p>The basic idea is simple: we learn by attaching agents to K-lines, but we don't attach them all with equal firmness. Instead, we make strong connections at a certain level of detail, but we make weaker connections at higher and lower levels. A K-line for a kite might include some properties like these:</p>\n<p>Whenever we turn on this K-line, it tries to activate all these agents, but those near the fringes are attached as though by twice used tape and tend to retreat when other agents challenge them. If most of the kites you've seen before were red and diamond-shaped, then when you hear about Jack's kite, those weak connections will lead you to assume that Jack's kite, too, is red and diamond-shaped. But if you should hear that Jack's kite is green, your weakly activated red-color agent memories will be suppressed by your strongly activated green-color agents. Let's call these kinds of weakly activated memories assumptions by default. Default assumptions, once aroused, stay active only when there are no conflicts. In psychological terms, they are things we assume when we have no particular reason to think otherwise. Later we'll see that default assumptions embody some of our most valuable kinds of commonsense knowledge: knowing what is usual or typical. For example, they're why we all assume that Jack has hands and feet. If such assumptions turn out to be wrong, their weak connections allow them to be easily displaced when better information comes to mind.</p>",
    "text": "\u201cJack is flying his kite.\u201d What knowledge do you need to understand this? It helps to know that you can't fly kites without any wind. It helps to know how to fly a kite. You would understand it better if you knew how kites are made, or where they're found, or what they cost. Understanding never ends. It is remarkable how much we can imagine about Jack's activity. Neither you nor I have ever seen Jack's kite, nor do we know its color, shape, or size, and yet our minds supply details from memories of other kites we've seen before. That sentence may have made you think of string, yet string wasn't mentioned. How does your mind arouse so many memories so quickly? And how does your mind know not to arouse too many memories \u2014 something that could also lead to serious problems? To explain this, I'll introduce what I call the level-band theory.\nThe basic idea is simple: we learn by attaching agents to K-lines, but we don't attach them all with equal firmness. Instead, we make strong connections at a certain level of detail, but we make weaker connections at higher and lower levels. A K-line for a kite might include some properties like these:\nWhenever we turn on this K-line, it tries to activate all these agents, but those near the fringes are attached as though by twice used tape and tend to retreat when other agents challenge them. If most of the kites you've seen before were red and diamond-shaped, then when you hear about Jack's kite, those weak connections will lead you to assume that Jack's kite, too, is red and diamond-shaped. But if you should hear that Jack's kite is green, your weakly activated red-color agent memories will be suppressed by your strongly activated green-color agents. Let's call these kinds of weakly activated memories assumptions by default. Default assumptions, once aroused, stay active only when there are no conflicts. In psychological terms, they are things we assume when we have no particular reason to think otherwise. Later we'll see that default assumptions embody some of our most valuable kinds of commonsense knowledge: knowing what is usual or typical. For example, they're why we all assume that Jack has hands and feet. If such assumptions turn out to be wrong, their weak connections allow them to be easily displaced when better information comes to mind.",
    "type": "article",
    "title": "8.5 level-bands",
    "tags": [
      {
        "score": 0.5921589732170105,
        "sentiment": 0.943,
        "count": 1,
        "label": "Jack Shephard",
        "uri": "https://diffbot.com/entity/XPb-cc-yAPbK9HeP1VhSLpQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5911551713943481,
        "sentiment": 0.476,
        "count": 2,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5524248480796814,
        "sentiment": 0.752,
        "count": 2,
        "label": "Jack",
        "uri": "https://diffbot.com/entity/Xjq8lsTVwPIOiicaSvDHrRQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      },
      {
        "score": 0.551252543926239,
        "sentiment": -0.396,
        "count": 2,
        "label": "string orchestra",
        "uri": "https://diffbot.com/entity/XefA8BhC6PPGBvS920gdgJA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Genre",
          "http://dbpedia.org/ontology/MusicGenre"
        ]
      },
      {
        "score": 0.5037999749183655,
        "sentiment": -0.169,
        "count": 1,
        "label": "So Many Memories",
        "uri": "https://diffbot.com/entity/XH7wcZoLONQ2ncXg2t26-Kw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 269212402087,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 268817645976,
    "gburl": "http://aurellem.org/society-of-mind/som-8.5.html-diffbotxyz3644048851",
    "lastCrawlTimeUTC": 1588764590,
    "timestamp": "Wed, 06 May 2020 11:29:50 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1427313023",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.8.html",
    "html": "<p>When several objects move at once, it's hard to keep track of them all. This also seems to be the case in every other realm of thought; the more things we think about, the harder it is to pay attention to them all. We're forced to focus on a few while losing track of all the rest. What causes these phenomena? I'll argue that they're aspects of the processes we use to control our short-term memories. These skills develop over time; an adult can do things with memories that infants cannot do at all, such as remembering details of an action's purpose and trajectory, and how various obstacles were overcome. An infant, though, can barely keep track of what it's holding in one hand and is likely to forget what's in its other hand.</p>\n<p>How does memory-control begin? Perhaps our infants first acquire control over a single pronome, which gives them the ability to keep in mind a <em>temporary polyneme.</em> This amounts to being able to maintain only a single <em>object of attention</em>; let's call it IT. Now even the ability to keep track of a single IT requires the development of certain skills of memory-control, for it takes the normal infant several months to become able to tolerate even a small interruption without losing its previous focus of interest.</p>\n<p>One kind of interruption comes, for example, when watching a ball that happens to roll behind a box. To a very young infant, that IT will simply disappear from mind. An older infant will remember IT and expect the ball soon to reappear; we can see this in the way the older infant's eyes look toward the far side of the box. If the ball does not soon reappear, the older child will actively reach around the box for it, which shows that the child has maintained some sort of representation of IT. Another variety of interruption can come from inside the child's own mind, from refocusing on the same object, but at a different level of detail. For example, when a young child concentrates upon a doll's shoe, it may forget its original concern with the doll itself. Later, that concern with the shoe may be replaced, in turn, when the baby becomes occupied with the end of the shoelace.</p>\n<p>But what's an IT? The ability to focus attention could start with some machinery for keeping track of simple polynemes for object-things. In later stages, an IT could represent more complex processes or scripts that keep track of entire Trans-actions with their various pronomes for Objects, Origins, Destinations, Obstacles, Trajectories, and Purposes. Eventually our ITs develop into complex systems of machinery that represent the things that are <em>on one's mind</em> at the moment. In later life, we become more able to maintain several ITs at once. This enables us to construct comparisons, predictions, and imaginary plans, and to begin to construct explanations in terms of chains of causes and reasons.</p>",
    "text": "When several objects move at once, it's hard to keep track of them all. This also seems to be the case in every other realm of thought; the more things we think about, the harder it is to pay attention to them all. We're forced to focus on a few while losing track of all the rest. What causes these phenomena? I'll argue that they're aspects of the processes we use to control our short-term memories. These skills develop over time; an adult can do things with memories that infants cannot do at all, such as remembering details of an action's purpose and trajectory, and how various obstacles were overcome. An infant, though, can barely keep track of what it's holding in one hand and is likely to forget what's in its other hand.\nHow does memory-control begin? Perhaps our infants first acquire control over a single pronome, which gives them the ability to keep in mind a temporary polyneme. This amounts to being able to maintain only a single object of attention; let's call it IT. Now even the ability to keep track of a single IT requires the development of certain skills of memory-control, for it takes the normal infant several months to become able to tolerate even a small interruption without losing its previous focus of interest.\nOne kind of interruption comes, for example, when watching a ball that happens to roll behind a box. To a very young infant, that IT will simply disappear from mind. An older infant will remember IT and expect the ball soon to reappear; we can see this in the way the older infant's eyes look toward the far side of the box. If the ball does not soon reappear, the older child will actively reach around the box for it, which shows that the child has maintained some sort of representation of IT. Another variety of interruption can come from inside the child's own mind, from refocusing on the same object, but at a different level of detail. For example, when a young child concentrates upon a doll's shoe, it may forget its original concern with the doll itself. Later, that concern with the shoe may be replaced, in turn, when the baby becomes occupied with the end of the shoelace.\nBut what's an IT? The ability to focus attention could start with some machinery for keeping track of simple polynemes for object-things. In later stages, an IT could represent more complex processes or scripts that keep track of entire Trans-actions with their various pronomes for Objects, Origins, Destinations, Obstacles, Trajectories, and Purposes. Eventually our ITs develop into complex systems of machinery that represent the things that are on one's mind at the moment. In later life, we become more able to maintain several ITs at once. This enables us to construct comparisons, predictions, and imaginary plans, and to begin to construct explanations in terms of chains of causes and reasons.",
    "type": "article",
    "title": "21.8 attention",
    "tags": [
      {
        "score": 0.6892408132553101,
        "sentiment": -0.65,
        "count": 5,
        "label": "baby",
        "uri": "https://diffbot.com/entity/X4MZ_yLLbO0ueNL-zoshUhg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5669809579849243,
        "sentiment": 0.253,
        "count": 1,
        "label": "Short-Term Memories",
        "uri": "https://diffbot.com/entity/X_T2HkD74PFWvQ0Z-k1c_dQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5278353691101074,
        "sentiment": -0.343,
        "count": 5,
        "label": "track and field",
        "uri": "https://diffbot.com/entity/X4_zDwLp-P6eD0jnhrKVMVg"
      }
    ],
    "docId": 47840461244,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 162061877655,
    "gburl": "http://aurellem.org/society-of-mind/som-21.8.html-diffbotxyz3381389624",
    "lastCrawlTimeUTC": 1588764402,
    "timestamp": "Wed, 06 May 2020 11:26:42 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1568603446",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-23.5.html",
    "html": "<h2><em>23.5</em> foreign accents</h2>\n<p>It is not unusual for an adult to learn a second language with nearly perfect mastery of grammar and vocabulary. But once past adolescence, most people never manage to imitate the new language's pronunciation perfectly, no matter how long and hard they work at it. In other words, they speak with <em>foreign accents.</em> Even when another speaker tries to help with <em>Say it like this, not that,</em> the learner is unable to learn what changes to make. Most people who change countries in their later teens never learn to speak the way the natives do.</p>\n<p>Why do adults find it so hard to learn how to pronounce new word sounds? Some like to say that this reflects a general decline in the learning capacities of older people, but that appears to be a myth. Instead, I suspect this particular disability is caused, more or less directly, by a genetically programmed mechanism that disables our ability to learn to make new connections in or between the agents we use to represent speech sounds. There is evidence that our brains use different machinery for recognizing language sounds than for recognizing other sorts of sounds, particularly for the little speech- sound units that language scientists call <em>phonemes.</em> Most human languages use less than a hundred phonemes.</p>\n<p>Why should we be able to learn many different speech sounds before the age of puberty but find it so much harder to learn new ones afterward? I suspect that this link to puberty is no coincidence. Instead, one or more of the genetically controlled mechanisms that brings on sexual maturity also acts to reduce the capacities of these particular agencies to learn to recognize and make new sounds! But why did this peculiar disability evolve? What evolutionary survival advantage would favor individuals whose genes reduce, after that age, this particular ability to learn? Consider this hypothesis:</p>\n<p>The onset of the childbearing age is the biological moment when a person's social role changes from learner to teacher. The <em>evolutionary purpose</em> of suppressing speech-sound learning may simply serve to prevent the parent from learning</p>\n<p>the child's speech &mdash; thus making the child learn the adult's speech instead!</p>\n<p>Wouldn't parents want to teach the children their language anyway? Not necessarily. In the short run, a parent is usually more concerned with communication than with instruction. Accordingly, if we found it easier to imitate our children's sounds, that's what we'd do. But if parents were inclined and able to learn to speak the ways their children do, those children would lose both incentive and opportunity to learn to speak like adults, and &mdash; if every child acquired a different set of language sounds &mdash; no common, public language would ever have evolved in the first place! If this is right, puberty-linked genes for suppressing speech-sound learning may have formed fairly early in the evolution of human languages. No one knows when that occurred, but if biologists could find and date the genes for this, we could obtain a clue about the time of language's unknown origin, perhaps within the last half million years. sh\\</p>",
    "text": "23.5 foreign accents\nIt is not unusual for an adult to learn a second language with nearly perfect mastery of grammar and vocabulary. But once past adolescence, most people never manage to imitate the new language's pronunciation perfectly, no matter how long and hard they work at it. In other words, they speak with foreign accents. Even when another speaker tries to help with Say it like this, not that, the learner is unable to learn what changes to make. Most people who change countries in their later teens never learn to speak the way the natives do.\nWhy do adults find it so hard to learn how to pronounce new word sounds? Some like to say that this reflects a general decline in the learning capacities of older people, but that appears to be a myth. Instead, I suspect this particular disability is caused, more or less directly, by a genetically programmed mechanism that disables our ability to learn to make new connections in or between the agents we use to represent speech sounds. There is evidence that our brains use different machinery for recognizing language sounds than for recognizing other sorts of sounds, particularly for the little speech- sound units that language scientists call phonemes. Most human languages use less than a hundred phonemes.\nWhy should we be able to learn many different speech sounds before the age of puberty but find it so much harder to learn new ones afterward? I suspect that this link to puberty is no coincidence. Instead, one or more of the genetically controlled mechanisms that brings on sexual maturity also acts to reduce the capacities of these particular agencies to learn to recognize and make new sounds! But why did this peculiar disability evolve? What evolutionary survival advantage would favor individuals whose genes reduce, after that age, this particular ability to learn? Consider this hypothesis:\nThe onset of the childbearing age is the biological moment when a person's social role changes from learner to teacher. The evolutionary purpose of suppressing speech-sound learning may simply serve to prevent the parent from learning\nthe child's speech \u2014 thus making the child learn the adult's speech instead!\nWouldn't parents want to teach the children their language anyway? Not necessarily. In the short run, a parent is usually more concerned with communication than with instruction. Accordingly, if we found it easier to imitate our children's sounds, that's what we'd do. But if parents were inclined and able to learn to speak the ways their children do, those children would lose both incentive and opportunity to learn to speak like adults, and \u2014 if every child acquired a different set of language sounds \u2014 no common, public language would ever have evolved in the first place! If this is right, puberty-linked genes for suppressing speech-sound learning may have formed fairly early in the evolution of human languages. No one knows when that occurred, but if biologists could find and date the genes for this, we could obtain a clue about the time of language's unknown origin, perhaps within the last half million years. sh\\",
    "type": "article",
    "title": "23.5 foreign accents",
    "docId": 140295160230,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 54566617513,
    "gburl": "http://aurellem.org/society-of-mind/som-23.5.html-diffbotxyz3053848601",
    "lastCrawlTimeUTC": 1588764472,
    "timestamp": "Wed, 06 May 2020 11:27:52 GMT"
  },
  {
    "sentiment": 0.376,
    "humanLanguage": "en",
    "diffbotUri": "article|3|305722472",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.4.html",
    "html": "<p>Ask anyone for memories from childhood, and everyone will readily produce a handful of stories like this:</p>\n<p>My neighbor's father died when I was four. I remember sitting with my friend in front of their house, watching people come and go. It was strange. No one said anything.</p>\n<p>It's hard to distinguish memories from memories of memories. Indeed, there's little evidence that any of our adult memories really go way back to infancy; what seem like early memories may be nothing more than reconstructions of our older thoughts. For one thing, recollections from our first five years seem strangely isolated; if we ask what happened earlier that day, the answer almost always is, <em>I can't remember that.</em> Furthermore, many of those early memories involve incidents so significant that they probably occupied the child's mind repeatedly over a period of years. Most suspicious of all is the fact that such recollections are frequently described as seen through other, older eyes &mdash; with the narrator portrayed inside the scene, right near the center of the stage. Since we never actually see ourselves, these must be reconstructed memories, rehearsed and reformulated since infancy.</p>\n<p>I suspect that this <em>amnesia of infancy</em> is no mere effect of decay over time but an inevitable result of growing out of infancy. A memory is not a separate entity, apart from how it works upon the mind. To remember an early experience, you must be able not only to <em>retrieve</em> some old records, but to reconstruct how your earlier mind reacted to them &mdash; and to do that, you would have to become an infant again. To outgrow infancy, you have to sacrifice your memories because they're written in an ancient script that your later selves can no longer read.</p>\n<p>We reconstruct our recent memories as well, since they portray less what we saw than what we recognized. From every moment to the next, your mental state is shaped not only by signals from the outer world, but by agents activated by the memories these evoke. For example, when you see a chair, what makes it appear to you to be a chair &mdash; rather than an assortment of sticks and boards? It must evoke some memories. Only a part of your impression comes from agents activated directly by your vision; most of what your higher-level agencies experience comes from the memories those vision-agents activate. Usually, we have no conscious sense of this happening, and we never use words like <em>memory</em> or <em>remembering</em> when the process works quickly and quietly; instead, we speak of <em>seeing</em> or <em>recognizing</em> or <em>knowing.</em> This is because such processes leave too few traces for the rest of the mind to contemplate; accordingly, such processes are unconscious, because consciousness requires short-term memory. It is only when a recognition involves substantial time and effort that we speak of <em>remembering.</em></p>\n<p>Then what do we mean by <em>memory</em>? Our brains use many different ways to store the traces of our pasts. No single word can describe so much, unless it is used only in a general, informal sense.</p>\n<p>Memories are processes that make some of our agents act in much the same ways they did at various times in the past.</p>",
    "text": "Ask anyone for memories from childhood, and everyone will readily produce a handful of stories like this:\nMy neighbor's father died when I was four. I remember sitting with my friend in front of their house, watching people come and go. It was strange. No one said anything.\nIt's hard to distinguish memories from memories of memories. Indeed, there's little evidence that any of our adult memories really go way back to infancy; what seem like early memories may be nothing more than reconstructions of our older thoughts. For one thing, recollections from our first five years seem strangely isolated; if we ask what happened earlier that day, the answer almost always is, I can't remember that. Furthermore, many of those early memories involve incidents so significant that they probably occupied the child's mind repeatedly over a period of years. Most suspicious of all is the fact that such recollections are frequently described as seen through other, older eyes \u2014 with the narrator portrayed inside the scene, right near the center of the stage. Since we never actually see ourselves, these must be reconstructed memories, rehearsed and reformulated since infancy.\nI suspect that this amnesia of infancy is no mere effect of decay over time but an inevitable result of growing out of infancy. A memory is not a separate entity, apart from how it works upon the mind. To remember an early experience, you must be able not only to retrieve some old records, but to reconstruct how your earlier mind reacted to them \u2014 and to do that, you would have to become an infant again. To outgrow infancy, you have to sacrifice your memories because they're written in an ancient script that your later selves can no longer read.\nWe reconstruct our recent memories as well, since they portray less what we saw than what we recognized. From every moment to the next, your mental state is shaped not only by signals from the outer world, but by agents activated by the memories these evoke. For example, when you see a chair, what makes it appear to you to be a chair \u2014 rather than an assortment of sticks and boards? It must evoke some memories. Only a part of your impression comes from agents activated directly by your vision; most of what your higher-level agencies experience comes from the memories those vision-agents activate. Usually, we have no conscious sense of this happening, and we never use words like memory or remembering when the process works quickly and quietly; instead, we speak of seeing or recognizing or knowing. This is because such processes leave too few traces for the rest of the mind to contemplate; accordingly, such processes are unconscious, because consciousness requires short-term memory. It is only when a recognition involves substantial time and effort that we speak of remembering.\nThen what do we mean by memory? Our brains use many different ways to store the traces of our pasts. No single word can describe so much, unless it is used only in a general, informal sense.\nMemories are processes that make some of our agents act in much the same ways they did at various times in the past.",
    "type": "article",
    "title": "15.4 memories of memories",
    "tags": [
      {
        "score": 0.6805055737495422,
        "sentiment": 0,
        "count": 6,
        "label": "baby",
        "uri": "https://diffbot.com/entity/X4MZ_yLLbO0ueNL-zoshUhg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 188935602571,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 94663442874,
    "gburl": "http://aurellem.org/society-of-mind/som-15.4.html-diffbotxyz1269063729",
    "lastCrawlTimeUTC": 1588764437,
    "timestamp": "Wed, 06 May 2020 11:27:17 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-883864208",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-4.1.html",
    "html": "<blockquote> <b>self</b> <i>n.</i> 1. the identity, character, or essential qualities of any person or thing. 2. the identity, personality, individuality, etc. of a given person; one's own person as distinct from all others. &mdash;Webster's Unabridged Dictionary </blockquote>\n<p>We all believe that human minds contain those special entities we call selves. But no one agrees about what they are. To keep things straight, I shall write <em>self</em> when speaking in a general sense about an entire person and reserve <em>Self</em> for talking about that more mysterious sense of personal identity. Here are some of the things people say about the Self:</p>\n<p>Self is the part of mind that's really me, or rather, it's the part of me &mdash; that is, part of my mind &mdash; that actually does the thinking and wanting and deciding and enjoying and</p>\n<p>suffering. It's the part that's most important to me because it's that which stays the same through all experience &mdash; the identity which ties everything together. And whether you can treat it scientifically or not, I know it's there, because it's me. Perhaps it's the sort of thing that Science can't explain.</p>\n<p>This isn't much of a definition, but I don't think it is a good idea to try to find a better one. It often does more harm than good to force definitions on things we don't understand. Besides, only in logic and mathematics do definitions ever capture concepts perfectly. The things we deal with in practical life are usually too complicated to be represented by neat, compact expressions. Especially when it comes to understanding minds, we still know so little that we can't be sure our ideas about psychology are even aimed in the right directions. In any case, one must not mistake defining things for knowing what they are. You can know what a tiger is without defining it. You may define a tiger, yet know scarcely anything about it.</p>\n<p>Even if our old ideas about the mind are wrong, we can learn a lot by trying to understand why we believe them. Instead of asking, <em>What are Selves?</em> we can ask, instead, <em>What are our ideas about Selves?</em> &mdash; and then we can ask, <em>What psychological functions do those ideas serve?</em> When we do this, it shows us that we do not have one such idea, but many.</p>\n<p>Our ideas about our Selves include beliefs about what we are. These include beliefs both about what we are capable of doing and about what we may be disposed to do. We exploit these beliefs whenever we solve problems or make plans. I'll refer to them, rather vaguely, as a person's self-images. In addition to our self-images, our ideas about ourselves also include ideas about what we'd like to be and ideas about what we ought to be. These, which I'll call a person's self-ideals, influence each person's growth from infancy, but we usually find them hard to express because they're inaccessible to consciousness.</p>",
    "text": "self n. 1. the identity, character, or essential qualities of any person or thing. 2. the identity, personality, individuality, etc. of a given person; one's own person as distinct from all others. \u2014Webster's Unabridged Dictionary\nWe all believe that human minds contain those special entities we call selves. But no one agrees about what they are. To keep things straight, I shall write self when speaking in a general sense about an entire person and reserve Self for talking about that more mysterious sense of personal identity. Here are some of the things people say about the Self:\nSelf is the part of mind that's really me, or rather, it's the part of me \u2014 that is, part of my mind \u2014 that actually does the thinking and wanting and deciding and enjoying and\nsuffering. It's the part that's most important to me because it's that which stays the same through all experience \u2014 the identity which ties everything together. And whether you can treat it scientifically or not, I know it's there, because it's me. Perhaps it's the sort of thing that Science can't explain.\nThis isn't much of a definition, but I don't think it is a good idea to try to find a better one. It often does more harm than good to force definitions on things we don't understand. Besides, only in logic and mathematics do definitions ever capture concepts perfectly. The things we deal with in practical life are usually too complicated to be represented by neat, compact expressions. Especially when it comes to understanding minds, we still know so little that we can't be sure our ideas about psychology are even aimed in the right directions. In any case, one must not mistake defining things for knowing what they are. You can know what a tiger is without defining it. You may define a tiger, yet know scarcely anything about it.\nEven if our old ideas about the mind are wrong, we can learn a lot by trying to understand why we believe them. Instead of asking, What are Selves? we can ask, instead, What are our ideas about Selves? \u2014 and then we can ask, What psychological functions do those ideas serve? When we do this, it shows us that we do not have one such idea, but many.\nOur ideas about our Selves include beliefs about what we are. These include beliefs both about what we are capable of doing and about what we may be disposed to do. We exploit these beliefs whenever we solve problems or make plans. I'll refer to them, rather vaguely, as a person's self-images. In addition to our self-images, our ideas about ourselves also include ideas about what we'd like to be and ideas about what we ought to be. These, which I'll call a person's self-ideals, influence each person's growth from infancy, but we usually find them hard to express because they're inaccessible to consciousness.",
    "type": "article",
    "title": "4.1 the self",
    "docId": 257562034570,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 256830456194,
    "gburl": "http://aurellem.org/society-of-mind/som-4.1.html-diffbotxyz965834596",
    "lastCrawlTimeUTC": 1588764375,
    "timestamp": "Wed, 06 May 2020 11:26:15 GMT"
  },
  {
    "sentiment": 0.584,
    "images": [
      {
        "naturalHeight": 186,
        "width": 448,
        "diffbotUri": "image|3|223847756",
        "url": "http://aurellem.org/society-of-mind/illus/ch6/6-2.png",
        "naturalWidth": 448,
        "primary": true,
        "height": 186
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1707255215",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.6.html",
    "html": "<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<p>What do you think you're thinking now? You might reply, <em>Why, just the thoughts I'm thinking now!</em> And that makes sense, in ordinary life, where <em>now</em> means <em>at this moment in time.</em> But the meaning of <em>now</em> is far less clear for an agent inside a society.</p>\n<p>It takes some time for changes in one part of a mind to affect the other parts. There's always some delay.</p>\n<p>For example, suppose you meet your friend Jack. Your agencies for Voices and Faces may recognize Jack's voice and face, and both send messages to an agency Names, which may recall Jack's name. But Voices may also send a <em>word-message</em> to Quotes, a language-based agency that has a way to remember phrases Jack has said before, while Faces may also send a message to Places, an agency concerned with space, which might recall some earlier place in which Jack's face was seen.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch6/6-2.png\"/></figure>\n<p>Now suppose we could ask both Places and Quotes which had happened first, seeing Jack or hearing his voice? We'd get two different answers! Places will first detect the face &mdash; while Quotes will first detect the voice. The seeming order of events depends upon which message reached each agent first &mdash; so the seeming sequence of events differs from one agent to another. Each agent will react in its own, slightly different way &mdash; because it has been affected by a slightly different <em>causal history,</em> which spreads like a wave into the past.</p>\n<p>It is simply impossible, in general, for any agent P to know for certain what another agent Q is doing at precisely the same time. The best that P can do is send a query straight to Q and hope that Q can get a truthful message back before other agents change Q's state &mdash; or change its message along the way. No portion of a mind can ever know everything that is happening at the same time in all the other agencies. Because of this, each agency must have at least a slightly different sense both of what has happened in the past &mdash; and of what is happening <em>now.</em> Each different agent of the mind lives in a slightly different world of time.</p>",
    "text": "Your browser does not support the video tag.\nWhat do you think you're thinking now? You might reply, Why, just the thoughts I'm thinking now! And that makes sense, in ordinary life, where now means at this moment in time. But the meaning of now is far less clear for an agent inside a society.\nIt takes some time for changes in one part of a mind to affect the other parts. There's always some delay.\nFor example, suppose you meet your friend Jack. Your agencies for Voices and Faces may recognize Jack's voice and face, and both send messages to an agency Names, which may recall Jack's name. But Voices may also send a word-message to Quotes, a language-based agency that has a way to remember phrases Jack has said before, while Faces may also send a message to Places, an agency concerned with space, which might recall some earlier place in which Jack's face was seen.\nNow suppose we could ask both Places and Quotes which had happened first, seeing Jack or hearing his voice? We'd get two different answers! Places will first detect the face \u2014 while Quotes will first detect the voice. The seeming order of events depends upon which message reached each agent first \u2014 so the seeming sequence of events differs from one agent to another. Each agent will react in its own, slightly different way \u2014 because it has been affected by a slightly different causal history, which spreads like a wave into the past.\nIt is simply impossible, in general, for any agent P to know for certain what another agent Q is doing at precisely the same time. The best that P can do is send a query straight to Q and hope that Q can get a truthful message back before other agents change Q's state \u2014 or change its message along the way. No portion of a mind can ever know everything that is happening at the same time in all the other agencies. Because of this, each agency must have at least a slightly different sense both of what has happened in the past \u2014 and of what is happening now. Each different agent of the mind lives in a slightly different world of time.",
    "type": "article",
    "title": "6.6 momentary mental time",
    "tags": [
      {
        "score": 0.6585684418678284,
        "sentiment": 0,
        "count": 5,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6565695405006409,
        "sentiment": 0,
        "count": 2,
        "label": "Faces",
        "uri": "https://diffbot.com/entity/OgRn5uBDQMkyA1QMo_m2uBQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      }
    ],
    "docId": 56076878232,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 60572811710,
    "gburl": "http://aurellem.org/society-of-mind/som-6.6.html-diffbotxyz3901891297",
    "lastCrawlTimeUTC": 1588764302,
    "timestamp": "Wed, 06 May 2020 11:25:02 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-76501542",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.3.html",
    "html": "<p>We've barely started to see what minds must do to comprehend the simplest children's tales. Let's look again at the beginning of our party story.</p>\n<p>Mary was invited to Jack's party.</p>\n<p>How marvelous that sentence is! How much it says in just six words! Two characters are introduced and quickly cast in clear-cut roles. We learn that there will be a party soon, with Jack as the host and Mary a guest &mdash; provided she accepts the invitation. We also learn that this setting is established in the past.</p>\n<p>Those six short words tell even more. We can expect the story to focus on Mary's activities rather than Jack's &mdash; because <em>Mary</em> is the first word that attracts our attention. But to accomplish that, the narrator had to use a clever grammar-tactic. Normally, an English-language sentence begins with a phrase that describes the Actor responsible for some action, and we usually represent this with a simple Trans-frame.</p>\n<p>JACK INVIT &mdash; ed MARY Donor action verb Recipient In this <em>active verb</em> form of sentence-frame, the verb is sandwiched between two nouns; the first describes a Donor and the second describes a Recipient. However, if our storyteller actually used the active form of sentence-frame, it would tend to mislead the listener into expecting Jack to be the central character of the story &mdash; if only because he is mentioned first. Fortunately, English grammar provides an alternative sentence-frame in which the Recipient is mentioned first &mdash; and which never mentions the Donor at all!</p>\n<p>MARY was INVIT &mdash; ed Recipient was verb &mdash; ed.</p>\n<p>How does the understanding listener detect this <em>passive verb</em> sentence-frame? Some language-agent has to notice the way the verb is sandwiched between <em>was</em> and <em>-ed.</em> As soon as this special subframe is recognized, the language-agency will reassign the first noun, Mary, not to the Donor terminal, but to the Recipient terminal &mdash; and thus Mary is represented as receiving the invitation. Why don't we need to say who the donor is? Because in this case the listener can assume it by default. Specifically, the expression <em>Jack's party</em> evokes a <em>party-invitation frame,</em> and in such situations it is typical for the host &mdash; or the host's parents &mdash; to invite the party guests. By thus arousing familiar frames, we can say a great deal in a very few words.</p>",
    "text": "We've barely started to see what minds must do to comprehend the simplest children's tales. Let's look again at the beginning of our party story.\nMary was invited to Jack's party.\nHow marvelous that sentence is! How much it says in just six words! Two characters are introduced and quickly cast in clear-cut roles. We learn that there will be a party soon, with Jack as the host and Mary a guest \u2014 provided she accepts the invitation. We also learn that this setting is established in the past.\nThose six short words tell even more. We can expect the story to focus on Mary's activities rather than Jack's \u2014 because Mary is the first word that attracts our attention. But to accomplish that, the narrator had to use a clever grammar-tactic. Normally, an English-language sentence begins with a phrase that describes the Actor responsible for some action, and we usually represent this with a simple Trans-frame.\nJACK INVIT \u2014 ed MARY Donor action verb Recipient In this active verb form of sentence-frame, the verb is sandwiched between two nouns; the first describes a Donor and the second describes a Recipient. However, if our storyteller actually used the active form of sentence-frame, it would tend to mislead the listener into expecting Jack to be the central character of the story \u2014 if only because he is mentioned first. Fortunately, English grammar provides an alternative sentence-frame in which the Recipient is mentioned first \u2014 and which never mentions the Donor at all!\nMARY was INVIT \u2014 ed Recipient was verb \u2014 ed.\nHow does the understanding listener detect this passive verb sentence-frame? Some language-agent has to notice the way the verb is sandwiched between was and -ed. As soon as this special subframe is recognized, the language-agency will reassign the first noun, Mary, not to the Donor terminal, but to the Recipient terminal \u2014 and thus Mary is represented as receiving the invitation. Why don't we need to say who the donor is? Because in this case the listener can assume it by default. Specifically, the expression Jack's party evokes a party-invitation frame, and in such situations it is typical for the host \u2014 or the host's parents \u2014 to invite the party guests. By thus arousing familiar frames, we can say a great deal in a very few words.",
    "type": "article",
    "title": "26.3 sentence-frames",
    "docId": 66217181593,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 27827241386,
    "gburl": "http://aurellem.org/society-of-mind/som-26.3.html-diffbotxyz2725761299",
    "lastCrawlTimeUTC": 1588764351,
    "timestamp": "Wed, 06 May 2020 11:25:51 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|197236998",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-28.1.html",
    "html": "<p>Why do angry people act as though some measure of aggression must be spent and, when no proper object lies in reach, strike out and damage harmless things? It almost seems as though our feelings can accumulate like fluids bottled up inside. In earlier times, some scientists identified these quantities with substances like bile and blood. No one believes those theories now &mdash; yet still we often speak of having mental energy and momentum or of succumbing to depletion or inertia. Do <em>mental quantities</em> really exist within the mind? If so, how are they made and stored, brought forth and then spent? And what are their relations to the quantities and magnitudes we read about in technical books? The answer is that words like <em>energy</em> and <em>force</em> are not used with much precision in everyday psychology. They still have the connotations that they carried several centuries ago, when they referred to commonsense ideas about vitality. Then, <em>energy</em> referred to vigor of action and expression, and <em>force</em> referred to the binding strength of a commitment or to the fighting strength of an army.</p>\n<p>Modern scientists use a concept of energy that, though narrower and more precise, not only explains more perfectly why engines stop when they run out of fuel, but also applies to our bodies as well: each of the cells of which we're made, including those inside the brain, requires some chemical energy in the form of food and oxygen. Accordingly, the body as a whole can do only a limited amount of physical work before it needs another meal. Now many people naively assume that our higher-level mental processes have similar requirements and that they need some second form of fuel &mdash; a mythical form of mental energy &mdash; to keep from becoming bored or mentally exhausted. And yet that simply isn't true! If each of Builder's agents has physical energy enough to do its work, then Builder &mdash; as an agency &mdash; needs nothing more to do its work. Builder, after all, is but a name for a certain assembly of agents. It can't require anything its separate agents do not need.</p>\n<p>Machines and brains require ordinary energy to do their jobs &mdash; and need no other, mental forms of energy. Causality is quite enough to keep them working toward their goals.</p>\n<p>But if our higher-level processes require no extra quantities like fuels or energies, what makes it seem to us as though they do? Why do so many people talk about their <em>levels of mental or emotional energy</em>? Why do tedious and boring occupations make us feel <em>run down</em>? We all experience so many such phenomena that we cannot help thinking our minds depend on many kinds of <em>mental quantities</em> &mdash; yet scientists apparently have shown that no such quantities exist. How can we explain this? It is not enough to say, simply, that these phenomena are illusions; we must understand why the illusions appear and, if possible, determine what functions they serve. The next few sections show how various illusions of mental force and energy evolve as convenient ways for mental agencies to regulate their transactions, much as many human communities have discovered how to use money.</p>",
    "text": "Why do angry people act as though some measure of aggression must be spent and, when no proper object lies in reach, strike out and damage harmless things? It almost seems as though our feelings can accumulate like fluids bottled up inside. In earlier times, some scientists identified these quantities with substances like bile and blood. No one believes those theories now \u2014 yet still we often speak of having mental energy and momentum or of succumbing to depletion or inertia. Do mental quantities really exist within the mind? If so, how are they made and stored, brought forth and then spent? And what are their relations to the quantities and magnitudes we read about in technical books? The answer is that words like energy and force are not used with much precision in everyday psychology. They still have the connotations that they carried several centuries ago, when they referred to commonsense ideas about vitality. Then, energy referred to vigor of action and expression, and force referred to the binding strength of a commitment or to the fighting strength of an army.\nModern scientists use a concept of energy that, though narrower and more precise, not only explains more perfectly why engines stop when they run out of fuel, but also applies to our bodies as well: each of the cells of which we're made, including those inside the brain, requires some chemical energy in the form of food and oxygen. Accordingly, the body as a whole can do only a limited amount of physical work before it needs another meal. Now many people naively assume that our higher-level mental processes have similar requirements and that they need some second form of fuel \u2014 a mythical form of mental energy \u2014 to keep from becoming bored or mentally exhausted. And yet that simply isn't true! If each of Builder's agents has physical energy enough to do its work, then Builder \u2014 as an agency \u2014 needs nothing more to do its work. Builder, after all, is but a name for a certain assembly of agents. It can't require anything its separate agents do not need.\nMachines and brains require ordinary energy to do their jobs \u2014 and need no other, mental forms of energy. Causality is quite enough to keep them working toward their goals.\nBut if our higher-level processes require no extra quantities like fuels or energies, what makes it seem to us as though they do? Why do so many people talk about their levels of mental or emotional energy? Why do tedious and boring occupations make us feel run down? We all experience so many such phenomena that we cannot help thinking our minds depend on many kinds of mental quantities \u2014 yet scientists apparently have shown that no such quantities exist. How can we explain this? It is not enough to say, simply, that these phenomena are illusions; we must understand why the illusions appear and, if possible, determine what functions they serve. The next few sections show how various illusions of mental force and energy evolve as convenient ways for mental agencies to regulate their transactions, much as many human communities have discovered how to use money.",
    "type": "article",
    "title": "28.1 the myth of mental energy",
    "docId": 66649014693,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 80636707228,
    "gburl": "http://aurellem.org/society-of-mind/som-28.1.html-diffbotxyz282311365",
    "lastCrawlTimeUTC": 1588764267,
    "timestamp": "Wed, 06 May 2020 11:24:27 GMT"
  },
  {
    "sentiment": 0.847,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1815772790",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-3.5.html",
    "html": "<p>In any actual child's mind, the urge to Play competes with other demanding urges, such as Eat and Sleep. What happens if another agent wrests control from Play, and what happens to the agents Play controlled?</p>\n<p>Suppose that our child is called away, no matter whether by someone else or by an internal urge like Sleep. What happens to the processes remaining active in the mind? One part of the child may still want to play, while another part wants to sleep. Perhaps the child will knock the tower down with a sudden, vengeful kick. What does it mean when children make such scenes? Is it that inner discipline breaks down to cause those savage acts? Not necessarily. Those <em>childish</em> acts might still make sense in other ways.</p>\n<p>Smashing takes so little time that Wrecker, freed from Play's constraint, need persist for only one more kick to gain the satisfaction of a final crash.</p>\n<p>Though childish violence might seem senseless by itself, it serves to communicate frustration at the loss of goal. Even if the parent scolds, that just confirms how well the message was transmitted and received. Destructive acts can serve constructive goals by leaving fewer problems to be solved. That kick may leave a mess outside, yet tidy up the child's mind.</p>\n<p>When children smash their treasured toys, we shouldn't ask for the reason why &mdash; since no such act has a single cause. Besides, it isn't true in a human mind that, when Sleep starts, then Play must quit and all its agents have to cease. A real child can go to bed &mdash; yet still build towers in its head.</p>",
    "text": "In any actual child's mind, the urge to Play competes with other demanding urges, such as Eat and Sleep. What happens if another agent wrests control from Play, and what happens to the agents Play controlled?\nSuppose that our child is called away, no matter whether by someone else or by an internal urge like Sleep. What happens to the processes remaining active in the mind? One part of the child may still want to play, while another part wants to sleep. Perhaps the child will knock the tower down with a sudden, vengeful kick. What does it mean when children make such scenes? Is it that inner discipline breaks down to cause those savage acts? Not necessarily. Those childish acts might still make sense in other ways.\nSmashing takes so little time that Wrecker, freed from Play's constraint, need persist for only one more kick to gain the satisfaction of a final crash.\nThough childish violence might seem senseless by itself, it serves to communicate frustration at the loss of goal. Even if the parent scolds, that just confirms how well the message was transmitted and received. Destructive acts can serve constructive goals by leaving fewer problems to be solved. That kick may leave a mess outside, yet tidy up the child's mind.\nWhen children smash their treasured toys, we shouldn't ask for the reason why \u2014 since no such act has a single cause. Besides, it isn't true in a human mind that, when Sleep starts, then Play must quit and all its agents have to cease. A real child can go to bed \u2014 yet still build towers in its head.",
    "type": "article",
    "title": "3.5 destructiveness",
    "tags": [
      {
        "score": 0.7309386134147644,
        "sentiment": 0.102,
        "count": 3,
        "label": "Sleep",
        "uri": "https://diffbot.com/entity/OHbQ7lGIKP_yOL41orRkOKg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      },
      {
        "score": 0.5939691662788391,
        "sentiment": -0.135,
        "count": 8,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 179022856595,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 4880482717,
    "gburl": "http://aurellem.org/society-of-mind/som-3.5.html-diffbotxyz2978451933",
    "lastCrawlTimeUTC": 1588764323,
    "timestamp": "Wed, 06 May 2020 11:25:23 GMT"
  },
  {
    "sentiment": 0.3,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-633305407",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-5.6.html",
    "html": "<p>Isn't it remarkable that words can portray human individuals? You might suppose this should be impossible, considering how much there is to say. Then what permits a writer to depict such seemingly real personalities? It is because we all agree on so many things that are left unsaid. For example, we assume that all the characters are possessed of what we call <em>commonsense knowledge,</em> and we also agree on many generalities about what we call <em>human nature.</em></p>\n<p>Hostility evokes defensiveness. Frustration arouses aggression.</p>\n<p>We also recognize that individuals have particular qualities and traits of character.</p>\n<p>Jane is tidy. Mary's timid. Grace is smart. That's not the sort of thing Charles does. It's not his style.</p>\n<p>Why should traits like these exist? Humanists are prone to boast about how hard it is to grasp the measure of a mind. But let's ask instead, <em>What makes personalities so easy to portray?</em> Why, for example, should any person tend toward a general quality of being neat, rather than simply being tidy about some things and messy about others? Why should our personalities show such coherencies? How could it be that a system assembled from a million agencies can be described by short and simple strings of words? Here are some possible reasons.</p>\n<p>Selectivity: First we should face the fact that our images of other minds are often falsely clear. We tend to think of another person's <em>personality</em> in terms of that which we can describe &mdash; and tend to set aside the rest as though it simply weren't there. Style: To escape the effort of making decisions we consider unimportant, we tend to develop policies that become so systematic that they can be discerned from the outside and characterized as personal traits.</p>\n<p>Predictability: Because it is hard to maintain friendship without trust, we try to conform to the expectations of our friends. Then, to the extent that we frame our images of our associates in terms of traits, we find ourselves teaching ourselves to behave in accord with those same descriptions. Self-Reliance: Thus, over time, imagined traits can make themselves actual! For even to carry out our own plans, we must be able to predict what we ourselves are likely to do &mdash; and that will become easier the more we simplify ourselves.</p>\n<p>It's nice to be able to trust our friends, but we need to be able to trust ourselves. How can that be possible when we can't be sure what's in our own heads? One way to accomplish this is by thinking of ourselves in terms of traits &mdash; and then proceeding to train ourselves to behave according to those self-images. Still, a personality is merely the surface of a person. What we call traits are only the regularities we manage to perceive. We never really know ourselves because there are so many other processes and policies that never show themselves directly in our behavior but work behind the scenes.</p>",
    "text": "Isn't it remarkable that words can portray human individuals? You might suppose this should be impossible, considering how much there is to say. Then what permits a writer to depict such seemingly real personalities? It is because we all agree on so many things that are left unsaid. For example, we assume that all the characters are possessed of what we call commonsense knowledge, and we also agree on many generalities about what we call human nature.\nHostility evokes defensiveness. Frustration arouses aggression.\nWe also recognize that individuals have particular qualities and traits of character.\nJane is tidy. Mary's timid. Grace is smart. That's not the sort of thing Charles does. It's not his style.\nWhy should traits like these exist? Humanists are prone to boast about how hard it is to grasp the measure of a mind. But let's ask instead, What makes personalities so easy to portray? Why, for example, should any person tend toward a general quality of being neat, rather than simply being tidy about some things and messy about others? Why should our personalities show such coherencies? How could it be that a system assembled from a million agencies can be described by short and simple strings of words? Here are some possible reasons.\nSelectivity: First we should face the fact that our images of other minds are often falsely clear. We tend to think of another person's personality in terms of that which we can describe \u2014 and tend to set aside the rest as though it simply weren't there. Style: To escape the effort of making decisions we consider unimportant, we tend to develop policies that become so systematic that they can be discerned from the outside and characterized as personal traits.\nPredictability: Because it is hard to maintain friendship without trust, we try to conform to the expectations of our friends. Then, to the extent that we frame our images of our associates in terms of traits, we find ourselves teaching ourselves to behave in accord with those same descriptions. Self-Reliance: Thus, over time, imagined traits can make themselves actual! For even to carry out our own plans, we must be able to predict what we ourselves are likely to do \u2014 and that will become easier the more we simplify ourselves.\nIt's nice to be able to trust our friends, but we need to be able to trust ourselves. How can that be possible when we can't be sure what's in our own heads? One way to accomplish this is by thinking of ourselves in terms of traits \u2014 and then proceeding to train ourselves to behave according to those self-images. Still, a personality is merely the surface of a person. What we call traits are only the regularities we manage to perceive. We never really know ourselves because there are so many other processes and policies that never show themselves directly in our behavior but work behind the scenes.",
    "type": "article",
    "title": "5.6 traits",
    "tags": [
      {
        "score": 0.5912021994590759,
        "sentiment": 0.914,
        "count": 1,
        "label": "Jane",
        "uri": "https://diffbot.com/entity/BXU6o2k0QMLuZRp1BLBw67g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/PeriodicalLiterature",
          "http://dbpedia.org/ontology/Magazine",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5381734371185303,
        "sentiment": 0.379,
        "count": 1,
        "label": "Grace",
        "uri": "https://diffbot.com/entity/X4S_zQmVfNf2IK9A1HD5gdw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5121772885322571,
        "sentiment": -0.389,
        "count": 2,
        "label": "personality psychology",
        "uri": "https://diffbot.com/entity/XJvHWLv7TN_K2zL-d5SqqBA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5045353174209595,
        "sentiment": 0,
        "count": 1,
        "label": "Charles I of England",
        "uri": "https://diffbot.com/entity/PG0X39-feNYGnW-HhWogSTg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.501019299030304,
        "sentiment": -0.232,
        "count": 1,
        "label": "How Hard It Is",
        "uri": "https://diffbot.com/entity/XI_-LvsIYOvS6Z6cuapZRZA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 182911730087,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 25282314658,
    "gburl": "http://aurellem.org/society-of-mind/som-5.6.html-diffbotxyz3536119056",
    "lastCrawlTimeUTC": 1588764237,
    "timestamp": "Wed, 06 May 2020 11:23:57 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-525603149",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-30.1.html",
    "html": "<p>What does <em>knowing</em> really mean? Suppose Mary (or some other creature or machine) can answer certain questions about the world &mdash; without the need to do any actual experiments. Then we'd agree that Mary knows those things about the world. But what would it mean, to you or to me, to hear Jack say that <em>Mary knows geometry</em>? For all we know, Mary might believe that circles are squares and it happens that Jack agrees! Jack's statement tells us more about Jack than about Mary.</p>\n<p>When Jack says, <em>Mary knows geometry,</em> this indicates to us that Jack would probably be satisfied by Mary's answers to the questions about geometry that he would be disposed to ask.</p>\n<p>The meaning of <em>Mary knows geometry</em> depends on who is saying it. After all, no one knows everything about geometry; that statement would not mean the same to us as to a mathematician, whose concepts of geometry are different from those of ordinary persons. In the same way, the meanings of many other terms depend upon the speaker's role. Even an apparently unambiguous statement like <em>This is a painting of a horse</em> shares this character: you can be sure of little more than that it displays a representation that in someone's view resembles a horse in some respects.</p>\n<p>Then why, when we talk about knowledge, don't we have to say who all those speakers and observers are? Because we make assumptions by default. When a stranger says that Mary knows geometry, we simply assume that the speaker would expect any typical person who knows Mary to agree that she knows geometry. Assumptions like that allow us to communicate; unless there is some reason to think otherwise, we assume that all the things involved are <em>typical.</em> It does not bother us that a professional mathematician might not agree that Mary knows geometry &mdash; because a mathematician doesn't fit our stereotype of a <em>typical person.</em></p>\n<p>You might maintain that none of this applies to you, since you know what you know about geometry. But there's still an observer on the scene, only now it is hiding inside your mind &mdash; namely, the portion of <em>you</em> that claims you know geometry. But the part of you that makes this claim has little in common with the other parts that actually do geometry for you; those agencies are probably incapable of speech and probably devoid of thoughts about your knowledge and beliefs.</p>\n<p>Naturally, we'd all prefer to think of knowledge as more positive and less provisional or relative. But little good has ever come from trying to link what we believe to our ideals about absolute truths. We always yearn for certainty, but the only thing beyond dispute is that there's always room for doubt. And doubt is not an enemy that sets constraints on what we know; the real danger to mental growth is perfect faith, doubt's antidote.</p>",
    "text": "What does knowing really mean? Suppose Mary (or some other creature or machine) can answer certain questions about the world \u2014 without the need to do any actual experiments. Then we'd agree that Mary knows those things about the world. But what would it mean, to you or to me, to hear Jack say that Mary knows geometry? For all we know, Mary might believe that circles are squares and it happens that Jack agrees! Jack's statement tells us more about Jack than about Mary.\nWhen Jack says, Mary knows geometry, this indicates to us that Jack would probably be satisfied by Mary's answers to the questions about geometry that he would be disposed to ask.\nThe meaning of Mary knows geometry depends on who is saying it. After all, no one knows everything about geometry; that statement would not mean the same to us as to a mathematician, whose concepts of geometry are different from those of ordinary persons. In the same way, the meanings of many other terms depend upon the speaker's role. Even an apparently unambiguous statement like This is a painting of a horse shares this character: you can be sure of little more than that it displays a representation that in someone's view resembles a horse in some respects.\nThen why, when we talk about knowledge, don't we have to say who all those speakers and observers are? Because we make assumptions by default. When a stranger says that Mary knows geometry, we simply assume that the speaker would expect any typical person who knows Mary to agree that she knows geometry. Assumptions like that allow us to communicate; unless there is some reason to think otherwise, we assume that all the things involved are typical. It does not bother us that a professional mathematician might not agree that Mary knows geometry \u2014 because a mathematician doesn't fit our stereotype of a typical person.\nYou might maintain that none of this applies to you, since you know what you know about geometry. But there's still an observer on the scene, only now it is hiding inside your mind \u2014 namely, the portion of you that claims you know geometry. But the part of you that makes this claim has little in common with the other parts that actually do geometry for you; those agencies are probably incapable of speech and probably devoid of thoughts about your knowledge and beliefs.\nNaturally, we'd all prefer to think of knowledge as more positive and less provisional or relative. But little good has ever come from trying to link what we believe to our ideals about absolute truths. We always yearn for certainty, but the only thing beyond dispute is that there's always room for doubt. And doubt is not an enemy that sets constraints on what we know; the real danger to mental growth is perfect faith, doubt's antidote.",
    "type": "article",
    "title": "30.1 knowing",
    "docId": 12828885414,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 173640040863,
    "gburl": "http://aurellem.org/society-of-mind/som-30.1.html-diffbotxyz2070267998",
    "lastCrawlTimeUTC": 1588764156,
    "timestamp": "Wed, 06 May 2020 11:22:36 GMT"
  },
  {
    "sentiment": 0.451,
    "humanLanguage": "en",
    "diffbotUri": "article|3|549381626",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.3.html",
    "html": "<p>How does an insubstantial word like <em>apple</em> lead you to think of a real thing &mdash; an object of a certain size that is red, round, sweet, and has a shiny, thin-peeled skin? How could a plain acoustic sound produce such complex states of mind, involving all those qualities of color, substance, taste, and shape? Presumably, each different quality involves a different agency. But then &mdash; in view of all we've said about why different agents can't communicate &mdash; how could such varying recipients all <em>understand</em> the selfsame messages? Do language- agents have unusual abilities to communicate with different kinds of agencies?</p>\n<p>Many people have tried to explain language as though it were separate from the rest of psychology. Indeed, the study of language itself was often divided into smaller subjects, called by traditional names like syntax, grammar, and semantics. But because there was no larger, coherent theory of thinking to which to attach those fragments, they tended to lose contact with one another and with reality. Once we assume that language and thought are different things, we're lost in trying to piece together what was never separate in the first place. This is why, in the following pages, I'll put aside most of the old language theories and return to the questions that led to them:</p>\n<p>How are words involved with mental processes? How does language enable people to communicate?</p>\n<p>In the next few sections, we'll introduce two kinds of agents that contribute to the power of words. The first kind, called <em>polynemes,</em> are involved with our long-term memories. A polyneme is a type of K-line; it sends the same, simple signal to many different agencies: each of those agencies must learn, for itself, what to do when it receives that signal. When you hear the word <em>apple,</em> a certain polyneme is aroused, and the signal from this polyneme will put your Color agency into a state that represents redness. The same signal will set your Shape agency into a state that represents roundness, and so forth. Thus, the polyneme for <em>apple</em> is really very simple; it knows nothing whatever about apples, colors, shapes, or anything else. It is merely a switch that turns on processes in other agencies, each of which has learned to respond in its own way.</p>\n<p>Later we'll discuss another type of language-agent that we'll call an <em>isonome.</em> Each isonome controls a short-term memory in each of many agencies. For example, suppose we had just been talking about a certain apple, and then I said, <em>Please put it in this pail.</em> In this case, you would assume that the word <em>it</em> refers to the apple. However, if we had been discussing your left shoe, you would assume <em>it</em> referred to that shoe. A word like <em>it</em> excites an isonome whose signal has no particular significance by itself, but controls what various agencies do with certain recent memories.</p>",
    "text": "How does an insubstantial word like apple lead you to think of a real thing \u2014 an object of a certain size that is red, round, sweet, and has a shiny, thin-peeled skin? How could a plain acoustic sound produce such complex states of mind, involving all those qualities of color, substance, taste, and shape? Presumably, each different quality involves a different agency. But then \u2014 in view of all we've said about why different agents can't communicate \u2014 how could such varying recipients all understand the selfsame messages? Do language- agents have unusual abilities to communicate with different kinds of agencies?\nMany people have tried to explain language as though it were separate from the rest of psychology. Indeed, the study of language itself was often divided into smaller subjects, called by traditional names like syntax, grammar, and semantics. But because there was no larger, coherent theory of thinking to which to attach those fragments, they tended to lose contact with one another and with reality. Once we assume that language and thought are different things, we're lost in trying to piece together what was never separate in the first place. This is why, in the following pages, I'll put aside most of the old language theories and return to the questions that led to them:\nHow are words involved with mental processes? How does language enable people to communicate?\nIn the next few sections, we'll introduce two kinds of agents that contribute to the power of words. The first kind, called polynemes, are involved with our long-term memories. A polyneme is a type of K-line; it sends the same, simple signal to many different agencies: each of those agencies must learn, for itself, what to do when it receives that signal. When you hear the word apple, a certain polyneme is aroused, and the signal from this polyneme will put your Color agency into a state that represents redness. The same signal will set your Shape agency into a state that represents roundness, and so forth. Thus, the polyneme for apple is really very simple; it knows nothing whatever about apples, colors, shapes, or anything else. It is merely a switch that turns on processes in other agencies, each of which has learned to respond in its own way.\nLater we'll discuss another type of language-agent that we'll call an isonome. Each isonome controls a short-term memory in each of many agencies. For example, suppose we had just been talking about a certain apple, and then I said, Please put it in this pail. In this case, you would assume that the word it refers to the apple. However, if we had been discussing your left shoe, you would assume it referred to that shoe. A word like it excites an isonome whose signal has no particular significance by itself, but controls what various agencies do with certain recent memories.",
    "type": "article",
    "title": "19.3 words and ideas",
    "tags": [
      {
        "score": 0.5622240304946899,
        "sentiment": 0.533,
        "count": 5,
        "label": "apple",
        "uri": "https://diffbot.com/entity/XgrXdplpfMRWDa_VBAOd2oA"
      },
      {
        "score": 0.5172280073165894,
        "sentiment": 0,
        "count": 1,
        "label": "The Wheel of Time",
        "uri": "https://diffbot.com/entity/XeS1jHtwMPvyBtw14wIth_w",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      }
    ],
    "docId": 150876307870,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 4830216590,
    "gburl": "http://aurellem.org/society-of-mind/som-19.3.html-diffbotxyz3925356999",
    "lastCrawlTimeUTC": 1588764104,
    "timestamp": "Wed, 06 May 2020 11:21:44 GMT"
  },
  {
    "sentiment": -0.619,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-784220786",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.4.html",
    "html": "<p>One frustration every teacher knows arises when a child learns a subject well enough to pass a test, yet never puts that skill to use on problems met in <em>real life.</em> It doesn't often help to scold but it sometimes helps to explain, through examples, how to apply the concept to other contexts. Why do some children seem to do this for themselves, automatically and spontaneously, while others seem to have to learn essentially the same thing over and over in different domains? Why are some children better than others at <em>transfer of learning</em> from one domain to another? It doesn't explain anything to say that those children are <em>smarter,</em> <em>brighter,</em> or <em>more intelligent.</em> Such vaguely defined capacities vary greatly even among different parts of the same mind.</p>\n<p>The power of what we learn depends on how we represent it in our minds. We've seen how the same experience can lead to learning different action scripts by replacing certain polynemes with isonomes.</p>\n<p>Certain of those versions will apply only to specific situations, others will apply to many more situations, and yet others will be so general and vague as to lead only to confusion. Some children learn to represent knowledge in versatile ways; others end up with accumulations of inflexible, single-purpose procedures or with almost useless generalities. How do children acquire their <em>representation skills</em> in the first place? An educational environment can lead a child to build large, complicated processes from smaller ones by laying out sequences of steps. Good teachers know what size to make each step and can often suggest analogies to help the child's mind to use what it already knows for building larger scripts and processes. By making each step small enough, we can keep the child from getting lost in unfamiliar worlds of meaningless alternatives; then the child will remain able to use previous skills to test and modify the growing new structures. But when a new fragment of knowledge or process constitutes too abrupt a break from the past, then none of the child's old recognizers and action scripts will apply to it; the child will get stuck, and <em>transfer of learning</em> won't occur. Why are some children better than others at <em>teaching themselves</em> to make changes inside their minds?</p>\n<p>Each child learns, from time to time, various better ways to learn &mdash; but no one understands how this is done. We tend to speak about <em>intelligence</em> because we find it virtually impossible to understand how this is done from watching only what the child does. The problem is that one can't observe a child's strategies for <em>learning how to learn</em> &mdash; because those strategies are twice removed from what we can see. It is hard enough to guess the character of the A-brain systems that directly cause those actions. Think how much more difficult it would be for an observer to imagine the multilayer teacher-learner structures that must have worked inside the child to train the A-brain agencies! And that observer has no way at all to guess what crucial <em>lucky accidents</em> may have led those hidden B-brains to persistent concerns with finding better ways to learn. Perhaps our educational research should be less concerned with teaching children to acquire particular skills and more concerned with how we learn to learn.</p>",
    "text": "One frustration every teacher knows arises when a child learns a subject well enough to pass a test, yet never puts that skill to use on problems met in real life. It doesn't often help to scold but it sometimes helps to explain, through examples, how to apply the concept to other contexts. Why do some children seem to do this for themselves, automatically and spontaneously, while others seem to have to learn essentially the same thing over and over in different domains? Why are some children better than others at transfer of learning from one domain to another? It doesn't explain anything to say that those children are smarter, brighter, or more intelligent. Such vaguely defined capacities vary greatly even among different parts of the same mind.\nThe power of what we learn depends on how we represent it in our minds. We've seen how the same experience can lead to learning different action scripts by replacing certain polynemes with isonomes.\nCertain of those versions will apply only to specific situations, others will apply to many more situations, and yet others will be so general and vague as to lead only to confusion. Some children learn to represent knowledge in versatile ways; others end up with accumulations of inflexible, single-purpose procedures or with almost useless generalities. How do children acquire their representation skills in the first place? An educational environment can lead a child to build large, complicated processes from smaller ones by laying out sequences of steps. Good teachers know what size to make each step and can often suggest analogies to help the child's mind to use what it already knows for building larger scripts and processes. By making each step small enough, we can keep the child from getting lost in unfamiliar worlds of meaningless alternatives; then the child will remain able to use previous skills to test and modify the growing new structures. But when a new fragment of knowledge or process constitutes too abrupt a break from the past, then none of the child's old recognizers and action scripts will apply to it; the child will get stuck, and transfer of learning won't occur. Why are some children better than others at teaching themselves to make changes inside their minds?\nEach child learns, from time to time, various better ways to learn \u2014 but no one understands how this is done. We tend to speak about intelligence because we find it virtually impossible to understand how this is done from watching only what the child does. The problem is that one can't observe a child's strategies for learning how to learn \u2014 because those strategies are twice removed from what we can see. It is hard enough to guess the character of the A-brain systems that directly cause those actions. Think how much more difficult it would be for an observer to imagine the multilayer teacher-learner structures that must have worked inside the child to train the A-brain agencies! And that observer has no way at all to guess what crucial lucky accidents may have led those hidden B-brains to persistent concerns with finding better ways to learn. Perhaps our educational research should be less concerned with teaching children to acquire particular skills and more concerned with how we learn to learn.",
    "type": "article",
    "title": "22.4 learning and teaching",
    "tags": [
      {
        "score": 0.6250883340835571,
        "sentiment": 0.208,
        "count": 1,
        "label": "teacher",
        "uri": "https://diffbot.com/entity/X68Svpe4cMVuxINSKHJg2-g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5746153593063354,
        "sentiment": 0,
        "count": 2,
        "label": "screenplay",
        "uri": "https://diffbot.com/entity/X8x_vrN0xMLW135RLD8gi7A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5723536014556885,
        "sentiment": 0.43,
        "count": 17,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 228500898178,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 6660538797,
    "gburl": "http://aurellem.org/society-of-mind/som-22.4.html-diffbotxyz3430707728",
    "lastCrawlTimeUTC": 1588764179,
    "timestamp": "Wed, 06 May 2020 11:22:59 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1633364604",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.4.html",
    "html": "<p>Most language-words are linked to many different polynemes, which correspond to the many <em>meaning-senses</em> of each word. To arouse so many polynemes at once would often lead to conflicts as each tries to set one's agencies into different states at the same time. If there are no other contextual clues, some of these conflicts would be resolved in accord with their connection strengths. For example, upon hearing <em>The astronomer married the star,</em> a playwright would tend to give priority to the theatrical sense of <em>star,</em> whereas an astronomer would think first of a distant sun, other things being equal.</p>\n<p>But other things are not usually equal. At every moment a person's mind is already involved with some <em>context</em> in which many agents are actively aroused. Because of this, as each new word arouses different polynemes, these will compete to change the states of those agents. Some of those changes will gain support as certain combinations of agents reinforce one another. Others that lose support and are left to stand alone will tend to weaken, and most ambiguities will thus be weeded out. In a few cycles, the entire system will firmly <em>lock in</em> on one meaning-sense for each word and firmly suppress the rest.</p>\n<p>A computer program that actually worked this way was developed by Jordan Pollack and David Waltz. When applied to the sentence, <em>John shot two bucks,</em> and supplied with the faintest context clue, the program would indeed usually settle into a single, consistent interpretation. In other words, after a few cycles, the agents ended up in a pattern of mutually supporting activities in which only one sense of each word remained strongly active while all the other meaning- senses were suppressed. Thereafter, whether this <em>alliance</em> of word-senses was involved with hunting or with gambling, it became so self-supporting that it could resist any subsequent small signal from outside. In effect, the system had found a stable, unambiguous interpretation of the sentence.</p>\n<p>What can be done if such a system settles on a wrong interpretation? Suppose, for example, that an <em>outdoors</em> clue had already made the system decide that John was hunting, but later it was told that John was gambling in the woods. Since a single new context clue might not be able to overcome an established alliance of meaning-senses, it might be necessary for some higher-level agency to start the system out afresh. What if the end result of locking-in were unacceptable to other agencies? Simply repeating the process would only lead to making the same mistake again. One way to prevent that would be to record which meaning-senses were adopted in the previous cycle and suppress them temporarily at the start of the next cycle. This would probably produce a new interpretation.</p>\n<p>There is no guarantee that this method will always find an interpretation that yields a meaning consistent with all the words of the sentence. Then, if the locking-in process should fail, the listener will be confused. There are other methods that one could attempt, for example, to imagine a new context and then restart the ring-closing process. But no single method will always work. To use the power of language, one must acquire many different ways to understand.</p>",
    "text": "Most language-words are linked to many different polynemes, which correspond to the many meaning-senses of each word. To arouse so many polynemes at once would often lead to conflicts as each tries to set one's agencies into different states at the same time. If there are no other contextual clues, some of these conflicts would be resolved in accord with their connection strengths. For example, upon hearing The astronomer married the star, a playwright would tend to give priority to the theatrical sense of star, whereas an astronomer would think first of a distant sun, other things being equal.\nBut other things are not usually equal. At every moment a person's mind is already involved with some context in which many agents are actively aroused. Because of this, as each new word arouses different polynemes, these will compete to change the states of those agents. Some of those changes will gain support as certain combinations of agents reinforce one another. Others that lose support and are left to stand alone will tend to weaken, and most ambiguities will thus be weeded out. In a few cycles, the entire system will firmly lock in on one meaning-sense for each word and firmly suppress the rest.\nA computer program that actually worked this way was developed by Jordan Pollack and David Waltz. When applied to the sentence, John shot two bucks, and supplied with the faintest context clue, the program would indeed usually settle into a single, consistent interpretation. In other words, after a few cycles, the agents ended up in a pattern of mutually supporting activities in which only one sense of each word remained strongly active while all the other meaning- senses were suppressed. Thereafter, whether this alliance of word-senses was involved with hunting or with gambling, it became so self-supporting that it could resist any subsequent small signal from outside. In effect, the system had found a stable, unambiguous interpretation of the sentence.\nWhat can be done if such a system settles on a wrong interpretation? Suppose, for example, that an outdoors clue had already made the system decide that John was hunting, but later it was told that John was gambling in the woods. Since a single new context clue might not be able to overcome an established alliance of meaning-senses, it might be necessary for some higher-level agency to start the system out afresh. What if the end result of locking-in were unacceptable to other agencies? Simply repeating the process would only lead to making the same mistake again. One way to prevent that would be to record which meaning-senses were adopted in the previous cycle and suppress them temporarily at the start of the next cycle. This would probably produce a new interpretation.\nThere is no guarantee that this method will always find an interpretation that yields a meaning consistent with all the words of the sentence. Then, if the locking-in process should fail, the listener will be confused. There are other methods that one could attempt, for example, to imagine a new context and then restart the ring-closing process. But no single method will always work. To use the power of language, one must acquire many different ways to understand.",
    "type": "article",
    "title": "20.4 locking-in and weeding-out",
    "docId": 246520218020,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 104298086843,
    "gburl": "http://aurellem.org/society-of-mind/som-20.4.html-diffbotxyz3226592047",
    "lastCrawlTimeUTC": 1588764210,
    "timestamp": "Wed, 06 May 2020 11:23:30 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|530307850",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.9.html",
    "html": "<p>When we watch a ball roll down a slope, we notice it seems to try to get around obstacles that lie in its path. If we didn't know about gravity, we might be tempted to think that the ball has the goal of moving down. But we know that the ball isn't <em>trying</em> to do anything; the impression of intention is only in the watcher's mind.</p>\n<p>When we experiment with Builder we also get the sense that it has a goal. Whenever you take its blocks away, it reaches out and takes them back. Whenever you knock its tower down, it rebuilds it. It seems to want a tower there, and it perseveres until the tower is done. Certainly Builder seems smarter than the rolling ball because it overcomes more complicated obstacles. But once we know how Builder works, we see that it's not so different from that ball: all it does is keep on finding blocks and putting them on top of other blocks. Does Builder really have a goal?</p>\n<p>One ingredient of having a goal is persistence. We wouldn't say that Builder wants a tower, if it didn't keep persisting in attempts to build one. But persistence alone is not enough &mdash; and neither Builder nor that rolling ball have any sense of where they want to go. The other critical ingredient of goal is to have some image or description of a wanted or desired state. Before we'd agree that Builder wants a tower, we'd have to make sure that it contains something like an image or a description of a tower. The idea of a difference-engine embodies both elements: a representation of some outcome and a mechanism to make it persist until that outcome is achieved.</p>\n<p>Do difference-engines <em>really</em> want? It is futile to ask that kind of question because it seeks a distinction where none exists &mdash; except in some observer's mind. We can think of a ball as a perfectly passive object that merely reacts to external forces. But the eighteenth- century physicist Jean Le Rond d'Alembert showed that one can also perfectly predict the behavior of a rolling ball by describing it as a difference-engine whose goal is to reduce its own energy. We need not force ourselves to decide questions like whether machines can have goals or not. Words should be our servants, not our masters. The notion of goal makes it easy to describe certain aspects of what people and machines can do; it offers us the opportunity to use simple descriptions in terms of active purposes instead of using unmanageably cumbersome descriptions of machinery.</p>\n<p>To be sure, this doesn't capture everything that people mean by <em>having goals.</em> We humans have so many ways of wanting things that no one scheme can embrace them all. Nevertheless, this idea has already led to many important developments both in Artificial Intelligence and in psychology. The difference-engine scheme remains the most useful conception of goal, purpose, or intention yet discovered.</p>",
    "text": "When we watch a ball roll down a slope, we notice it seems to try to get around obstacles that lie in its path. If we didn't know about gravity, we might be tempted to think that the ball has the goal of moving down. But we know that the ball isn't trying to do anything; the impression of intention is only in the watcher's mind.\nWhen we experiment with Builder we also get the sense that it has a goal. Whenever you take its blocks away, it reaches out and takes them back. Whenever you knock its tower down, it rebuilds it. It seems to want a tower there, and it perseveres until the tower is done. Certainly Builder seems smarter than the rolling ball because it overcomes more complicated obstacles. But once we know how Builder works, we see that it's not so different from that ball: all it does is keep on finding blocks and putting them on top of other blocks. Does Builder really have a goal?\nOne ingredient of having a goal is persistence. We wouldn't say that Builder wants a tower, if it didn't keep persisting in attempts to build one. But persistence alone is not enough \u2014 and neither Builder nor that rolling ball have any sense of where they want to go. The other critical ingredient of goal is to have some image or description of a wanted or desired state. Before we'd agree that Builder wants a tower, we'd have to make sure that it contains something like an image or a description of a tower. The idea of a difference-engine embodies both elements: a representation of some outcome and a mechanism to make it persist until that outcome is achieved.\nDo difference-engines really want? It is futile to ask that kind of question because it seeks a distinction where none exists \u2014 except in some observer's mind. We can think of a ball as a perfectly passive object that merely reacts to external forces. But the eighteenth- century physicist Jean Le Rond d'Alembert showed that one can also perfectly predict the behavior of a rolling ball by describing it as a difference-engine whose goal is to reduce its own energy. We need not force ourselves to decide questions like whether machines can have goals or not. Words should be our servants, not our masters. The notion of goal makes it easy to describe certain aspects of what people and machines can do; it offers us the opportunity to use simple descriptions in terms of active purposes instead of using unmanageably cumbersome descriptions of machinery.\nTo be sure, this doesn't capture everything that people mean by having goals. We humans have so many ways of wanting things that no one scheme can embrace them all. Nevertheless, this idea has already led to many important developments both in Artificial Intelligence and in psychology. The difference-engine scheme remains the most useful conception of goal, purpose, or intention yet discovered.",
    "type": "article",
    "title": "7.9 intentions",
    "docId": 263124484495,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 174277362103,
    "gburl": "http://aurellem.org/society-of-mind/som-7.9.html-diffbotxyz1929937992",
    "lastCrawlTimeUTC": 1588764131,
    "timestamp": "Wed, 06 May 2020 11:22:11 GMT"
  },
  {
    "sentiment": 0.454,
    "images": [
      {
        "naturalHeight": 197,
        "width": 401,
        "diffbotUri": "image|3|309308639",
        "url": "http://aurellem.org/society-of-mind/illus/ch15/15-3.png",
        "naturalWidth": 401,
        "primary": true,
        "height": 197
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-200487010",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.8.html",
    "html": "<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<p>What controls the working of the mind from one moment to the next? How do we keep our place when doing complicated jobs, so that when interrupted from outside &mdash; or by another thought from inside &mdash; we can <em>get back</em> to where we were, instead of having to start all over again? How do we keep in mind which things we've tried and what we've learned along the way, so that we don't go round and round in loops?</p>\n<p>No one yet knows how memories control themselves inside our brain; perhaps each major agency has somewhat different processes, each suited to the special kinds of jobs it does. The diagram below suggests some of the sorts of memory-machinery we'd expect to find inside a typical large agency.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch15/15-3.png\"/></figure>\n<p>We'll assume that every substantial agency has several <em>micromemory-units,</em> each of which is a sort of temporary K-line that can quickly store or restore the state of many of the agents in that agency. Each agency also has several <em>short-term memory-units,</em> which can, in turn, store or restore the states of the micromemories themselves. When any of these temporary memory-units are reused, the information that was stored in them is erased &mdash; unless it has somehow been <em>transferred</em> into more <em>permanent</em> or <em>long-term</em> memory-systems. There is good evidence that, in human brains, the processes that transfer information into long-term memory are very slow, requiring time intervals that range from minutes to hours. Accordingly, most temporary memories are permanently lost.</p>\n<p>A growing child acquires many ways to control all these mechanisms. Accordingly, our diagram includes the flow of information among the other agencies. Since this memory-controlling agency must also learn and remember, our diagram includes a memory-system for it as well.</p>",
    "text": "Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag.\nWhat controls the working of the mind from one moment to the next? How do we keep our place when doing complicated jobs, so that when interrupted from outside \u2014 or by another thought from inside \u2014 we can get back to where we were, instead of having to start all over again? How do we keep in mind which things we've tried and what we've learned along the way, so that we don't go round and round in loops?\nNo one yet knows how memories control themselves inside our brain; perhaps each major agency has somewhat different processes, each suited to the special kinds of jobs it does. The diagram below suggests some of the sorts of memory-machinery we'd expect to find inside a typical large agency.\nWe'll assume that every substantial agency has several micromemory-units, each of which is a sort of temporary K-line that can quickly store or restore the state of many of the agents in that agency. Each agency also has several short-term memory-units, which can, in turn, store or restore the states of the micromemories themselves. When any of these temporary memory-units are reused, the information that was stored in them is erased \u2014 unless it has somehow been transferred into more permanent or long-term memory-systems. There is good evidence that, in human brains, the processes that transfer information into long-term memory are very slow, requiring time intervals that range from minutes to hours. Accordingly, most temporary memories are permanently lost.\nA growing child acquires many ways to control all these mechanisms. Accordingly, our diagram includes the flow of information among the other agencies. Since this memory-controlling agency must also learn and remember, our diagram includes a memory-system for it as well.",
    "type": "article",
    "title": "15.8 anatomy of memory",
    "tags": [
      {
        "score": 0.7581943869590759,
        "sentiment": -0.364,
        "count": 1,
        "label": "Neuroanatomy of memory",
        "uri": "https://diffbot.com/entity/Xvvxz64FXMFu0btDSl2LWrA"
      },
      {
        "score": 0.7262726426124573,
        "sentiment": 0,
        "count": 0,
        "label": "computer memory",
        "uri": "https://diffbot.com/entity/XWsavN-0wOLquQDMlQ-MDYw"
      },
      {
        "score": 0.6894887089729309,
        "sentiment": -0.489,
        "count": 1,
        "label": "computer data storage",
        "uri": "https://diffbot.com/entity/XkjiomTf4OOyZQYbFejhr5g"
      },
      {
        "score": 0.5727269053459167,
        "sentiment": -0.882,
        "count": 4,
        "label": "browser game",
        "uri": "https://diffbot.com/entity/XWqfGVRk5MNiKlDY006Ltbw"
      },
      {
        "score": 0.5466569066047668,
        "sentiment": -0.806,
        "count": 4,
        "label": "video art",
        "uri": "https://diffbot.com/entity/XQXzEmZu1MJKtizgHKbp4eg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5402954816818237,
        "sentiment": 0.371,
        "count": 1,
        "label": "Mclusky Do Dallas",
        "uri": "https://diffbot.com/entity/XBObujlWsOoGMtf49YZZjGg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5373119711875916,
        "sentiment": -0.133,
        "count": 1,
        "label": "Start All Over Again",
        "uri": "https://diffbot.com/entity/XPiATki4-MeG6LQzQANxKIg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Single"
        ]
      }
    ],
    "docId": 49470194049,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 13768016268,
    "gburl": "http://aurellem.org/society-of-mind/som-15.8.html-diffbotxyz3489770428",
    "lastCrawlTimeUTC": 1588764007,
    "timestamp": "Wed, 06 May 2020 11:20:07 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-167329463",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.8.html",
    "html": "<p>What enables us to tolerate an interruption and then return to our previous thoughts? This must engage the agents that control our short-term memories. It is important also to recognize that many interruptions come not only from outside, but also from inside the mind. For example, all but the simplest discourses make interruptions in the trains of thought they start. Consider this sentence:</p>\n<p>The thief who took the moon moved it to Paris.</p>\n<p>We can regard this as expressing one thought that is interrupted by another. The principal intention of the speaker is to express this Trans-frame:</p>\n<p>The thief moved the moon (from ?) to Paris. Actor Trans Object Origin Destination The speaker, realizing that the listener may not know who the thief was, interrupts the main sentence with a <em>relative clause</em> &mdash; <em>who took the moon</em> &mdash; to further describe that Actor thief. As it happens, this interrupting clause also has the form of a Trans-frame &mdash; so now the language-agency must deal with two such frames at once.</p>\n<p>Who took the moon (from ?) (to?) Actor Trans Object Origin Destination English tends to use certain wh words, like <em>which</em> and <em>who,</em> to interrupt a listener's language-agency and cause its short-term memories to temporarily store away some of their present pronome assignments. This provides the language-agency with more capacity to understand the interrupting phrase. In the case of the moon sentence, the word <em>who</em> instructs the listener to prepare to elaborate the description of the Actor thief. Once this is done, the language-agency can <em>re-member</em> its previous state in the process of understanding the main sentence. We can often tell when to use an interruption process even though the initial wh word is missing; however, this doesn't always work so well:</p>\n<p>The cotton clothing is made of is grown in the south.</p>\n<p>This sentence is confusing because the reader tends to treat the word <em>cotton</em> in <em>cotton clothing</em> as an adjective that modifies <em>clothing,</em> when the writer meant it as a noun. The same sentence is easier to understand when set in a larger context:</p>\n<p>Where do people grow the cotton that is used to make clothing? --- The cotton clothing is made of is grown in the south.</p>\n<p>The first sentence activates the noun sense of <em>cotton</em> and asks a question about that subject. Now a question is really a sort of command: it makes the reader focus attention on a certain subject. Here, it prepares the reader to add more structure to the representation of the cotton noun, so there is less need for an explicit interruption signal. Still, it is very curious how rarely we bother to use any signal at all for marking the end of an interrupting phrase. We never say a word that means <em>un-who.</em> Evidently, we're usually ready to assume that the interrupting phrase is complete.</p>",
    "text": "What enables us to tolerate an interruption and then return to our previous thoughts? This must engage the agents that control our short-term memories. It is important also to recognize that many interruptions come not only from outside, but also from inside the mind. For example, all but the simplest discourses make interruptions in the trains of thought they start. Consider this sentence:\nThe thief who took the moon moved it to Paris.\nWe can regard this as expressing one thought that is interrupted by another. The principal intention of the speaker is to express this Trans-frame:\nThe thief moved the moon (from ?) to Paris. Actor Trans Object Origin Destination The speaker, realizing that the listener may not know who the thief was, interrupts the main sentence with a relative clause \u2014 who took the moon \u2014 to further describe that Actor thief. As it happens, this interrupting clause also has the form of a Trans-frame \u2014 so now the language-agency must deal with two such frames at once.\nWho took the moon (from ?) (to?) Actor Trans Object Origin Destination English tends to use certain wh words, like which and who, to interrupt a listener's language-agency and cause its short-term memories to temporarily store away some of their present pronome assignments. This provides the language-agency with more capacity to understand the interrupting phrase. In the case of the moon sentence, the word who instructs the listener to prepare to elaborate the description of the Actor thief. Once this is done, the language-agency can re-member its previous state in the process of understanding the main sentence. We can often tell when to use an interruption process even though the initial wh word is missing; however, this doesn't always work so well:\nThe cotton clothing is made of is grown in the south.\nThis sentence is confusing because the reader tends to treat the word cotton in cotton clothing as an adjective that modifies clothing, when the writer meant it as a noun. The same sentence is easier to understand when set in a larger context:\nWhere do people grow the cotton that is used to make clothing? --- The cotton clothing is made of is grown in the south.\nThe first sentence activates the noun sense of cotton and asks a question about that subject. Now a question is really a sort of command: it makes the reader focus attention on a certain subject. Here, it prepares the reader to add more structure to the representation of the cotton noun, so there is less need for an explicit interruption signal. Still, it is very curious how rarely we bother to use any signal at all for marking the end of an interrupting phrase. We never say a word that means un-who. Evidently, we're usually ready to assume that the interrupting phrase is complete.",
    "type": "article",
    "title": "22.8 interruptions",
    "tags": [
      {
        "score": 0.6781666278839111,
        "sentiment": 0,
        "count": 2,
        "label": "Short-Term Memories",
        "uri": "https://diffbot.com/entity/X_T2HkD74PFWvQ0Z-k1c_dQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.6570388674736023,
        "sentiment": -0.266,
        "count": 4,
        "label": "actor",
        "uri": "https://diffbot.com/entity/X9qXzfDyuMNOCw-kCz5CNyQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6233183741569519,
        "sentiment": 0,
        "count": 2,
        "label": "Paris",
        "uri": "https://diffbot.com/entity/AAKTRl2QUPhKIGhFIvkHsng",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Settlement",
          "http://dbpedia.org/ontology/City",
          "http://dbpedia.org/ontology/Locality",
          "http://dbpedia.org/ontology/Capital",
          "http://dbpedia.org/ontology/Organisation"
        ]
      },
      {
        "score": 0.5763121247291565,
        "sentiment": -0.37,
        "count": 1,
        "label": "Destination English",
        "uri": "https://diffbot.com/entity/Bt927SysnPkSIf9T7HiEbpQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5426466464996338,
        "sentiment": 0,
        "count": 1,
        "label": "As It Happens",
        "uri": "https://diffbot.com/entity/XvcFZESA1MMq8ZTKwlXLpxQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/RadioProgram"
        ]
      },
      {
        "score": 0.5314107537269592,
        "sentiment": 0,
        "count": 1,
        "label": "Origin Systems",
        "uri": "https://diffbot.com/entity/Ccb_oHREFPvC2aSvyOr652A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5206834077835083,
        "sentiment": -0.348,
        "count": 3,
        "label": "theft",
        "uri": "https://diffbot.com/entity/X2WdGGz3qNYek52ZMhvyrpA"
      }
    ],
    "docId": 127492014519,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 88889688475,
    "gburl": "http://aurellem.org/society-of-mind/som-22.8.html-diffbotxyz2454412400",
    "lastCrawlTimeUTC": 1588763985,
    "timestamp": "Wed, 06 May 2020 11:19:45 GMT"
  },
  {
    "images": [
      {
        "naturalHeight": 118,
        "width": 379,
        "diffbotUri": "image|3|1690365470",
        "url": "http://aurellem.org/society-of-mind/illus/ch17/17-2.png",
        "naturalWidth": 379,
        "primary": true,
        "height": 118
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|802111150",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.2.html",
    "html": "<p>Suppose a child were playing in a certain way, and a stranger appeared and began to scold and criticize. The child would become frightened and disturbed and try to escape. But if, in the same situation, the child's parent arrived and proceeded to scold and criticize, the result would be different. Instead of being frightened, the child would feel guilty and ashamed, and instead of trying to escape, the child would try to change what it was doing, in attempts to seek reassurance and approval.</p>\n<p>I suspect that these two scenarios engage different learning mechanisms. In the encounter with the forbidding visitor, the child might learn <em>I should not try to achieve my present goal in this kind of situation.</em> But when scolded by someone to whom the child is <em>attached,</em> the child might learn <em>I ought not to want to achieve that goal at all!</em> In the first case, it is a matter of learning which goal to pursue in which circumstance; in the second instance, it is more a question of what goals one should have. If my theory is right, the presence of the attachment-person actually switches the effect of learning over to different sets of agents. To see the difference, let's make a small reformulation of the concept of a difference-engine to represent three different kinds of learning that an infant might use.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch17/17-2.png\"/></figure>\n<p>In the case of ordinary forms of failure or success signals, the learner modifies the methods used to reach the goal. In the case of fear-provoking disturbances, the learner may modify the description of the situation itself.</p>\n<p>In the case of attachment-related failure or reward signals, the learner modifies which goals are considered worthy of pursuit.</p>\n<p>So far as I know, this is a new theory about attachment. It asserts that there are particular types of learning that can proceed only in the presence of the particular individuals to whom one has become attached.</p>",
    "text": "Suppose a child were playing in a certain way, and a stranger appeared and began to scold and criticize. The child would become frightened and disturbed and try to escape. But if, in the same situation, the child's parent arrived and proceeded to scold and criticize, the result would be different. Instead of being frightened, the child would feel guilty and ashamed, and instead of trying to escape, the child would try to change what it was doing, in attempts to seek reassurance and approval.\nI suspect that these two scenarios engage different learning mechanisms. In the encounter with the forbidding visitor, the child might learn I should not try to achieve my present goal in this kind of situation. But when scolded by someone to whom the child is attached, the child might learn I ought not to want to achieve that goal at all! In the first case, it is a matter of learning which goal to pursue in which circumstance; in the second instance, it is more a question of what goals one should have. If my theory is right, the presence of the attachment-person actually switches the effect of learning over to different sets of agents. To see the difference, let's make a small reformulation of the concept of a difference-engine to represent three different kinds of learning that an infant might use.\nIn the case of ordinary forms of failure or success signals, the learner modifies the methods used to reach the goal. In the case of fear-provoking disturbances, the learner may modify the description of the situation itself.\nIn the case of attachment-related failure or reward signals, the learner modifies which goals are considered worthy of pursuit.\nSo far as I know, this is a new theory about attachment. It asserts that there are particular types of learning that can proceed only in the presence of the particular individuals to whom one has become attached.",
    "type": "article",
    "title": "17.2 attachment-learning",
    "docId": 161106706833,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 153046892966,
    "gburl": "http://aurellem.org/society-of-mind/som-17.2.html-diffbotxyz1272709135",
    "lastCrawlTimeUTC": 1588764028,
    "timestamp": "Wed, 06 May 2020 11:20:28 GMT"
  },
  {
    "images": [
      {
        "naturalHeight": 145,
        "width": 331,
        "diffbotUri": "image|3|1787097728",
        "url": "http://aurellem.org/society-of-mind/illus/ch21/21-4.png",
        "naturalWidth": 331,
        "primary": true,
        "height": 145
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1066462949",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.4.html",
    "html": "<p>If agents had huge minds like ours, they could talk the way people do &mdash; and Add could say, <em>Please, Get an apple and Put it in the pail.</em> Perhaps our largest agencies can deal with messages like that, but smaller agencies like Get cannot interpret such expressions because they're much too specialized to understand complicated wants and needs. Then how could Get know what to get &mdash; in order to find an apple rather than a block, a fork, or a paper doll? To examine this problem, we'll have to make some assumptions about what happens in a listener's mind. For the moment, let's simply assume that the result is to activate a Builder-like society with these ingredients:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch21/21-4.png\"/></figure>\n<p>At first sight, it seems as though all these agents are involved with the apple and the pail. But a closer look shows only the low-level agents Look-for and Grasp are actually concerned with the physical aspects of actual objects; all the others are merely <em>middle-level managers.</em> For example, the agent Get doesn't actually <em>get</em> anything; it only turns on Find and Grasp at the proper time. To be sure, Look-for will need some information about what to look for &mdash; that is, about an apple's appearance, and Move will need information about the apple's actual location. Nevertheless, we'll see that this information can become available to those agents without any need for messages from Get. To see how agents can operate without explicit messages, let's compare two ordinary language descriptions of how to put an apple into a pail. Our first script mentions each object explicitly in every line, the way one might speak to a novice.</p>\n<p>Look for an apple. Move the arm and hand to the apple's location. Prepare the hand to grasp an apple-shaped object. Grasp the apple. Now look for the pail. Move the arm and hand to the pail's location. Release the hand's grip on the apple.</p>\n<p>Now let's rewrite this description in a style more typical of normal speech.</p>\n<p>Look for an apple. Move the arm and hand to its location. Prepare the hand to grasp an object of that shape. Grasp it. Now look for the pail. Move the arm and hand to its location. Release the hand's grip.</p>\n<p>This second script uses the words <em>apple</em> and <em>pail</em> only once. This is how we usually speak; once something has been mentioned, we normally don't use its name again. Instead, whenever possible, we replace a name by a pronoun word. In the next few sections I'll argue that it is not only in language that we replace things with pronounlike tokens; we also do this in many other forms of thought. It could scarcely be otherwise, because in order to make sense to us, our sentences must mirror the structures and methods we use for managing our memories.</p>",
    "text": "If agents had huge minds like ours, they could talk the way people do \u2014 and Add could say, Please, Get an apple and Put it in the pail. Perhaps our largest agencies can deal with messages like that, but smaller agencies like Get cannot interpret such expressions because they're much too specialized to understand complicated wants and needs. Then how could Get know what to get \u2014 in order to find an apple rather than a block, a fork, or a paper doll? To examine this problem, we'll have to make some assumptions about what happens in a listener's mind. For the moment, let's simply assume that the result is to activate a Builder-like society with these ingredients:\nAt first sight, it seems as though all these agents are involved with the apple and the pail. But a closer look shows only the low-level agents Look-for and Grasp are actually concerned with the physical aspects of actual objects; all the others are merely middle-level managers. For example, the agent Get doesn't actually get anything; it only turns on Find and Grasp at the proper time. To be sure, Look-for will need some information about what to look for \u2014 that is, about an apple's appearance, and Move will need information about the apple's actual location. Nevertheless, we'll see that this information can become available to those agents without any need for messages from Get. To see how agents can operate without explicit messages, let's compare two ordinary language descriptions of how to put an apple into a pail. Our first script mentions each object explicitly in every line, the way one might speak to a novice.\nLook for an apple. Move the arm and hand to the apple's location. Prepare the hand to grasp an apple-shaped object. Grasp the apple. Now look for the pail. Move the arm and hand to the pail's location. Release the hand's grip on the apple.\nNow let's rewrite this description in a style more typical of normal speech.\nLook for an apple. Move the arm and hand to its location. Prepare the hand to grasp an object of that shape. Grasp it. Now look for the pail. Move the arm and hand to its location. Release the hand's grip.\nThis second script uses the words apple and pail only once. This is how we usually speak; once something has been mentioned, we normally don't use its name again. Instead, whenever possible, we replace a name by a pronoun word. In the next few sections I'll argue that it is not only in language that we replace things with pronounlike tokens; we also do this in many other forms of thought. It could scarcely be otherwise, because in order to make sense to us, our sentences must mirror the structures and methods we use for managing our memories.",
    "type": "article",
    "title": "21.4 communication among agents",
    "docId": 208242033042,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 208396534177,
    "gburl": "http://aurellem.org/society-of-mind/som-21.4.html-diffbotxyz3012390849",
    "lastCrawlTimeUTC": 1588764051,
    "timestamp": "Wed, 06 May 2020 11:20:51 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1757222342",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-13.6.html",
    "html": "<p>We can get more insight about children from another experiment of Piaget's. A child is shown a short block resting on a longer one and is asked to draw the scene. Next the child is asked to draw a sketch of what might happen if we pushed the upper block a little to the right. At first, the result is more or less what we'd expect.</p>\n<p>But when we ask the child to do the same repeatedly, we see a strange result. The top block suddenly grows shorter as it meets the edge of the long block!</p>\n<p>To understand what happened, just put yourself in the child's place. You've started to draw the upper edge of the short box, but how do you decide where to stop?</p>\n<p>Younger children don't yet possess much ability to draw lines in good proportion. Instead, they tend to use procedures that locate each new feature in some recognizable relationship to other features already represented in the drawing &mdash; that is, to <em>easily described places</em> that have previously been depicted. Since there are no such features near the middle of the long block, the child will use the same method, whatever it is, for the first few drawings. But it is easy to describe the location of the end of the long block, and that's why this is where the younger children tend to stop, once they approach that neighborhood. Piaget called this the <em>frontier effect</em> &mdash; the tendency to place new features at locations that have easily described relationships to other, already represented features.</p>\n<p>Why can't children simply copy what they see? We adults don't appreciate how complicated copying really is &mdash; because we can't recall what it was like before we learned to do it. To make a good copy, the child would have to draw each line to a scale and direction consistent with all the others. But these young children are scarcely able to trace the outline of an object with a finger; they certainly cannot mentally transport an entire figure shape from one location to another. So it is actually easier for the child to do what adults might consider more <em>abstract</em>: first to construct a mental description of the relations involved in the scene, and then to design a drawing -scheme to represent those relationships. It can require more skill to produce what we regard as a simple copy or imitation than to produce what we consider to be an <em>abstract</em> representation!</p>",
    "text": "We can get more insight about children from another experiment of Piaget's. A child is shown a short block resting on a longer one and is asked to draw the scene. Next the child is asked to draw a sketch of what might happen if we pushed the upper block a little to the right. At first, the result is more or less what we'd expect.\nBut when we ask the child to do the same repeatedly, we see a strange result. The top block suddenly grows shorter as it meets the edge of the long block!\nTo understand what happened, just put yourself in the child's place. You've started to draw the upper edge of the short box, but how do you decide where to stop?\nYounger children don't yet possess much ability to draw lines in good proportion. Instead, they tend to use procedures that locate each new feature in some recognizable relationship to other features already represented in the drawing \u2014 that is, to easily described places that have previously been depicted. Since there are no such features near the middle of the long block, the child will use the same method, whatever it is, for the first few drawings. But it is easy to describe the location of the end of the long block, and that's why this is where the younger children tend to stop, once they approach that neighborhood. Piaget called this the frontier effect \u2014 the tendency to place new features at locations that have easily described relationships to other, already represented features.\nWhy can't children simply copy what they see? We adults don't appreciate how complicated copying really is \u2014 because we can't recall what it was like before we learned to do it. To make a good copy, the child would have to draw each line to a scale and direction consistent with all the others. But these young children are scarcely able to trace the outline of an object with a finger; they certainly cannot mentally transport an entire figure shape from one location to another. So it is actually easier for the child to do what adults might consider more abstract: first to construct a mental description of the relations involved in the scene, and then to design a drawing -scheme to represent those relationships. It can require more skill to produce what we regard as a simple copy or imitation than to produce what we consider to be an abstract representation!",
    "type": "article",
    "title": "13.6 the frontier effect",
    "tags": [
      {
        "score": 0.6452601552009583,
        "sentiment": 0,
        "count": 2,
        "label": "Jean Piaget",
        "uri": "https://diffbot.com/entity/PDLtioHPGOfSddrmNWrXIYg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.6297779083251953,
        "sentiment": -0.483,
        "count": 12,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 244899856813,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 6353502640,
    "gburl": "http://aurellem.org/society-of-mind/som-13.6.html-diffbotxyz2491685214",
    "lastCrawlTimeUTC": 1588764083,
    "timestamp": "Wed, 06 May 2020 11:21:23 GMT"
  },
  {
    "sentiment": -0.206,
    "images": [
      {
        "naturalHeight": 169,
        "width": 342,
        "diffbotUri": "image|3|1413193431",
        "url": "http://aurellem.org/society-of-mind/illus/ch8/8-10.png",
        "naturalWidth": 342,
        "primary": true,
        "height": 169
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1065093269",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.11.html",
    "html": "<p>According to our concept of memory, the K-lines of each agency grow into a new society. So, to keep things straight, let's call the original agents S-agents and call their society the S-society. Given any S-society, we can imagine building memories for it by constructing a corresponding K-society for it. When we start making a K-society, we must link each K-line directly to S-agents, because there are no other K-lines we can connect them to. Later we can use the more efficient policy of linking new K-lines to old ones. But this will lead to a different problem of efficiency: the connections to the original S-agents will become increasingly remote and indirect. Then everything will begin to slow down &mdash; unless the K-society continues to make at least some new connections to the original S-society. That would be easy to arrange, if the K-society grows in the form of a <em>layer</em> close to its S-society. The diagram below suggests such an arrangement.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch8/8-10.png\"/></figure>\n<p>If arranged this way, the layer pairs could form a curious sort of computer. As S-agents excite K-agents and vice versa, a sort of spiraling activity would ensue. Over time, the location of that activity might tend to drift upward or down and might also tend to spread out; without some control, the system might soon become chaotic. But it would be hard to control the system from within, nor would that serve the purposes of other agencies. However, we can easily imagine how yet another, third agency could confine and control the K-S system's activity &mdash; by specifying which level-band should remain active and suppressing all the rest. Indeed, that is precisely the sort of coarse control that a B-brain might exercise, since it could do all this without needing to understand the fine details of what is happening inside the A-brain. The third agency might simply look on and say impatiently, <em>This isn't getting anywhere: move up to take a higher-level view of the situation.</em> Or it might say, <em>That looks like progress, so move farther down and fill in more details.</em></p>\n<p>Is there any essential difference between the K-and S-societies? Not really &mdash; except that the S-society came first. Indeed, we can imagine an endless sequence of such societies, in which each new one learns to exploit the last. Later we'll propose that this is how our minds develop in infancy &mdash; as sequences of layers of societies. Each new layer begins as a set of K-lines, which starts by learning to exploit whatever skills have been acquired by the previous layer. Whenever a layer acquires some useful and substantial skill, it tends to stop learning and changing &mdash; and then yet another new layer can begin to learn to exploit the capabilities of the last. Each new layer begins as a student, learning new ways to use what older layers can already do. Then it slows its learning rate &mdash; and starts to serve both as subject and as teacher to the layers that form afterward.</p>",
    "text": "According to our concept of memory, the K-lines of each agency grow into a new society. So, to keep things straight, let's call the original agents S-agents and call their society the S-society. Given any S-society, we can imagine building memories for it by constructing a corresponding K-society for it. When we start making a K-society, we must link each K-line directly to S-agents, because there are no other K-lines we can connect them to. Later we can use the more efficient policy of linking new K-lines to old ones. But this will lead to a different problem of efficiency: the connections to the original S-agents will become increasingly remote and indirect. Then everything will begin to slow down \u2014 unless the K-society continues to make at least some new connections to the original S-society. That would be easy to arrange, if the K-society grows in the form of a layer close to its S-society. The diagram below suggests such an arrangement.\nIf arranged this way, the layer pairs could form a curious sort of computer. As S-agents excite K-agents and vice versa, a sort of spiraling activity would ensue. Over time, the location of that activity might tend to drift upward or down and might also tend to spread out; without some control, the system might soon become chaotic. But it would be hard to control the system from within, nor would that serve the purposes of other agencies. However, we can easily imagine how yet another, third agency could confine and control the K-S system's activity \u2014 by specifying which level-band should remain active and suppressing all the rest. Indeed, that is precisely the sort of coarse control that a B-brain might exercise, since it could do all this without needing to understand the fine details of what is happening inside the A-brain. The third agency might simply look on and say impatiently, This isn't getting anywhere: move up to take a higher-level view of the situation. Or it might say, That looks like progress, so move farther down and fill in more details.\nIs there any essential difference between the K-and S-societies? Not really \u2014 except that the S-society came first. Indeed, we can imagine an endless sequence of such societies, in which each new one learns to exploit the last. Later we'll propose that this is how our minds develop in infancy \u2014 as sequences of layers of societies. Each new layer begins as a set of K-lines, which starts by learning to exploit whatever skills have been acquired by the previous layer. Whenever a layer acquires some useful and substantial skill, it tends to stop learning and changing \u2014 and then yet another new layer can begin to learn to exploit the capabilities of the last. Each new layer begins as a student, learning new ways to use what older layers can already do. Then it slows its learning rate \u2014 and starts to serve both as subject and as teacher to the layers that form afterward.",
    "type": "article",
    "title": "8.11 layers of societies",
    "tags": [
      {
        "score": 0.8910709619522095,
        "sentiment": 0,
        "count": 5,
        "label": "society",
        "uri": "https://diffbot.com/entity/X5-r2onDFMwqrJxIepxIeQw"
      },
      {
        "score": 0.6016415953636169,
        "sentiment": 0.504,
        "count": 3,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5167132019996643,
        "sentiment": 0.528,
        "count": 1,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      }
    ],
    "docId": 27260797320,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 25945309601,
    "gburl": "http://aurellem.org/society-of-mind/som-8.11.html-diffbotxyz3237363256",
    "lastCrawlTimeUTC": 1588763921,
    "timestamp": "Wed, 06 May 2020 11:18:41 GMT"
  },
  {
    "images": [
      {
        "naturalHeight": 209,
        "width": 294,
        "diffbotUri": "image|3|1096107552",
        "url": "http://aurellem.org/society-of-mind/illus/ch20/20-4.png",
        "naturalWidth": 294,
        "primary": true,
        "height": 209
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-820357191",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.8.html",
    "html": "<h2><em>20.8</em> connection lines</h2>\n<p>The diagram below depicts a connection-scheme that permits many agents to communicate with one another, yet uses surprisingly few connection wires. It was invented by Calvin E. Mooers in 1946, before the modern era of computers. Here is how we could use just ten wires to enable any of several hundred <em>transmitting-agents</em> to activate any of a similar number of <em>receiving-agents.</em> The trick is to make each transmitting-agent excite not one, but five of those wires, chosen at random from the available ten. Then each receiving-agent is provided with an AND-agent connected to recognize the same five-wire combination.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch20/20-4.png\"/></figure>\n<p>In this example, each receiving-agent is aroused by precisely one transmitting-agent. If we wanted each receiving-agent to respond to several transmitting-agents, we could join together several separate recognizers so that the receiving-agent's input looks like a tree with a recognizer at the tip of each branch. How could those receivers learn which input patterns to recognize? One way would be to use the kind of evidence-weighing machinery we described earlier. Indeed, for brain cells that would seem quite plausible, since a typical brain cell actually has a treelike network to collect its input signals. No one yet knows quite what those networks do, but I wouldn't be surprised if many of them turn out to be simple Perceptron-like learning machines.</p>\n<p>The network shown in the diagram above has a serious deficiency: it can transmit only one signal at a time. The problem is that if several transmitting-agents were aroused at once, almost all ten connecting wires would be activated, which would then arouse all the receiving-agents and cause an avalanche. However, we can make that problem disappear by providing the system with enough additional connection wires. For example, suppose there were ten thousand connection wires rather than ten, and that each transmitting-agent became attached to about fifty of them. Then, even if one hundred agents were to send their signals all at once, there would be less than one chance in a trillion that this would erroneously activate any particular receiving-agent!</p>",
    "text": "20.8 connection lines\nThe diagram below depicts a connection-scheme that permits many agents to communicate with one another, yet uses surprisingly few connection wires. It was invented by Calvin E. Mooers in 1946, before the modern era of computers. Here is how we could use just ten wires to enable any of several hundred transmitting-agents to activate any of a similar number of receiving-agents. The trick is to make each transmitting-agent excite not one, but five of those wires, chosen at random from the available ten. Then each receiving-agent is provided with an AND-agent connected to recognize the same five-wire combination.\nIn this example, each receiving-agent is aroused by precisely one transmitting-agent. If we wanted each receiving-agent to respond to several transmitting-agents, we could join together several separate recognizers so that the receiving-agent's input looks like a tree with a recognizer at the tip of each branch. How could those receivers learn which input patterns to recognize? One way would be to use the kind of evidence-weighing machinery we described earlier. Indeed, for brain cells that would seem quite plausible, since a typical brain cell actually has a treelike network to collect its input signals. No one yet knows quite what those networks do, but I wouldn't be surprised if many of them turn out to be simple Perceptron-like learning machines.\nThe network shown in the diagram above has a serious deficiency: it can transmit only one signal at a time. The problem is that if several transmitting-agents were aroused at once, almost all ten connecting wires would be activated, which would then arouse all the receiving-agents and cause an avalanche. However, we can make that problem disappear by providing the system with enough additional connection wires. For example, suppose there were ten thousand connection wires rather than ten, and that each transmitting-agent became attached to about fifty of them. Then, even if one hundred agents were to send their signals all at once, there would be less than one chance in a trillion that this would erroneously activate any particular receiving-agent!",
    "type": "article",
    "title": "20.8 connection lines",
    "docId": 39464616368,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 57820856717,
    "gburl": "http://aurellem.org/society-of-mind/som-20.8.html-diffbotxyz3452578478",
    "lastCrawlTimeUTC": 1588763956,
    "timestamp": "Wed, 06 May 2020 11:19:16 GMT"
  },
  {
    "sentiment": -0.739,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-448756401",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-27.1.html",
    "html": "<p>Our reader must be anxious to know what finally became of Mary and that kite. Here is more of that story.</p>\n<p>Mary was invited to Jack's party. She wondered if he would like a kite. Jane said, <em>Jack already has a kite. He will make you take it back.</em></p>\n<p>What does the pronoun <em>it</em> mean here? Clearly Jane is speaking not of the kite that already belongs to Jack, but of the new kite that Mary is thinking of giving to him. But what leads the listener to assume that this is what the storyteller meant? There are many issues here besides the question of which kite is involved. How do we know <em>it</em> refers to a kite at all? Does <em>take it back</em> mean take it back from Jack or to return it to the store? For the sake of simplicity, let's put aside the other possibilities and assume that <em>it</em> must mean a kite. But in order to decide which kite is meant, we still must understand the larger phrase <em>take it back.</em> This phrase must refer to some structure already in the listener's mind; the narrator expects the listener to find the appropriate structure by activating an appropriate fragment of commonsense knowledge about giving and receiving birthday presents. But since every listener knows so many things, what sorts of processes could activate the appropriate knowledge without taking too much time? In 1974 Eugene Charniak, a graduate student at MIT, asked how each phrase of this story works to prepare the reader to comprehend the subsequent phrases. He suggested that whenever we hear about a particular event, specific recognition-agents are thereby aroused. These then proceed actively to watch and wait for other related types of events. (Because these recognition-agents lurk silently, to intervene only in certain circumstances, they are sometimes called <em>demons.</em>) For example, whenever a story contains the slightest hint that someone may have purchased a gift, specific demons might be aroused that watch for events like these:</p>\n<p>If there is evidence that the recipient rejects the gift, look for signs of it being returned. If you see evidence of a gift being returned, look for signs that the recipient rejected it.</p>\n<p>Charniak's thesis raised many questions. How easy should it be to activate demons? How long should they then remain active? If too few demons are aroused, we'll be slow to understand what's happening. But if too many become active, we'll get confused by false alarms. There are no simple solutions to these problems, and what we call <em>understanding</em> is a huge accumulation of skills. You might understand certain parts of a story by using separate, isolated demons; you might comprehend other aspects of that same story by using larger-scale processes that try to match the sequence of events to various remembered scripts; yet other understandings might depend upon which agents are aroused by various micronemes. How much of the fascination in telling a story, or in listening to one, comes from the manipulations of our demons' expectations?</p>",
    "text": "Our reader must be anxious to know what finally became of Mary and that kite. Here is more of that story.\nMary was invited to Jack's party. She wondered if he would like a kite. Jane said, Jack already has a kite. He will make you take it back.\nWhat does the pronoun it mean here? Clearly Jane is speaking not of the kite that already belongs to Jack, but of the new kite that Mary is thinking of giving to him. But what leads the listener to assume that this is what the storyteller meant? There are many issues here besides the question of which kite is involved. How do we know it refers to a kite at all? Does take it back mean take it back from Jack or to return it to the store? For the sake of simplicity, let's put aside the other possibilities and assume that it must mean a kite. But in order to decide which kite is meant, we still must understand the larger phrase take it back. This phrase must refer to some structure already in the listener's mind; the narrator expects the listener to find the appropriate structure by activating an appropriate fragment of commonsense knowledge about giving and receiving birthday presents. But since every listener knows so many things, what sorts of processes could activate the appropriate knowledge without taking too much time? In 1974 Eugene Charniak, a graduate student at MIT, asked how each phrase of this story works to prepare the reader to comprehend the subsequent phrases. He suggested that whenever we hear about a particular event, specific recognition-agents are thereby aroused. These then proceed actively to watch and wait for other related types of events. (Because these recognition-agents lurk silently, to intervene only in certain circumstances, they are sometimes called demons.) For example, whenever a story contains the slightest hint that someone may have purchased a gift, specific demons might be aroused that watch for events like these:\nIf there is evidence that the recipient rejects the gift, look for signs of it being returned. If you see evidence of a gift being returned, look for signs that the recipient rejected it.\nCharniak's thesis raised many questions. How easy should it be to activate demons? How long should they then remain active? If too few demons are aroused, we'll be slow to understand what's happening. But if too many become active, we'll get confused by false alarms. There are no simple solutions to these problems, and what we call understanding is a huge accumulation of skills. You might understand certain parts of a story by using separate, isolated demons; you might comprehend other aspects of that same story by using larger-scale processes that try to match the sequence of events to various remembered scripts; yet other understandings might depend upon which agents are aroused by various micronemes. How much of the fascination in telling a story, or in listening to one, comes from the manipulations of our demons' expectations?",
    "type": "article",
    "title": "27.1 demons",
    "tags": [
      {
        "score": 0.7233216762542725,
        "sentiment": 0.893,
        "count": 3,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6959756016731262,
        "sentiment": 0,
        "count": 2,
        "label": "Jane Porter",
        "uri": "https://diffbot.com/entity/XRDk8ongQPH-fcfMfyffPfQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6444723010063171,
        "sentiment": 0,
        "count": 2,
        "label": "Eugene Charniak",
        "uri": "https://diffbot.com/entity/PkGZiYLDqNBir1Wymc-adJw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.6270052194595337,
        "sentiment": 0.104,
        "count": 7,
        "label": "narrative",
        "uri": "https://diffbot.com/entity/X-yV-ag8aOfmhY9T-yRt-wg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5922631621360779,
        "sentiment": 0.954,
        "count": 2,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5359408855438232,
        "sentiment": 0,
        "count": 1,
        "label": "Mary",
        "uri": "https://diffbot.com/entity/XLJiTV1IYOfm5VzpUZMsTsQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.518200159072876,
        "sentiment": 0.27,
        "count": 1,
        "label": "Jack Shephard",
        "uri": "https://diffbot.com/entity/XPb-cc-yAPbK9HeP1VhSLpQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5101557970046997,
        "sentiment": 0,
        "count": 1,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 77545144741,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 49958764982,
    "gburl": "http://aurellem.org/society-of-mind/som-27.1.html-diffbotxyz2197974286",
    "lastCrawlTimeUTC": 1588763893,
    "timestamp": "Wed, 06 May 2020 11:18:13 GMT"
  },
  {
    "sentiment": -0.334,
    "images": [
      {
        "naturalHeight": 113,
        "width": 397,
        "diffbotUri": "image|3|-1616977329",
        "url": "http://aurellem.org/society-of-mind/illus/ch8/8-1.png",
        "naturalWidth": 397,
        "primary": true,
        "height": 113
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|740458454",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.2.html",
    "html": "<p>Suppose once, long ago, you solved a certain problem P. Some of your agents were active then; others were quiet. Now let's suppose that a certain <em>learning process</em> caused the agents that were active then to become attached to a certain agent kP, which we'll call a K-line. If you ever activate kP afterward, it will turn on just the agents that were active then, when you first solved that problem P!</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch8/8-1.png\"/></figure>\n<p>Today you have a different problem. Your mind is in a new state, with agents Q aroused. Something in your mind suspects that Q is similar to P &mdash; and activates kP.</p>\n<p>Now two sets of agents are active in your mind at once: the Q-agents of your recent thoughts and the P-agents aroused by that old memory. If everything goes well, perhaps both sets of agents will work together to solve today's problem. And that's our simplest concept of what memories are and how they're formed.</p>\n<p>What happens if the now active agents get into conflicts with those the K-line tries to activate? One policy might be to give priority to the K-line's agents. But we wouldn't want our memories to rearouse old states of mind so strongly that they overwhelm our present thoughts &mdash; for then we might lose track of what we're thinking now and wipe out all the work we've done. We only want some hints, suggestions, and ideas. Another policy would give the presently active agents priority over the remembered ones, and yet another policy would suppress both, according to the principle of noncompromise. This diagram shows what happens for each of these policies if we assume that neighboring agents tend to get into conflicts:</p>\n<p>The ideal scheme would activate exactly those P 's that would be most helpful in solving the present problem. But that would be too much to ask of any simple strategy.</p>",
    "text": "Suppose once, long ago, you solved a certain problem P. Some of your agents were active then; others were quiet. Now let's suppose that a certain learning process caused the agents that were active then to become attached to a certain agent kP, which we'll call a K-line. If you ever activate kP afterward, it will turn on just the agents that were active then, when you first solved that problem P!\nToday you have a different problem. Your mind is in a new state, with agents Q aroused. Something in your mind suspects that Q is similar to P \u2014 and activates kP.\nNow two sets of agents are active in your mind at once: the Q-agents of your recent thoughts and the P-agents aroused by that old memory. If everything goes well, perhaps both sets of agents will work together to solve today's problem. And that's our simplest concept of what memories are and how they're formed.\nWhat happens if the now active agents get into conflicts with those the K-line tries to activate? One policy might be to give priority to the K-line's agents. But we wouldn't want our memories to rearouse old states of mind so strongly that they overwhelm our present thoughts \u2014 for then we might lose track of what we're thinking now and wipe out all the work we've done. We only want some hints, suggestions, and ideas. Another policy would give the presently active agents priority over the remembered ones, and yet another policy would suppress both, according to the principle of noncompromise. This diagram shows what happens for each of these policies if we assume that neighboring agents tend to get into conflicts:\nThe ideal scheme would activate exactly those P 's that would be most helpful in solving the present problem. But that would be too much to ask of any simple strategy.",
    "type": "article",
    "title": "8.2 re-membering",
    "tags": [
      {
        "score": 0.7417558431625366,
        "sentiment": 0,
        "count": 3,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      },
      {
        "score": 0.5749180912971497,
        "sentiment": 0.899,
        "count": 3,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.563500702381134,
        "sentiment": 0,
        "count": 8,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 135034601882,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 159063884176,
    "gburl": "http://aurellem.org/society-of-mind/som-8.2.html-diffbotxyz3937967933",
    "lastCrawlTimeUTC": 1588763865,
    "timestamp": "Wed, 06 May 2020 11:17:45 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|777464322",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.8.html",
    "html": "<p>We're always learning from experience by seeing some examples and then applying them to situations that we've never seen before. A single frightening growl or bark may lead a baby to fear all dogs of similar size &mdash; or, even, animals of every kind. How do we make generalizations from fragmentary bits of evidence? A dog of mine was once hit by a car, and it never went down the same street again &mdash; but it never stopped chasing cars on other streets.</p>\n<p>Philosophers of every period have tried to generalize about how we learn so much from our experiences. They have proposed many theories about this, using names like <em>abstraction,</em> <em>induction,</em> <em>abduction,</em> and so forth. But no one has found a way to make consistently correct generalizations &mdash; presumably because no such foolproof scheme exists, and whatever we <em>learn</em> may turn out to be wrong. In any case, we humans do not learn in accord with any fixed and constant set of principles; instead, we accumulate societies of learning-schemes that differ both in quality and kind.</p>\n<p>We've already seen several ways to generalize. One way is to construct uniframes by formulating descriptions that suppress details we regard as insignificant. A related idea is built into our concept of a <em>level-band.</em> Yet another scheme is implicit in the concept of a polyneme, which tries to guess the character of things by combining expectations based upon some independent properties. In any case, there is an intimate relationship between how we <em>represent</em> what we already know and the generalizations that will seem most plausible. For example, when we first proposed a <em>recognizer</em> for a chair, we composed it from the polynemes for several already familiar ideas, namely seats, legs, and backs. We gave these features certain weights.</p>\n<p>If we changed the values of those evidence weights, this would produce new recognizer-agents. For example, with a negative weight for <em>back,</em> the new agent would reject chairs but would accept benches, stools, or tables. If all the weights were increased (but the required total were kept the same), the new recognizer would accept a wider class of furniture or furniture with more parts hidden from view &mdash; as well as other objects that weren't furniture at all.</p>\n<p>Why would there be any substantial likelihood that such variations would produce useful recognizers? That would be unlikely indeed, if we assembled new recognizers by combining old ones selected at random. But there is a much better chance for usefulness if each new recognizer is made by combining signals from agents that have already proven themselves useful in related contexts. As Douglas Hofstadter has explained:</p>\n<p>Making variations on a theme is the crux of creativity. But it is not some magical, mysterious process that occurs when two indivisible concepts collide; it is a consequence of the divisibility of concepts into already significant subconceptual elements.</p>",
    "text": "We're always learning from experience by seeing some examples and then applying them to situations that we've never seen before. A single frightening growl or bark may lead a baby to fear all dogs of similar size \u2014 or, even, animals of every kind. How do we make generalizations from fragmentary bits of evidence? A dog of mine was once hit by a car, and it never went down the same street again \u2014 but it never stopped chasing cars on other streets.\nPhilosophers of every period have tried to generalize about how we learn so much from our experiences. They have proposed many theories about this, using names like abstraction, induction, abduction, and so forth. But no one has found a way to make consistently correct generalizations \u2014 presumably because no such foolproof scheme exists, and whatever we learn may turn out to be wrong. In any case, we humans do not learn in accord with any fixed and constant set of principles; instead, we accumulate societies of learning-schemes that differ both in quality and kind.\nWe've already seen several ways to generalize. One way is to construct uniframes by formulating descriptions that suppress details we regard as insignificant. A related idea is built into our concept of a level-band. Yet another scheme is implicit in the concept of a polyneme, which tries to guess the character of things by combining expectations based upon some independent properties. In any case, there is an intimate relationship between how we represent what we already know and the generalizations that will seem most plausible. For example, when we first proposed a recognizer for a chair, we composed it from the polynemes for several already familiar ideas, namely seats, legs, and backs. We gave these features certain weights.\nIf we changed the values of those evidence weights, this would produce new recognizer-agents. For example, with a negative weight for back, the new agent would reject chairs but would accept benches, stools, or tables. If all the weights were increased (but the required total were kept the same), the new recognizer would accept a wider class of furniture or furniture with more parts hidden from view \u2014 as well as other objects that weren't furniture at all.\nWhy would there be any substantial likelihood that such variations would produce useful recognizers? That would be unlikely indeed, if we assembled new recognizers by combining old ones selected at random. But there is a much better chance for usefulness if each new recognizer is made by combining signals from agents that have already proven themselves useful in related contexts. As Douglas Hofstadter has explained:\nMaking variations on a theme is the crux of creativity. But it is not some magical, mysterious process that occurs when two indivisible concepts collide; it is a consequence of the divisibility of concepts into already significant subconceptual elements.",
    "type": "article",
    "title": "19.8 generalizing",
    "docId": 35336749469,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 112017260959,
    "gburl": "http://aurellem.org/society-of-mind/som-19.8.html-diffbotxyz3268464520",
    "lastCrawlTimeUTC": 1588763731,
    "timestamp": "Wed, 06 May 2020 11:15:31 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1085991290",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.3.html",
    "html": "<p>Many modern scientists think it quaint to talk about <em>mental states.</em> They feel this idea is too <em>subjective</em> to be scientific, and they prefer to base their theories of psychology on ideas about information pro- cessing. This has produced many good theories about problem solving, pattern recognition, and other important facets of psychology, but on the whole it hasn't led to useful ways to describe the workings of our dispositions, attitudes, and feelings.</p>\n<p>Is this because, as many think, our feelings are inherently more complicated than the things we more easily describe in words? Not necessarily: our memories of attitudes and feelings could come from relatively simple K-line mechanisms &mdash; yet still be inexpressible. This is because K-lines can easily record relatively widespread and diffuse activities and, later, reactivate them all at once. This helps explain a familiar psychological phenomenon:</p>\n<p>The experiences we find easiest to recollect are often just the kinds we find the hardest to describe.</p>\n<p>For example, a novice can remember how it felt to be at a concert. A more proficient amateur can remember more of the music itself &mdash; the rhythms and the harmonies and melodies. But only skilled musicians can recall the smaller details of timbre, texture, and arrangement. Why do we find it easier to recollect our attitudes and feelings than to describe what actually took place? That's just what we should expect from memories of the K-line kind. Suppose that a certain sentiment or disposition involved the activities of many different agents. It would be easy to construct a huge K-line with which we could, later, make ourselves approximately reexperience that complicated state &mdash; simply by rearousing the same activities. But this would not automatically enable us to describe those feelings, which is another matter entirely, because it would require us to summarize that huge, dispersed activity in terms of some much more compact arrangement of verbal expressions.</p>\n<p>We cannot always judge the complexity of our mental states by how easily we can express them in words. A certain state of mind might involve a mass of information simply too enormous and diverse to express in any small number of words, yet not be very complicated in any interesting sense. Furthermore, the things we can express in words are, to a large extent, constrained by the social process through which we learn to use those words. In order for a word to have a predictable effect on other persons, we must maintain strict, public discipline on how that word is used &mdash; whereas each individual's private, internal signals need not be so constrained. The signals that come from our nonverbal agents can have K-line connections that branch out very rapidly to arouse other agents. If each member of such a society were to arouse a mere hundred others, then in only three or four steps the activity of a single one of them could affect a million other agents.</p>\n<p>Once we think in terms of K-line memories, it becomes easy to imagine, at least in principle, how a person could recall a general impression of a complex previous experience &mdash; but it becomes hard to understand how a person can so easily comprehend a specific statement like <em>John has more candy than Mary.</em> If this theory is correct, the traditional view must be upside down, which regards it as easy to understand how minds can deal with <em>facts</em> and <em>propositions,</em> but hard to see how minds could have diffuse, hard-to-express dispositions.</p>",
    "text": "Many modern scientists think it quaint to talk about mental states. They feel this idea is too subjective to be scientific, and they prefer to base their theories of psychology on ideas about information pro- cessing. This has produced many good theories about problem solving, pattern recognition, and other important facets of psychology, but on the whole it hasn't led to useful ways to describe the workings of our dispositions, attitudes, and feelings.\nIs this because, as many think, our feelings are inherently more complicated than the things we more easily describe in words? Not necessarily: our memories of attitudes and feelings could come from relatively simple K-line mechanisms \u2014 yet still be inexpressible. This is because K-lines can easily record relatively widespread and diffuse activities and, later, reactivate them all at once. This helps explain a familiar psychological phenomenon:\nThe experiences we find easiest to recollect are often just the kinds we find the hardest to describe.\nFor example, a novice can remember how it felt to be at a concert. A more proficient amateur can remember more of the music itself \u2014 the rhythms and the harmonies and melodies. But only skilled musicians can recall the smaller details of timbre, texture, and arrangement. Why do we find it easier to recollect our attitudes and feelings than to describe what actually took place? That's just what we should expect from memories of the K-line kind. Suppose that a certain sentiment or disposition involved the activities of many different agents. It would be easy to construct a huge K-line with which we could, later, make ourselves approximately reexperience that complicated state \u2014 simply by rearousing the same activities. But this would not automatically enable us to describe those feelings, which is another matter entirely, because it would require us to summarize that huge, dispersed activity in terms of some much more compact arrangement of verbal expressions.\nWe cannot always judge the complexity of our mental states by how easily we can express them in words. A certain state of mind might involve a mass of information simply too enormous and diverse to express in any small number of words, yet not be very complicated in any interesting sense. Furthermore, the things we can express in words are, to a large extent, constrained by the social process through which we learn to use those words. In order for a word to have a predictable effect on other persons, we must maintain strict, public discipline on how that word is used \u2014 whereas each individual's private, internal signals need not be so constrained. The signals that come from our nonverbal agents can have K-line connections that branch out very rapidly to arouse other agents. If each member of such a society were to arouse a mere hundred others, then in only three or four steps the activity of a single one of them could affect a million other agents.\nOnce we think in terms of K-line memories, it becomes easy to imagine, at least in principle, how a person could recall a general impression of a complex previous experience \u2014 but it becomes hard to understand how a person can so easily comprehend a specific statement like John has more candy than Mary. If this theory is correct, the traditional view must be upside down, which regards it as easy to understand how minds can deal with facts and propositions, but hard to see how minds could have diffuse, hard-to-express dispositions.",
    "type": "article",
    "title": "8.3 mental states and dispositions",
    "docId": 72381415838,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 119436804517,
    "gburl": "http://aurellem.org/society-of-mind/som-8.3.html-diffbotxyz3845802464",
    "lastCrawlTimeUTC": 1588763765,
    "timestamp": "Wed, 06 May 2020 11:16:05 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-157186666",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-2.4.html",
    "html": "<blockquote> It has been the persuasion of an immense majority of human beings that sensibility and thought [as distinguished from matter] are, in their own nature, less susceptible of division and decay, and that, when the body is resolved into its elements, the principle which animated it will remain perpetual and unchanged. However, it is probable that what we call thought is not an actual being, but no more than the relation between certain parts of that infinitely varied mass, of which the rest of the universe is composed, and which ceases to exist as soon as those parts change their position with respect to each other. &mdash;Percy Bysshe Shelley </blockquote>\n<p>What is Life? One dissects a body but finds no life inside. What is Mind? One dissects a brain but finds no mind therein. Are life and mind so much more than the <em>sum of their parts</em> that it is useless to search for them? To answer that, consider this parody of a conversation between a Holist and an ordinary Citizen.</p>\n<p>Holist: <em>I'll prove no box can hold a mouse. A box is made by nailing six boards together. But it's obvious that no box can hold a mouse unless it has some &lsquo;mousetightness&rsquo; or &lsquo;containment.&rsquo; Now, no single board contains any containment, since the mouse can just walk away from it. And if there is no containment in one board, there can't be any in six boards. So the box can have no mousetightness at all. Theoretically, then, the mouse can escape!</em></p>\n<p>Citizen: <em>Amazing. Then what does keep a mouse in a box?</em></p>\n<p>Holist: <em>Oh, simple. Even though it has no real mouse- tightness, a good box can &lsquo;simulate&rsquo; it so well that the mouse is fooled and can't figure out how to escape.</em></p>\n<p>What, then, keeps the mouse confined? Of course, it is the way a box prevents motion in all directions, because each board bars escape in a certain direction. The left side keeps the mouse from going left, the right from going right, the top keeps it from leaping out, and so on. The secret of a box is simply in how the boards are arranged to prevent motion in all directions!</p>\n<p>That's what containing means. So it's silly to expect any separate board by itself to contain any containment, even though each contributes to the containing. It is like the cards of a straight flush in poker: only the full hand has any value at all.</p>\n<p>The same applies to words like life and mind. It is foolish to use these words for describing the smallest components of living things because these words were invented to describe how larger assemblies interact. Like boxing-in, words like living and thinking are useful for describing phenomena that result from certain combinations of relationships. The reason box seems nonmysterious is that everyone understands how the boards of a well-made box interact to prevent motion in any direction. In fact, the word life has already lost most of its mystery &mdash; at least for modern biologists, because they understand so many of the important interactions among the chemicals in cells. But mind still holds its mystery &mdash; because we still know so little about how mental agents interact to accomplish all the things they do.</p>",
    "text": "It has been the persuasion of an immense majority of human beings that sensibility and thought [as distinguished from matter] are, in their own nature, less susceptible of division and decay, and that, when the body is resolved into its elements, the principle which animated it will remain perpetual and unchanged. However, it is probable that what we call thought is not an actual being, but no more than the relation between certain parts of that infinitely varied mass, of which the rest of the universe is composed, and which ceases to exist as soon as those parts change their position with respect to each other. \u2014Percy Bysshe Shelley\nWhat is Life? One dissects a body but finds no life inside. What is Mind? One dissects a brain but finds no mind therein. Are life and mind so much more than the sum of their parts that it is useless to search for them? To answer that, consider this parody of a conversation between a Holist and an ordinary Citizen.\nHolist: I'll prove no box can hold a mouse. A box is made by nailing six boards together. But it's obvious that no box can hold a mouse unless it has some \u2018mousetightness\u2019 or \u2018containment.\u2019 Now, no single board contains any containment, since the mouse can just walk away from it. And if there is no containment in one board, there can't be any in six boards. So the box can have no mousetightness at all. Theoretically, then, the mouse can escape!\nCitizen: Amazing. Then what does keep a mouse in a box?\nHolist: Oh, simple. Even though it has no real mouse- tightness, a good box can \u2018simulate\u2019 it so well that the mouse is fooled and can't figure out how to escape.\nWhat, then, keeps the mouse confined? Of course, it is the way a box prevents motion in all directions, because each board bars escape in a certain direction. The left side keeps the mouse from going left, the right from going right, the top keeps it from leaping out, and so on. The secret of a box is simply in how the boards are arranged to prevent motion in all directions!\nThat's what containing means. So it's silly to expect any separate board by itself to contain any containment, even though each contributes to the containing. It is like the cards of a straight flush in poker: only the full hand has any value at all.\nThe same applies to words like life and mind. It is foolish to use these words for describing the smallest components of living things because these words were invented to describe how larger assemblies interact. Like boxing-in, words like living and thinking are useful for describing phenomena that result from certain combinations of relationships. The reason box seems nonmysterious is that everyone understands how the boards of a well-made box interact to prevent motion in any direction. In fact, the word life has already lost most of its mystery \u2014 at least for modern biologists, because they understand so many of the important interactions among the chemicals in cells. But mind still holds its mystery \u2014 because we still know so little about how mental agents interact to accomplish all the things they do.",
    "type": "article",
    "title": "2.4 Holes and parts",
    "docId": 77731709325,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 180644004276,
    "gburl": "http://aurellem.org/society-of-mind/som-2.4.html-diffbotxyz3201266986",
    "lastCrawlTimeUTC": 1588763798,
    "timestamp": "Wed, 06 May 2020 11:16:38 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|2108872941",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.7.html",
    "html": "<p>When we first introduced Papert's principle &mdash; that is, the idea of growing by inserting new levels of management into old agencies &mdash; we did not ask when new layers should be built. If managers are inserted too soon, when their workers are still immature, little will get done. And if those managers come too late, that, too, would delay the mental growth. What could ensure that managers are not engaged too late or too soon? We all know children who seem to have matured too quickly or too slowly, in various respects, to match other areas of their growth. In an ideal system, each developing agency would be controlled by another agency equipped to introduce new agents just when they're needed &mdash; that is, when enough has been learned to justify the start of another stage. In any case, it would surely be disastrous if all our potential capacity to learn became available too soon. If every agent could learn from birth, they'd all be overwhelmed by infantile ideas.</p>\n<p>One way to regulate such things would be to actuate new agencies at genetically predetermined times. At various stages of biological <em>maturity,</em> certain classes of agents would be enabled to establish new connections, while others would be forced to slow their growth by making permanent connections that, till then, had been reversible. Could any clockwork scheme like this be guaranteed to work? Consider the fact that most of our children acquire agents like Reversible and Confined before they are five years old. For those children, at least, it would suffice to activate new intermediate-level agents at that age, so those children could proceed to build agents like Appearance and History. However, children who weren't ready yet would then be slightly handicapped by being forced to build some less-than-usually effective Societies-of-More. Nor would that rigid maturation- schedule serve well those children who had already moved <em>ahead of schedule.</em> It would be better to have systems in which the timing of each stage depends on what has actually happened earlier.</p>\n<p>One way a stagelike episode might start could stem from what we called the investment principle: once a certain skill surpasses all its close competitors, it becomes increasingly likely to be employed &mdash; and thereby increases its opportunities to develop even further. This self-enhancing effect can cause a spurt of rapid progress in which a particular skill quickly comes to dominate the scene. One way a stagelike episode might end could stem from what we called the exception principle. To see how this could happen, suppose that a certain agency develops so useful a way to do some job that many other agencies soon learn to exploit that capability. The more those other agencies become dependent on that skill, the more disruption will result from every further <em>improvement</em> in it &mdash; since it now has more customers to please! Even increasing the speed of one process could damage other agencies that depend upon how long it takes to work. Thus, once a scheme persists for long enough, it gets to be extremely hard to change &mdash; not because of limitations inherent in itself or in the agency that developed it, but because of how the rest of the society depends upon its present form.</p>\n<p>Once it becomes too hard to change an old agency, it is time to build another one; further progress may require revolution rather than evolution. This is another reason why a complex system must be grown in a sequence of separate steps.</p>",
    "text": "When we first introduced Papert's principle \u2014 that is, the idea of growing by inserting new levels of management into old agencies \u2014 we did not ask when new layers should be built. If managers are inserted too soon, when their workers are still immature, little will get done. And if those managers come too late, that, too, would delay the mental growth. What could ensure that managers are not engaged too late or too soon? We all know children who seem to have matured too quickly or too slowly, in various respects, to match other areas of their growth. In an ideal system, each developing agency would be controlled by another agency equipped to introduce new agents just when they're needed \u2014 that is, when enough has been learned to justify the start of another stage. In any case, it would surely be disastrous if all our potential capacity to learn became available too soon. If every agent could learn from birth, they'd all be overwhelmed by infantile ideas.\nOne way to regulate such things would be to actuate new agencies at genetically predetermined times. At various stages of biological maturity, certain classes of agents would be enabled to establish new connections, while others would be forced to slow their growth by making permanent connections that, till then, had been reversible. Could any clockwork scheme like this be guaranteed to work? Consider the fact that most of our children acquire agents like Reversible and Confined before they are five years old. For those children, at least, it would suffice to activate new intermediate-level agents at that age, so those children could proceed to build agents like Appearance and History. However, children who weren't ready yet would then be slightly handicapped by being forced to build some less-than-usually effective Societies-of-More. Nor would that rigid maturation- schedule serve well those children who had already moved ahead of schedule. It would be better to have systems in which the timing of each stage depends on what has actually happened earlier.\nOne way a stagelike episode might start could stem from what we called the investment principle: once a certain skill surpasses all its close competitors, it becomes increasingly likely to be employed \u2014 and thereby increases its opportunities to develop even further. This self-enhancing effect can cause a spurt of rapid progress in which a particular skill quickly comes to dominate the scene. One way a stagelike episode might end could stem from what we called the exception principle. To see how this could happen, suppose that a certain agency develops so useful a way to do some job that many other agencies soon learn to exploit that capability. The more those other agencies become dependent on that skill, the more disruption will result from every further improvement in it \u2014 since it now has more customers to please! Even increasing the speed of one process could damage other agencies that depend upon how long it takes to work. Thus, once a scheme persists for long enough, it gets to be extremely hard to change \u2014 not because of limitations inherent in itself or in the agency that developed it, but because of how the rest of the society depends upon its present form.\nOnce it becomes too hard to change an old agency, it is time to build another one; further progress may require revolution rather than evolution. This is another reason why a complex system must be grown in a sequence of separate steps.",
    "type": "article",
    "title": "17.7 genetic timetables",
    "docId": 112677765554,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 193973109162,
    "gburl": "http://aurellem.org/society-of-mind/som-17.7.html-diffbotxyz3038951051",
    "lastCrawlTimeUTC": 1588763830,
    "timestamp": "Wed, 06 May 2020 11:17:10 GMT"
  },
  {
    "sentiment": 0.726,
    "images": [
      {
        "naturalHeight": 98,
        "width": 217,
        "diffbotUri": "image|3|1093336989",
        "url": "http://aurellem.org/society-of-mind/illus/ch20/20-1.png",
        "naturalWidth": 217,
        "primary": true,
        "height": 98
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|631712016",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.3.html",
    "html": "<h2><em>20.3</em> visual ambiguity</h2>\n<p>We usually think of <em>ambiguity</em> as an aspect of language &mdash; but ambiguities are just as common in vision, too. What's that structure shown below? It could be regarded as nine separate blocks, as an arch supported by two other arches, or as a single, complicated, nine-block arch!</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch20/20-1.png\"/></figure>\n<p>What process makes us able to see that superarch as composed of three little arches rather than of nine separate blocks? How, for that matter, do we recognize those as blocks in the first place, instead of seeing only lines and corners? These <em>ambiguities</em> are normally resolved so quickly and quietly that our higher-level agencies have no sense of conflict at all. To be sure, we sometimes have the sense of perceiving the same structure in several ways at once &mdash; for example, as both a single complex arch and as three separate simpler arches. But we usually lock in on one particular interpretation.</p>\n<p>Sometimes no lower-level information can resolve an ambiguity &mdash; as in the case of this example by Oliver Selfridge.</p>\n<p>Here, there is no difference whatever between the H and the A, yet we see them as having distinct identities in their different contexts. Evidently, the <em>simulus</em> produced by the visual sense is strongly affected by the state of some language-related agency. Furthermore, just as we can describe the same figure in different ways, we often can describe different figures in the same way. Thus, we recognize all these figures as similar, though no two of them are actually the same:</p>\n<p>If we described each of these in terms of the lengths, directions, and locations of their lines, they would all seem very different. But we can make them all seem much the same by describing each of them in the same way, perhaps like this: <em>a triangle with two lines extended from one of its vertices.</em> The point is that what we <em>see</em> does not depend only on what reaches our eyes from the outside world. The manner in which we interpret those stimuli depends to a large extent on what is already taking place inside our agencies.</p>",
    "text": "20.3 visual ambiguity\nWe usually think of ambiguity as an aspect of language \u2014 but ambiguities are just as common in vision, too. What's that structure shown below? It could be regarded as nine separate blocks, as an arch supported by two other arches, or as a single, complicated, nine-block arch!\nWhat process makes us able to see that superarch as composed of three little arches rather than of nine separate blocks? How, for that matter, do we recognize those as blocks in the first place, instead of seeing only lines and corners? These ambiguities are normally resolved so quickly and quietly that our higher-level agencies have no sense of conflict at all. To be sure, we sometimes have the sense of perceiving the same structure in several ways at once \u2014 for example, as both a single complex arch and as three separate simpler arches. But we usually lock in on one particular interpretation.\nSometimes no lower-level information can resolve an ambiguity \u2014 as in the case of this example by Oliver Selfridge.\nHere, there is no difference whatever between the H and the A, yet we see them as having distinct identities in their different contexts. Evidently, the simulus produced by the visual sense is strongly affected by the state of some language-related agency. Furthermore, just as we can describe the same figure in different ways, we often can describe different figures in the same way. Thus, we recognize all these figures as similar, though no two of them are actually the same:\nIf we described each of these in terms of the lengths, directions, and locations of their lines, they would all seem very different. But we can make them all seem much the same by describing each of them in the same way, perhaps like this: a triangle with two lines extended from one of its vertices. The point is that what we see does not depend only on what reaches our eyes from the outside world. The manner in which we interpret those stimuli depends to a large extent on what is already taking place inside our agencies.",
    "type": "article",
    "title": "20.3 visual ambiguity",
    "tags": [
      {
        "score": 0.838703453540802,
        "sentiment": 0.215,
        "count": 3,
        "label": "visual arts",
        "uri": "https://diffbot.com/entity/XrdRz28aaOkKCb_Cd5UdWUw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5357787609100342,
        "sentiment": 0,
        "count": 1,
        "label": "language",
        "uri": "https://diffbot.com/entity/XZLUv8RZWNw62dTgKTTa7JQ"
      },
      {
        "score": 0.5272917151451111,
        "sentiment": 0,
        "count": 1,
        "label": "Oliver Selfridge",
        "uri": "https://diffbot.com/entity/PeTuFVFv0MPWJ1wmZVxdSKw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 66718286242,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 96590414236,
    "gburl": "http://aurellem.org/society-of-mind/som-20.3.html-diffbotxyz1405537525",
    "lastCrawlTimeUTC": 1588763677,
    "timestamp": "Wed, 06 May 2020 11:14:37 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|312315062",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.4.html",
    "html": "<p>Dictionary definitions never say enough. Every child knows that a party is more than just a gathering assembled to celebrate someone's birthday. But no brief definition can describe the complicated customs, rules, and regulations that typical communities prescribe for such ceremonies. When I was a child, a birthday party could be expected to include at least the elements of the following script:</p>\n<p>ARRIVAL. Greeting. Be well dressed. GIFT. Give birthday present to host or guest of honor.</p>\n<p>GAMES. Activities like blindfold competitions. DECOR. Balloons. Favors. Crepe-paper decorations. PARTY-MEAL. Hotdogs, candies, ice-cream, etc. CAKE. With candles to represent the host's age. CEREMONY. Host tries to extinguish candles with single breath (to make a wish). SONG. All guests sing birthday song and eat cake.</p>\n<p>This is merely an outline, for every item leads to other conditions and requirements. The birthday present has to please the host, of course, but there are other strong constraints on it as well. It ought to be brand new, of good quality, and it should not be ostentatiously extravagant. It ought to be suitably <em>party-wrapped</em> &mdash; that is, covered with a certain kind of color-printed wrapping paper and tied with colored ribbon. There are also constraints on other items in the script. The birthday cake should be covered with a sweet sugar frosting. In my childhood, the ice cream usually consisted of three colored stripes of different flavors: vanilla, strawberry, and chocolate. Because I did not like the strawberry flavor, my personal party script included the extra steps of finding another child willing to make a trade.</p>\n<p>To all their young participants, such parties unfold exactly as a party should, with all these queer complexities. We take our social customs for granted, as though they were natural phenomena. Few guests or hosts will ever wonder why their parties have those explicit forms or ask about their origins. As far as any child can tell, that's just how parties ought to go; they always did and always will. And so it is with almost everything we know.</p>",
    "text": "Dictionary definitions never say enough. Every child knows that a party is more than just a gathering assembled to celebrate someone's birthday. But no brief definition can describe the complicated customs, rules, and regulations that typical communities prescribe for such ceremonies. When I was a child, a birthday party could be expected to include at least the elements of the following script:\nARRIVAL. Greeting. Be well dressed. GIFT. Give birthday present to host or guest of honor.\nGAMES. Activities like blindfold competitions. DECOR. Balloons. Favors. Crepe-paper decorations. PARTY-MEAL. Hotdogs, candies, ice-cream, etc. CAKE. With candles to represent the host's age. CEREMONY. Host tries to extinguish candles with single breath (to make a wish). SONG. All guests sing birthday song and eat cake.\nThis is merely an outline, for every item leads to other conditions and requirements. The birthday present has to please the host, of course, but there are other strong constraints on it as well. It ought to be brand new, of good quality, and it should not be ostentatiously extravagant. It ought to be suitably party-wrapped \u2014 that is, covered with a certain kind of color-printed wrapping paper and tied with colored ribbon. There are also constraints on other items in the script. The birthday cake should be covered with a sweet sugar frosting. In my childhood, the ice cream usually consisted of three colored stripes of different flavors: vanilla, strawberry, and chocolate. Because I did not like the strawberry flavor, my personal party script included the extra steps of finding another child willing to make a trade.\nTo all their young participants, such parties unfold exactly as a party should, with all these queer complexities. We take our social customs for granted, as though they were natural phenomena. Few guests or hosts will ever wonder why their parties have those explicit forms or ask about their origins. As far as any child can tell, that's just how parties ought to go; they always did and always will. And so it is with almost everything we know.",
    "type": "article",
    "title": "26.4 a party-frame",
    "docId": 88544559542,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 35688825229,
    "gburl": "http://aurellem.org/society-of-mind/som-26.4.html-diffbotxyz120352515",
    "lastCrawlTimeUTC": 1588763603,
    "timestamp": "Wed, 06 May 2020 11:13:23 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1491633862",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.3.html",
    "html": "<p>In order for a mind to think, it has to juggle fragments of its mental states. Suppose you want to rearrange the furniture inside a room you know. Your attention keeps shifting, first to one corner, then to another, next to the center of the room, and then, perhaps, to how the light falls on some object on a shelf. Different ideas and images interrupt each other. At one moment it seems as though your entire mind were focused on one small detail; at another moment you might dwell on why you are thinking about that room in the first place; then you might find yourself comparing or contrasting two different rearrangements of that scene: <em>If that couch were over here, there would be room for guests to chat &mdash; but no, that would block the path so that they wouldn't be able to enter.</em></p>\n<p>How do our various agencies keep track of imaginary changes in scenes? Where do the different versions go when out of mind, and how do we get them back again? They must be stored as memories.</p>\n<p>But what do we mean by that? Some readers may be surprised to learn that biologists still have no well-established theory of what happens in our brains when memories are formed. Psychologists, however, do agree that there must be at least two different mechanisms. We appear to have <em>long-term memories,</em> which can persist for days or years or all one's life. We also have <em>short-term memories,</em> which last only for seconds or minutes. In the next few sections we'll talk mostly about the uses of these transient traces of our recent thoughts. For example, whenever we get stuck in the course of solving a problem, we need to be able to backtrack, modify our strategy, and try again. To do this we need those short-term memories, if only not to repeat the same mistake.</p>\n<p>How much do we remember? Sometimes we surprise ourselves by remembering things we didn't know we knew. Could this mean that we remember everything? Some older theories in psychology have supposed this to be true, and there are many legends of persons having fabulous abilities. For example, we often hear about people with <em>photographic memories</em> that enable them to quickly memorize all the fine details of a complicated picture or a page of text in a few seconds. So far as I can tell, all of these tales are unfounded myths, and only professional magicians or charlatans can produce such demonstrations.</p>\n<p>In any case, I suspect we never really remember very much about a particular experience. Instead, our various agencies selectively decide, unconsciously, to transfer only certain states into their long-term memories &mdash; perhaps because they have been classified as useful, dangerous, unusual, or significant in other respects. It would be of little use for us simply to maintain vast stores of unclassified memories if, every time we needed one, we had to search through all of them. Nor would it be of any use for them all to flood at once into our agencies. Instead, each of us must develop fruitful and effective ways to organize our memories &mdash; but how that's done is inaccessible to consciousness. What barriers keep us from knowing such things? The next few sections sketch out some theories, both about how our memory-systems work and why we can't find this out directly by examining our own thoughts.</p>",
    "text": "In order for a mind to think, it has to juggle fragments of its mental states. Suppose you want to rearrange the furniture inside a room you know. Your attention keeps shifting, first to one corner, then to another, next to the center of the room, and then, perhaps, to how the light falls on some object on a shelf. Different ideas and images interrupt each other. At one moment it seems as though your entire mind were focused on one small detail; at another moment you might dwell on why you are thinking about that room in the first place; then you might find yourself comparing or contrasting two different rearrangements of that scene: If that couch were over here, there would be room for guests to chat \u2014 but no, that would block the path so that they wouldn't be able to enter.\nHow do our various agencies keep track of imaginary changes in scenes? Where do the different versions go when out of mind, and how do we get them back again? They must be stored as memories.\nBut what do we mean by that? Some readers may be surprised to learn that biologists still have no well-established theory of what happens in our brains when memories are formed. Psychologists, however, do agree that there must be at least two different mechanisms. We appear to have long-term memories, which can persist for days or years or all one's life. We also have short-term memories, which last only for seconds or minutes. In the next few sections we'll talk mostly about the uses of these transient traces of our recent thoughts. For example, whenever we get stuck in the course of solving a problem, we need to be able to backtrack, modify our strategy, and try again. To do this we need those short-term memories, if only not to repeat the same mistake.\nHow much do we remember? Sometimes we surprise ourselves by remembering things we didn't know we knew. Could this mean that we remember everything? Some older theories in psychology have supposed this to be true, and there are many legends of persons having fabulous abilities. For example, we often hear about people with photographic memories that enable them to quickly memorize all the fine details of a complicated picture or a page of text in a few seconds. So far as I can tell, all of these tales are unfounded myths, and only professional magicians or charlatans can produce such demonstrations.\nIn any case, I suspect we never really remember very much about a particular experience. Instead, our various agencies selectively decide, unconsciously, to transfer only certain states into their long-term memories \u2014 perhaps because they have been classified as useful, dangerous, unusual, or significant in other respects. It would be of little use for us simply to maintain vast stores of unclassified memories if, every time we needed one, we had to search through all of them. Nor would it be of any use for them all to flood at once into our agencies. Instead, each of us must develop fruitful and effective ways to organize our memories \u2014 but how that's done is inaccessible to consciousness. What barriers keep us from knowing such things? The next few sections sketch out some theories, both about how our memory-systems work and why we can't find this out directly by examining our own thoughts.",
    "type": "article",
    "title": "15.3 memory",
    "docId": 180876739002,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 104152678813,
    "gburl": "http://aurellem.org/society-of-mind/som-15.3.html-diffbotxyz3041335734",
    "lastCrawlTimeUTC": 1588763703,
    "timestamp": "Wed, 06 May 2020 11:15:03 GMT"
  },
  {
    "sentiment": -0.532,
    "humanLanguage": "en",
    "diffbotUri": "article|3|385663357",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.3.html",
    "html": "<p>Soon after learning how to put an apple into a pail, a child will discover that it now can put the apple into a box or put an onion into the pail. What magic tricks allow us to <em>de-specialize</em> whatever skills we learn? We've already seen one way to do this simply by replacing certain polynemes with less specific isonomes. For example, our first apple-into-pail procedure was so specialized that it could be used only to put apples into pails &mdash; because it is based on using specific polynemes for those objects. However, the second script just as easily puts onions into pails or umbrellas into suitcases, because it engages no polynemes at all, but only the Origin and Destination pronomes. This script is more versatile because those pronomes can be assigned to anything! Learning to think in terms of isonomes must be a crucial step in many types of mental growth.</p>\n<p>None of our many chaining tricks would have much use if each were permanently tied to one specific polyneme like <em>owl</em> or <em>car</em> or <em>cup</em> or <em>gear.</em> However, once we learn to build our process scripts with isonomes, each can be applied to many kinds of reasoning &mdash; to logic, cause, dependency, and all the rest. But changing polynemes to isonomes will not always work. What could keep a child from trying to apply the script that works on <em>put the apple in the block</em> to <em>put the ocean in the cup</em>? To prevent such absurdities, our script must also place appropriate constraints on the Origin and Destination &mdash; for example, to ensure that the Destination must represent a container large enough to hold the Origin thing, and that the container be open toward the top. If all this seems too obvious to say, just watch a baby's first attempts to put an object in a pail or pick up food with a spoon or a fork. It takes many weeks or months of work to bring such skills to the point of usefulness. If we generalize too recklessly by changing all our polynemes to isonomes, few of our generalizations will actually work.</p>\n<p>What we call <em>generalizing</em> is not any single process or concept, but a functional term for the huge societies of different methods we use to extend the powers of our skills. No single policy will work for all domains of thought, and each refinement of technique will affect the quality of the generalizations we make. Converting polynemes to isonomes may be a potentially powerful skill, but it must be adapted to different realms. Once we accumulate enough examples of how a new script fails and succeeds in several situations, we can try to build a uniframe to embody good constraints. But no matter which policy we adopt, we must always expect some exceptions. You cannot carry birds in pails, no matter how well they fit inside. Premature generalizations could lead to such large accumulations of constraints, censors, and exceptions that it would be better to retain the original polynemes.</p>",
    "text": "Soon after learning how to put an apple into a pail, a child will discover that it now can put the apple into a box or put an onion into the pail. What magic tricks allow us to de-specialize whatever skills we learn? We've already seen one way to do this simply by replacing certain polynemes with less specific isonomes. For example, our first apple-into-pail procedure was so specialized that it could be used only to put apples into pails \u2014 because it is based on using specific polynemes for those objects. However, the second script just as easily puts onions into pails or umbrellas into suitcases, because it engages no polynemes at all, but only the Origin and Destination pronomes. This script is more versatile because those pronomes can be assigned to anything! Learning to think in terms of isonomes must be a crucial step in many types of mental growth.\nNone of our many chaining tricks would have much use if each were permanently tied to one specific polyneme like owl or car or cup or gear. However, once we learn to build our process scripts with isonomes, each can be applied to many kinds of reasoning \u2014 to logic, cause, dependency, and all the rest. But changing polynemes to isonomes will not always work. What could keep a child from trying to apply the script that works on put the apple in the block to put the ocean in the cup? To prevent such absurdities, our script must also place appropriate constraints on the Origin and Destination \u2014 for example, to ensure that the Destination must represent a container large enough to hold the Origin thing, and that the container be open toward the top. If all this seems too obvious to say, just watch a baby's first attempts to put an object in a pail or pick up food with a spoon or a fork. It takes many weeks or months of work to bring such skills to the point of usefulness. If we generalize too recklessly by changing all our polynemes to isonomes, few of our generalizations will actually work.\nWhat we call generalizing is not any single process or concept, but a functional term for the huge societies of different methods we use to extend the powers of our skills. No single policy will work for all domains of thought, and each refinement of technique will affect the quality of the generalizations we make. Converting polynemes to isonomes may be a potentially powerful skill, but it must be adapted to different realms. Once we accumulate enough examples of how a new script fails and succeeds in several situations, we can try to build a uniframe to embody good constraints. But no matter which policy we adopt, we must always expect some exceptions. You cannot carry birds in pails, no matter how well they fit inside. Premature generalizations could lead to such large accumulations of constraints, censors, and exceptions that it would be better to retain the original polynemes.",
    "type": "article",
    "title": "22.3 de-specializing",
    "tags": [
      {
        "score": 0.5971194505691528,
        "sentiment": 0,
        "count": 3,
        "label": "apple",
        "uri": "https://diffbot.com/entity/XgrXdplpfMRWDa_VBAOd2oA"
      },
      {
        "score": 0.5828322172164917,
        "sentiment": -0.381,
        "count": 2,
        "label": "Origin Systems",
        "uri": "https://diffbot.com/entity/Ccb_oHREFPvC2aSvyOr652A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5356531143188477,
        "sentiment": 0,
        "count": 6,
        "label": "screenplay",
        "uri": "https://diffbot.com/entity/X8x_vrN0xMLW135RLD8gi7A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 212808008126,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 67534946698,
    "gburl": "http://aurellem.org/society-of-mind/som-22.3.html-diffbotxyz403810220",
    "lastCrawlTimeUTC": 1588763654,
    "timestamp": "Wed, 06 May 2020 11:14:14 GMT"
  },
  {
    "images": [
      {
        "naturalHeight": 157,
        "width": 410,
        "diffbotUri": "image|3|-435822561",
        "url": "http://aurellem.org/society-of-mind/illus/ch24/24-3.png",
        "naturalWidth": 410,
        "primary": true,
        "height": 157
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-260727470",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-24.6.html",
    "html": "<p>When you think about an object in a certain place, many different processes go on inside your mind. Some of your agencies know the visual direction in which that object lies, others can direct your hand to reach toward it, and yet other agencies anticipate how it would feel if it touched your skin. It is one thing to know that a block has flat sides and right angles, another to be able to recognize a block by sight, and yet another to be able to shape your hand to grasp that shape or recognize it from feeling it in your grasp. How do so many different agencies communicate about places and shapes?</p>\n<p>No one yet knows how shapes and places are represented in the brain. The agencies that do such things have been evolving since animals first began to move. Some of those agencies must be involved with postures of the arm and hand, others must represent what we discover from the images inside our eyes, and yet others must represent the relations between our bodies and the objects that surround us.</p>\n<p>How can we use so many different kinds of information at once? In the following sections I'll propose a new hypothesis to deal with this: that many agencies inside our brains use frames whose terminals are controlled by interaction-square arrays. Only now we'll use those square-arrays not to represent the interactions of different causes, but to describe the relations between closely related locations. For example, thinking of the appearance of a certain place or object would involve arousing a squarelike family of frames, each of which in turn represents a detailed view of the corresponding portion of that scene. If we actually use such processes, this could explain some psychological phenomena.</p>\n<p>If you were walking through a circular tube, you could scarcely keep from thinking in terms of bottom and top and sides &mdash; however vaguely their boundaries are defined. Without a way to represent the scene in terms of familiar parts, you'd have no well-established thinking skills to apply to it.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch24/24-3.png\"/></figure>\n<p>The diagram is meant to suggest that we represent directions and places by attaching them to a special set of pronome-like agents that we shall call <em>direction-nemes.</em> Later we'll see how these might be involved in surprisingly many realms of thought.</p>",
    "text": "When you think about an object in a certain place, many different processes go on inside your mind. Some of your agencies know the visual direction in which that object lies, others can direct your hand to reach toward it, and yet other agencies anticipate how it would feel if it touched your skin. It is one thing to know that a block has flat sides and right angles, another to be able to recognize a block by sight, and yet another to be able to shape your hand to grasp that shape or recognize it from feeling it in your grasp. How do so many different agencies communicate about places and shapes?\nNo one yet knows how shapes and places are represented in the brain. The agencies that do such things have been evolving since animals first began to move. Some of those agencies must be involved with postures of the arm and hand, others must represent what we discover from the images inside our eyes, and yet others must represent the relations between our bodies and the objects that surround us.\nHow can we use so many different kinds of information at once? In the following sections I'll propose a new hypothesis to deal with this: that many agencies inside our brains use frames whose terminals are controlled by interaction-square arrays. Only now we'll use those square-arrays not to represent the interactions of different causes, but to describe the relations between closely related locations. For example, thinking of the appearance of a certain place or object would involve arousing a squarelike family of frames, each of which in turn represents a detailed view of the corresponding portion of that scene. If we actually use such processes, this could explain some psychological phenomena.\nIf you were walking through a circular tube, you could scarcely keep from thinking in terms of bottom and top and sides \u2014 however vaguely their boundaries are defined. Without a way to represent the scene in terms of familiar parts, you'd have no well-established thinking skills to apply to it.\nThe diagram is meant to suggest that we represent directions and places by attaching them to a special set of pronome-like agents that we shall call direction-nemes. Later we'll see how these might be involved in surprisingly many realms of thought.",
    "type": "article",
    "title": "24.6 direction-nemes",
    "docId": 249638683065,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 205607436733,
    "gburl": "http://aurellem.org/society-of-mind/som-24.6.html-diffbotxyz591293815",
    "lastCrawlTimeUTC": 1588763630,
    "timestamp": "Wed, 06 May 2020 11:13:50 GMT"
  },
  {
    "sentiment": 0.451,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-704837011",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.7.html",
    "html": "<p>Why do we find it so hard to explain the meanings of things? Because what something <em>means</em> depends upon each different person's state of mind. If so, you might suspect that nothing means exactly the same thing to any two different persons. But if that were the case, where could you start? If every meaning in a person's mind depended on all the other meanings there, wouldn't everything go in circles? And if you couldn't break into those circles, wouldn't it all become too subjective to make good science? No. There is nothing wrong with phenomena in which many things depend on one another. And you don't have to be in those circles in order to understand them; you simply have to make good theories about them. It is a pleasant dream to imagine things being defined so perfectly that different people could understand things in exactly the same ways. But that ideal can't be achieved, because in order for two minds to agree perfectly, at every level of detail, they'd have to be identical.</p>\n<p>The closest we can come to agreeing on meanings is in mathematics, when we talk of things like <em>Three</em> and <em>Five.</em> But even something as impersonal as <em>Five</em> never stands isolated in a person's mind but becomes part of a huge network. For example, we sometimes think of <em>Five</em> for counting things, as when we recite <em>One, Two, Three, Four, Five</em> while taking care l) to touch each thing only once, and 2) never to touch anything more than once. One way to ensure that is to pick up each thing as it's counted and remove it. Another way is to match a group of things to a certain standard set of Five &mdash; such as the fingers of your hand &mdash; or to that silent stream of syllables spoken in the mind. If, one by one, the things are matched and none are left behind, then there were Five. Another way to think of Five is to imagine some familiar shape &mdash; a pentagon, an X or V or W, a star, or even an airplane:</p>\n<p>That way, a child might even come to understand a larger number before a smaller one. I actually knew one child who seemed to know Six before she knew Five, because she'd played so much with sets of triangles and hexagons.</p>\n<p>Each number meaning works in different problem worlds. To ask which meaning is correct &mdash; to count, match, or put into groups &mdash; is foolishness: each method helps the others, and all of them together make a mass of skills that grow in power and efficiency. The really useful <em>meanings</em> are not the flimsy logic chains of definitions, but the much harder-to-express networks of ways to remember, compare, and change things. A logic chain can break easily, but you get stuck less often when you use a cross-connected meaning- network; then, when any sense of meaning fails, you simply switch to another sense. Consider, for example, how many different Twos a child knows: two hands, two feet, two shoes, two socks, and all their interchangeabilities. As for Threes, recall the popular children's tale about three bears. The bears themselves are usually perceived as Two and One &mdash; Momma and Poppa Bear, plus Baby Bear. But their forbidden porridge bowls are seen as quite another sort of Three:</p>\n<p>too hot, too cold, and then just right; a compromise between extremes.</p>",
    "text": "Why do we find it so hard to explain the meanings of things? Because what something means depends upon each different person's state of mind. If so, you might suspect that nothing means exactly the same thing to any two different persons. But if that were the case, where could you start? If every meaning in a person's mind depended on all the other meanings there, wouldn't everything go in circles? And if you couldn't break into those circles, wouldn't it all become too subjective to make good science? No. There is nothing wrong with phenomena in which many things depend on one another. And you don't have to be in those circles in order to understand them; you simply have to make good theories about them. It is a pleasant dream to imagine things being defined so perfectly that different people could understand things in exactly the same ways. But that ideal can't be achieved, because in order for two minds to agree perfectly, at every level of detail, they'd have to be identical.\nThe closest we can come to agreeing on meanings is in mathematics, when we talk of things like Three and Five. But even something as impersonal as Five never stands isolated in a person's mind but becomes part of a huge network. For example, we sometimes think of Five for counting things, as when we recite One, Two, Three, Four, Five while taking care l) to touch each thing only once, and 2) never to touch anything more than once. One way to ensure that is to pick up each thing as it's counted and remove it. Another way is to match a group of things to a certain standard set of Five \u2014 such as the fingers of your hand \u2014 or to that silent stream of syllables spoken in the mind. If, one by one, the things are matched and none are left behind, then there were Five. Another way to think of Five is to imagine some familiar shape \u2014 a pentagon, an X or V or W, a star, or even an airplane:\nThat way, a child might even come to understand a larger number before a smaller one. I actually knew one child who seemed to know Six before she knew Five, because she'd played so much with sets of triangles and hexagons.\nEach number meaning works in different problem worlds. To ask which meaning is correct \u2014 to count, match, or put into groups \u2014 is foolishness: each method helps the others, and all of them together make a mass of skills that grow in power and efficiency. The really useful meanings are not the flimsy logic chains of definitions, but the much harder-to-express networks of ways to remember, compare, and change things. A logic chain can break easily, but you get stuck less often when you use a cross-connected meaning- network; then, when any sense of meaning fails, you simply switch to another sense. Consider, for example, how many different Twos a child knows: two hands, two feet, two shoes, two socks, and all their interchangeabilities. As for Threes, recall the popular children's tale about three bears. The bears themselves are usually perceived as Two and One \u2014 Momma and Poppa Bear, plus Baby Bear. But their forbidden porridge bowls are seen as quite another sort of Three:\ntoo hot, too cold, and then just right; a compromise between extremes.",
    "type": "article",
    "title": "18.7 what is a number?",
    "tags": [
      {
        "score": 0.602871298789978,
        "sentiment": 0,
        "count": 0,
        "label": "One, Two, Three",
        "uri": "https://diffbot.com/entity/XL9ip0nVWNo-y0IPp2thmvQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      }
    ],
    "docId": 34220966279,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 224649412992,
    "gburl": "http://aurellem.org/society-of-mind/som-18.7.html-diffbotxyz2570644450",
    "lastCrawlTimeUTC": 1588763525,
    "timestamp": "Wed, 06 May 2020 11:12:05 GMT"
  },
  {
    "images": [
      {
        "naturalHeight": 140,
        "width": 348,
        "diffbotUri": "image|3|-788484212",
        "url": "http://aurellem.org/society-of-mind/illus/ch26/26-13.png",
        "naturalWidth": 348,
        "primary": true,
        "height": 140
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|65921715",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.8.html",
    "html": "<p>We've seen how a four-word sentence such as <em>Round squares steal honestly</em> could be made to fit a certain four-terminal frame. But what about a sentence like <em>The thief who took the moon moved it to Paris</em>? It would be dreadful if we had to learn a new and special ten-word frame for each particular type of ten-word string! Clearly we don't do any such thing. Instead, we use the pronoun <em>who</em> to make the listener find and fill a second frame. This suggests a multistage theory. In the earliest stages of learning to speak, we simply fill the terminals of word-string frames with nemes for words. Then, later, we learn to fill those terminals with other filled-in language-frames. For example, we can describe our moon sentence as based on a top-level Trans-frame for <em>move</em> whose Actor terminal contains a second Trans-frame for <em>took</em>:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch26/26-13.png\"/></figure>\n<p>Using frames this way simplifies the job of learning to speak by reducing the number of different kinds of frames we have to learn. But it makes language learning harder, too, because we have to learn to work with several frames at once.</p>\n<p>How do we know which terminals to fill with which words? It isn't so hard to deal with <em>red, round, thin-peeled fruit,</em> since each such property involves a different agency. But that won't work for <em>Mary loves Jack,</em> since <em>Jack loves Mary</em> has the very same words, and only their order indicates their different roles. Each child must learn how the order of words affects which terminal each phrase should fill. As it happens, English applies the same policy both to <em>Mary loves Jack</em> and to our moon sentence:</p>\n<p>Assign the Actor pronome to the phrase before the verb. Assign the Object pronome to the phrase after the verb.</p>\n<p>The policies for assigning phrases to pronomes vary from one language to another. The word order for Actor and Object is less constrained in Latin than in English, because in Latin those roles can be specified by altering the nouns themselves. In both languages we often indicate which words should be assigned to other pronome roles by using specific prepositions like <em>for,</em> <em>by,</em> and <em>with.</em> In many cases, different verb types use the same prepositions to indicate the use of different pronomes. At first such usages may seem to be arbitrary, but they frequently encode important systematic metaphors; in section 21.2 we saw how <em>from</em> and <em>to</em> are used to make analogies between space and time. How did our language- forms evolve? We have no record of their earliest forms,</p>\n<p>but they surely were affected at every stage by the kinds of questions and problems that seemed important at the time. The features of present-day languages may still contain some clues about our ancestors' concerns.</p>",
    "text": "We've seen how a four-word sentence such as Round squares steal honestly could be made to fit a certain four-terminal frame. But what about a sentence like The thief who took the moon moved it to Paris? It would be dreadful if we had to learn a new and special ten-word frame for each particular type of ten-word string! Clearly we don't do any such thing. Instead, we use the pronoun who to make the listener find and fill a second frame. This suggests a multistage theory. In the earliest stages of learning to speak, we simply fill the terminals of word-string frames with nemes for words. Then, later, we learn to fill those terminals with other filled-in language-frames. For example, we can describe our moon sentence as based on a top-level Trans-frame for move whose Actor terminal contains a second Trans-frame for took:\nUsing frames this way simplifies the job of learning to speak by reducing the number of different kinds of frames we have to learn. But it makes language learning harder, too, because we have to learn to work with several frames at once.\nHow do we know which terminals to fill with which words? It isn't so hard to deal with red, round, thin-peeled fruit, since each such property involves a different agency. But that won't work for Mary loves Jack, since Jack loves Mary has the very same words, and only their order indicates their different roles. Each child must learn how the order of words affects which terminal each phrase should fill. As it happens, English applies the same policy both to Mary loves Jack and to our moon sentence:\nAssign the Actor pronome to the phrase before the verb. Assign the Object pronome to the phrase after the verb.\nThe policies for assigning phrases to pronomes vary from one language to another. The word order for Actor and Object is less constrained in Latin than in English, because in Latin those roles can be specified by altering the nouns themselves. In both languages we often indicate which words should be assigned to other pronome roles by using specific prepositions like for, by, and with. In many cases, different verb types use the same prepositions to indicate the use of different pronomes. At first such usages may seem to be arbitrary, but they frequently encode important systematic metaphors; in section 21.2 we saw how from and to are used to make analogies between space and time. How did our language- forms evolve? We have no record of their earliest forms,\nbut they surely were affected at every stage by the kinds of questions and problems that seemed important at the time. The features of present-day languages may still contain some clues about our ancestors' concerns.",
    "type": "article",
    "title": "26.8 frames for verbs",
    "docId": 78269383046,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 126466294203,
    "gburl": "http://aurellem.org/society-of-mind/som-26.8.html-diffbotxyz2330641322",
    "lastCrawlTimeUTC": 1588763476,
    "timestamp": "Wed, 06 May 2020 11:11:16 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1433612984",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.3.html",
    "html": "<p>Whenever we consider an action, such as moving from one place to another, we almost always have particular concerns like these:</p>\n<p>Where does the action start? Where does it end? What instrument is used? What is its purpose or goal? What are its effects? What difference will it make?</p>\n<p>We could represent several of these questions with a simple diagram, which we'll call the Trans-frame.</p>\n<p>In the early 1970s, Roger Schank developed ways to represent many situations in terms of a relatively few kinds of relations which he called <em>conceptual dependencies.</em> One of these, called P-Trans, represents a physical motion from one place to another. Another, called M-Trans, represents the sort of mental transportation involved when John tells Mary his telephone number; some information <em>moves</em> from John's memory to Mary's memory. A third type of conceptual dependency, called A-Trans, represents what is involved when Mary buys John's house. The house itself doesn't move at all, but its <em>ownership</em> is transferred from John's estate to Mary's estate.</p>\n<p>But why should we want to represent, in the same way, three such different ideas: transportation in space, transmission of ideas, and transfer of ownership? I suspect that it is for the same reason that our language uses the same word fragment trans for all of them: this is one of those pervasive, systematic cross-realm correspondences that enables us to apply the same or similar mental skills to many different realms of thought. For example, suppose you were to drive first from Boston to New York, and then from New York to Washington. Obviously the overall effect would be equivalent to driving from Boston to Washington &mdash; but that wouldn't be so <em>obvious</em> unless you used a certain kind of mental chaining skill. Similarly, if John told you his phone number, and you then told it to Mary, this would end up much as though John had told Mary directly. And if you first bought John's house and then sold it to Mary, the net result, again, would be as though Mary had bought it directly from John. All three forms of Trans-frames can be used in chains! This means that once you learn efficient chain-manipulating skills, you can apply them to many different kinds of situations and actions. Once you know how to do it, constructing mental chains seems as easy as stringing beads. All you have to do is replace each Trans-frame's Destination with the next one's Origin.</p>",
    "text": "Whenever we consider an action, such as moving from one place to another, we almost always have particular concerns like these:\nWhere does the action start? Where does it end? What instrument is used? What is its purpose or goal? What are its effects? What difference will it make?\nWe could represent several of these questions with a simple diagram, which we'll call the Trans-frame.\nIn the early 1970s, Roger Schank developed ways to represent many situations in terms of a relatively few kinds of relations which he called conceptual dependencies. One of these, called P-Trans, represents a physical motion from one place to another. Another, called M-Trans, represents the sort of mental transportation involved when John tells Mary his telephone number; some information moves from John's memory to Mary's memory. A third type of conceptual dependency, called A-Trans, represents what is involved when Mary buys John's house. The house itself doesn't move at all, but its ownership is transferred from John's estate to Mary's estate.\nBut why should we want to represent, in the same way, three such different ideas: transportation in space, transmission of ideas, and transfer of ownership? I suspect that it is for the same reason that our language uses the same word fragment trans for all of them: this is one of those pervasive, systematic cross-realm correspondences that enables us to apply the same or similar mental skills to many different realms of thought. For example, suppose you were to drive first from Boston to New York, and then from New York to Washington. Obviously the overall effect would be equivalent to driving from Boston to Washington \u2014 but that wouldn't be so obvious unless you used a certain kind of mental chaining skill. Similarly, if John told you his phone number, and you then told it to Mary, this would end up much as though John had told Mary directly. And if you first bought John's house and then sold it to Mary, the net result, again, would be as though Mary had bought it directly from John. All three forms of Trans-frames can be used in chains! This means that once you learn efficient chain-manipulating skills, you can apply them to many different kinds of situations and actions. Once you know how to do it, constructing mental chains seems as easy as stringing beads. All you have to do is replace each Trans-frame's Destination with the next one's Origin.",
    "type": "article",
    "title": "21.3 trans-frames",
    "docId": 91345568141,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 269257310598,
    "gburl": "http://aurellem.org/society-of-mind/som-21.3.html-diffbotxyz1669162328",
    "lastCrawlTimeUTC": 1588763500,
    "timestamp": "Wed, 06 May 2020 11:11:40 GMT"
  },
  {
    "sentiment": 0.348,
    "humanLanguage": "en",
    "diffbotUri": "article|3|438400763",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.4.html",
    "html": "<p>What does a word like <em>apple</em> mean? This is really many questions in one.</p>\n<p>How could hearing the word <em>apple</em> make you <em>imagine</em> an apple? How could seeing an apple activate a word-agent for <em>apple</em>? How could thinking about an apple make one think of the word for <em>apple</em>? How could seeing an apple make one wordlessly recall the flavor of an apple?</p>\n<p>It's usually impossible to perfectly <em>define</em> a word because you cannot capture everything you mean in just a phrase; an apple means a thousand things. However, you can usually say some of what you mean by making lists of properties. For example, you could say that an <em>apple</em> is something round and red and good to eat. But what exactly is a <em>property</em>? Again, it's hard to define that idea &mdash; but there are several things to say about what properties we like our properties to have.</p>\n<p>We like the kinds of properties that do not change capriciously.</p>\n<p>The color of your car will stay the same from day to day, and, barring accidents, so will its basic size and shape, as well as the substances of which it is made. Now, suppose you were to paint that car a new color: its shape and size would remain the same. This suggests another thing we like to find in our properties:</p>\n<p>The most useful sets of properties are those whose members do not interact too much.</p>\n<p>This explains the universal popularity of that particular combination of properties: size, color, shape, and substance. Because these attributes scarcely interact at all with one another, you can put them together in any combination whatsoever, to make an object that is either large or small, red or green, wooden or glass, and having the shape of a sphere or of a cube. And we derive a wonderful power from representing things in terms of properties that do not interact: this makes imagination practical. It lets us anticipate what will happen when we invent new combinations and variations we've never seen before. For example, suppose that a certain object almost works for a certain job &mdash; except for being a bit too small; then you can imagine using a <em>larger</em> one. In the same way, you can imagine changing the color of a dress or its size, shape, or the fabric of which it's made, without altering any of its other properties.</p>\n<p>Why is it so easy to imagine the effects of such changes? First, these properties reflect the nature of reality; when we change an object's color or shape, its other properties are usually left unchanged. However, that doesn't explain why such changes do not interact inside the mind. Why is it so easy to imagine a small brown wooden cube or a long red silk skirt? The simplest explanation is that we represent each of the properties of material, color, size and shape in separate agencies. Then those properties can simultaneously arouse separate partial states of mind at once, in several divisions of the mind. That way, a single word can activate many different kinds of thoughts at once! Thus the word <em>apple</em> can set your Color agency into a <em>redness</em> state, put your Shape agency into a <em>roundness</em> state &mdash; or, really, into a representation of an indented sphere with a stem &mdash; and cause your Taste and Size agencies to react in accord with memories of previous experiences with apples. How does language do such things?</p>",
    "text": "What does a word like apple mean? This is really many questions in one.\nHow could hearing the word apple make you imagine an apple? How could seeing an apple activate a word-agent for apple? How could thinking about an apple make one think of the word for apple? How could seeing an apple make one wordlessly recall the flavor of an apple?\nIt's usually impossible to perfectly define a word because you cannot capture everything you mean in just a phrase; an apple means a thousand things. However, you can usually say some of what you mean by making lists of properties. For example, you could say that an apple is something round and red and good to eat. But what exactly is a property? Again, it's hard to define that idea \u2014 but there are several things to say about what properties we like our properties to have.\nWe like the kinds of properties that do not change capriciously.\nThe color of your car will stay the same from day to day, and, barring accidents, so will its basic size and shape, as well as the substances of which it is made. Now, suppose you were to paint that car a new color: its shape and size would remain the same. This suggests another thing we like to find in our properties:\nThe most useful sets of properties are those whose members do not interact too much.\nThis explains the universal popularity of that particular combination of properties: size, color, shape, and substance. Because these attributes scarcely interact at all with one another, you can put them together in any combination whatsoever, to make an object that is either large or small, red or green, wooden or glass, and having the shape of a sphere or of a cube. And we derive a wonderful power from representing things in terms of properties that do not interact: this makes imagination practical. It lets us anticipate what will happen when we invent new combinations and variations we've never seen before. For example, suppose that a certain object almost works for a certain job \u2014 except for being a bit too small; then you can imagine using a larger one. In the same way, you can imagine changing the color of a dress or its size, shape, or the fabric of which it's made, without altering any of its other properties.\nWhy is it so easy to imagine the effects of such changes? First, these properties reflect the nature of reality; when we change an object's color or shape, its other properties are usually left unchanged. However, that doesn't explain why such changes do not interact inside the mind. Why is it so easy to imagine a small brown wooden cube or a long red silk skirt? The simplest explanation is that we represent each of the properties of material, color, size and shape in separate agencies. Then those properties can simultaneously arouse separate partial states of mind at once, in several divisions of the mind. That way, a single word can activate many different kinds of thoughts at once! Thus the word apple can set your Color agency into a redness state, put your Shape agency into a roundness state \u2014 or, really, into a representation of an indented sphere with a stem \u2014 and cause your Taste and Size agencies to react in accord with memories of previous experiences with apples. How does language do such things?",
    "type": "article",
    "title": "19.4 objects and properties",
    "tags": [
      {
        "score": 0.6211363673210144,
        "sentiment": 0.764,
        "count": 12,
        "label": "apple",
        "uri": "https://diffbot.com/entity/XgrXdplpfMRWDa_VBAOd2oA"
      },
      {
        "score": 0.5001033544540405,
        "sentiment": 0.405,
        "count": 1,
        "label": "Dig Out Your Soul",
        "uri": "https://diffbot.com/entity/XOZFmJbBbMh-avK043BbITw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 116232389023,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 65137344937,
    "gburl": "http://aurellem.org/society-of-mind/som-19.4.html-diffbotxyz1564018961",
    "lastCrawlTimeUTC": 1588763545,
    "timestamp": "Wed, 06 May 2020 11:12:25 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-301779563",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-14.5.html",
    "html": "<blockquote> To him that has, more shall be given; but from him that has not, the little that he has shall be taken away. &mdash;St. Matthew </blockquote>\n<p>Some ideas acquire undue influence. The prominence of the body-support idea is well-deserved; no other scheme compares to its ability to help us link things into causelike chains. But there are other, not so honorable ways for ideas to gather influence.</p>\n<p>The Investment Principle: Our oldest ideas have unfair advantages over those that come later. The earlier we learn a skill, the more methods we can acquire for using it. Each new idea must then compete against the larger mass of skills the old ideas have accumulated.</p>\n<p>This is why it's so much easier to do new things in older ways. Each new idea, however good in principle, seems awkward until we master it. So old ideas keep gaining strength, while new ones can rarely catch up. Furthermore, our oldest and best-developed skills will be the first to spread to other realms of thought where again they'll start out far enough ahead to keep any new ideas from taking root.</p>\n<p>In the short run, you will usually do better by using an old idea than by starting out anew. If you can already play the piano well, it is easy to start playing the organ in the same way. The many superficial similarities will make it hard for you to tell which aspects of your old skills are unsuitable, and the easiest course is to keep applying your old technique, trying to patch each flaw until none show. In the long run, you'd probably do better by starting fresh with a new technique &mdash; and then borrowing what you can from your older skills. The trouble is that we're almost always immersed in the <em>short run.</em> So the principles both of investment and of exception make us reluctant to tamper with our well-established skills and uniframes lest we endanger all that we have built upon those old foundations. I don't mean to say there's anything wrong, in principle, with using what you are comfortable with and already know. But it is dangerous to support your old ideas merely by accumulating ways to sidestep their deficiencies. That only increases the power of your old ideas to overcome new ones and could lead your style of thought to base itself yet all the more, as time goes by, upon less and less.</p>\n<p>Evolution illustrates how processes can become enslaved by the investment principle. Why do so many animals contain their brains inside their heads &mdash; as with fish, amphibians, reptiles, birds, and bats? This arrangement was inherited long before our earliest aquatic ancestor first crawled upon the land three hundred million years ago. For many of those animals &mdash; woodpeckers, for example &mdash; another arrangement might serve at least as well. But once the pattern of centralizing so many functions in the head was established, it carried with it great networks of dependencies involving many aspects of anatomy. Because of this, any mutation that changed any part of that arrangement would disrupt many other parts and lead to dreadful handicaps, at least in the short run of evolution. And because evolution is so inherently short-sighted, it would not help if, over longer spans of time, such changes could lead to advantages. Perhaps the best example of this can be seen in the fact that virtually every detail of every plant and animal on earth is written in terms of a genetic code that has scarcely changed a single bit in a billion years. It does not seem to be a particularly efficient or reliable code, yet so many structures have been based on it that all living things are stuck with it! To change a single detail of that code would cause so many proteins to get tangled up that not a single cell could live.</p>",
    "text": "To him that has, more shall be given; but from him that has not, the little that he has shall be taken away. \u2014St. Matthew\nSome ideas acquire undue influence. The prominence of the body-support idea is well-deserved; no other scheme compares to its ability to help us link things into causelike chains. But there are other, not so honorable ways for ideas to gather influence.\nThe Investment Principle: Our oldest ideas have unfair advantages over those that come later. The earlier we learn a skill, the more methods we can acquire for using it. Each new idea must then compete against the larger mass of skills the old ideas have accumulated.\nThis is why it's so much easier to do new things in older ways. Each new idea, however good in principle, seems awkward until we master it. So old ideas keep gaining strength, while new ones can rarely catch up. Furthermore, our oldest and best-developed skills will be the first to spread to other realms of thought where again they'll start out far enough ahead to keep any new ideas from taking root.\nIn the short run, you will usually do better by using an old idea than by starting out anew. If you can already play the piano well, it is easy to start playing the organ in the same way. The many superficial similarities will make it hard for you to tell which aspects of your old skills are unsuitable, and the easiest course is to keep applying your old technique, trying to patch each flaw until none show. In the long run, you'd probably do better by starting fresh with a new technique \u2014 and then borrowing what you can from your older skills. The trouble is that we're almost always immersed in the short run. So the principles both of investment and of exception make us reluctant to tamper with our well-established skills and uniframes lest we endanger all that we have built upon those old foundations. I don't mean to say there's anything wrong, in principle, with using what you are comfortable with and already know. But it is dangerous to support your old ideas merely by accumulating ways to sidestep their deficiencies. That only increases the power of your old ideas to overcome new ones and could lead your style of thought to base itself yet all the more, as time goes by, upon less and less.\nEvolution illustrates how processes can become enslaved by the investment principle. Why do so many animals contain their brains inside their heads \u2014 as with fish, amphibians, reptiles, birds, and bats? This arrangement was inherited long before our earliest aquatic ancestor first crawled upon the land three hundred million years ago. For many of those animals \u2014 woodpeckers, for example \u2014 another arrangement might serve at least as well. But once the pattern of centralizing so many functions in the head was established, it carried with it great networks of dependencies involving many aspects of anatomy. Because of this, any mutation that changed any part of that arrangement would disrupt many other parts and lead to dreadful handicaps, at least in the short run of evolution. And because evolution is so inherently short-sighted, it would not help if, over longer spans of time, such changes could lead to advantages. Perhaps the best example of this can be seen in the fact that virtually every detail of every plant and animal on earth is written in terms of a genetic code that has scarcely changed a single bit in a billion years. It does not seem to be a particularly efficient or reliable code, yet so many structures have been based on it that all living things are stuck with it! To change a single detail of that code would cause so many proteins to get tangled up that not a single cell could live.",
    "type": "article",
    "title": "14.5 the investment principle",
    "docId": 251298726276,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 146330272164,
    "gburl": "http://aurellem.org/society-of-mind/som-14.5.html-diffbotxyz2625316413",
    "lastCrawlTimeUTC": 1588763576,
    "timestamp": "Wed, 06 May 2020 11:12:56 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1987148084",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-28.3.html",
    "html": "<p>We've scarcely mentioned at all inside this book the kinds of quantities that could be <em>measured</em> &mdash; though surely brain cells use them all the time. For example, it seems quite likely that many of our agents employ quantitative schemes for summarizing evidence or establishing the strengths of connections. But I have said little about such matters because I suspect that such matters play diminished roles, the more we move toward higher-level operations of the mind. This is because whenever we're forced to compare magnitudes, we have to pay a heavy price: it tends to terminate what we call <em>thinking.</em></p>\n<p>Whenever we turn to measurements, we forfeit some uses of intellect. Currencies and magnitudes help us make comparisons only by concealing the differences among what they purport to represent.</p>\n<p>By their nature, quantitative descriptions are so one-dimensional and featureless that they cannot help but conceal the structures that give rise to them. This is inescapable, since any act that makes two different things comparable must do it by deflecting our attention from their differences. Numbers themselves are the greatest masters of disguise because they perfectly conceal all traces of their origins. Add five and eight to make thirteen, and tell that answer to a friend: thirteen will be all your friend can know, since no amount of ingenious thought can ever show that it came from adding five and eight! It's much the same inside the head: quantitative judgments help us make decisions only by keeping us from thinking too much about the actual evidence.</p>\n<p>No matter that such judgments have faults; you often have no choice but to choose. This happens when you can't stay where you are and must turn either right or left. Somewhere in some agencies, alternatives must be compared &mdash; and sometimes one can find no way except by using currencies. Then, various agents in your brain may turn to whatever quantities &mdash; chemical, electrical, or whatever &mdash; that happen to be available. Any substance or quantity whose availability is limited can be made to serve as a currency. But when we make our theories about how such systems work, we simply must remember not to make the easy mistake of confusing those quantities with their adopted functions and thus, for example, believing that certain drugs are inherently <em>stimulating</em> or <em>depressing,</em> or that certain foodstuffs are inherently more <em>natural,</em> or more <em>healthy.</em> Most of the properties of a currency are not inherent &mdash; but merely conventional.</p>\n<p>In any case, we should never assume that the quality or character of a thought process depends directly on the nature of the circumstances that evoke it. There is no quality of <em>sweetness</em> inherent in sugar itself, which is a mere chemical. Its quality of sweetness is, in effect, a currency involved with certain agencies that are connected to sensors that detect the presence of sugar. Those agencies evolved that way because whenever we have hunger goals, it pays to recognize the taste of sugar as a <em>sign of success</em> &mdash; simply because sugar itself supplies energy, is easy to detect, and usually indicates the presence of other edible sources of nutrition. Similarly, inside our brains, many agencies have come to influence one another by controlling the amounts of various chemicals in much the way that many kinds of human transactions have come to use substances like candy, coins, or bags of salt &mdash; or banknotes backed by promises.</p>",
    "text": "We've scarcely mentioned at all inside this book the kinds of quantities that could be measured \u2014 though surely brain cells use them all the time. For example, it seems quite likely that many of our agents employ quantitative schemes for summarizing evidence or establishing the strengths of connections. But I have said little about such matters because I suspect that such matters play diminished roles, the more we move toward higher-level operations of the mind. This is because whenever we're forced to compare magnitudes, we have to pay a heavy price: it tends to terminate what we call thinking.\nWhenever we turn to measurements, we forfeit some uses of intellect. Currencies and magnitudes help us make comparisons only by concealing the differences among what they purport to represent.\nBy their nature, quantitative descriptions are so one-dimensional and featureless that they cannot help but conceal the structures that give rise to them. This is inescapable, since any act that makes two different things comparable must do it by deflecting our attention from their differences. Numbers themselves are the greatest masters of disguise because they perfectly conceal all traces of their origins. Add five and eight to make thirteen, and tell that answer to a friend: thirteen will be all your friend can know, since no amount of ingenious thought can ever show that it came from adding five and eight! It's much the same inside the head: quantitative judgments help us make decisions only by keeping us from thinking too much about the actual evidence.\nNo matter that such judgments have faults; you often have no choice but to choose. This happens when you can't stay where you are and must turn either right or left. Somewhere in some agencies, alternatives must be compared \u2014 and sometimes one can find no way except by using currencies. Then, various agents in your brain may turn to whatever quantities \u2014 chemical, electrical, or whatever \u2014 that happen to be available. Any substance or quantity whose availability is limited can be made to serve as a currency. But when we make our theories about how such systems work, we simply must remember not to make the easy mistake of confusing those quantities with their adopted functions and thus, for example, believing that certain drugs are inherently stimulating or depressing, or that certain foodstuffs are inherently more natural, or more healthy. Most of the properties of a currency are not inherent \u2014 but merely conventional.\nIn any case, we should never assume that the quality or character of a thought process depends directly on the nature of the circumstances that evoke it. There is no quality of sweetness inherent in sugar itself, which is a mere chemical. Its quality of sweetness is, in effect, a currency involved with certain agencies that are connected to sensors that detect the presence of sugar. Those agencies evolved that way because whenever we have hunger goals, it pays to recognize the taste of sugar as a sign of success \u2014 simply because sugar itself supplies energy, is easy to detect, and usually indicates the presence of other edible sources of nutrition. Similarly, inside our brains, many agencies have come to influence one another by controlling the amounts of various chemicals in much the way that many kinds of human transactions have come to use substances like candy, coins, or bags of salt \u2014 or banknotes backed by promises.",
    "type": "article",
    "title": "28.3 quantity and quality",
    "docId": 60224504196,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 75465949607,
    "gburl": "http://aurellem.org/society-of-mind/som-28.3.html-diffbotxyz1598574210",
    "lastCrawlTimeUTC": 1588763375,
    "timestamp": "Wed, 06 May 2020 11:09:35 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1306143286",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.7.html",
    "html": "<p>It would be wonderful if we could classify all behavior into two types: <em>built-in</em> and <em>learned.</em> But there simply is no clear-cut boundary between heredity and environment. Later, I'll describe an agency that is sure to learn one particular thing: to recognize human beings. But if such an agency is destined to end up with a certain particular behavior, is it reasonable to say that it learns? Since this type of activity appears to have no common name, we'll call it <em>predestined learning.</em></p>\n<p>Every child eventually learns to reach for food. To be sure, each different child lives through a different history of <em>reaching-act</em> experiences. Nevertheless, according to our theory of <em>nearness models of space,</em> all those children will end up with generally similar results because that outcome is constrained by the nearness relations of real-world space. Why make the brain use a tedious learning process when the final outcome seems so clear? Why not build in the answer genetically? One reason could be that learning is more economical. It would require an enormous store of genetic information to force each separate nerve cell to make precisely the right connections, whereas it would require much less information to specify the construction of a learning machine designed to unscramble whatever irregularities result from a less constrained design.</p>\n<p>This is why it isn't sensible to ask, <em>Is the child's conception of space acquired or inherited?</em> We acquire our conceptions of space by using agencies that learn in accord with processes determined by inheritance. These agencies proceed to learn from experience &mdash; but the outcomes of their learning processes are virtually predestined by the spatial geometry of our body parts. This kind of mixture of adaptation and predestination is quite common in biology, not only in the brain's development but in that of the rest of the body as well. How, for example, do our genes control the shapes and sizes of our bones? They may begin with some relatively precise specification of the types and location of certain early cells. But that alone would not be adequate for animals that themselves have to adapt to different conditions; therefore those early cells must themselves be programmed to adapt to the various chemical and mechanical influences that may later be imposed on them. Such systems are essential for our development, since our organs must become able to perform various tightly constrained activities, yet also be able to adapt to changing circumstances.</p>\n<p>Perhaps the growth of the Society-of-More is another instance of predestined learning, for it seems to develop in every normal child without much outside help. It seems clear that this complex agency is not built directly by inborn genes; instead, we each discover our own ways to represent comparisons &mdash; yet we all arrive at much the same final outcome. Presumably, genetic hints must help with this by supplying new layers of agents at roughly the right times and places.</p>",
    "text": "It would be wonderful if we could classify all behavior into two types: built-in and learned. But there simply is no clear-cut boundary between heredity and environment. Later, I'll describe an agency that is sure to learn one particular thing: to recognize human beings. But if such an agency is destined to end up with a certain particular behavior, is it reasonable to say that it learns? Since this type of activity appears to have no common name, we'll call it predestined learning.\nEvery child eventually learns to reach for food. To be sure, each different child lives through a different history of reaching-act experiences. Nevertheless, according to our theory of nearness models of space, all those children will end up with generally similar results because that outcome is constrained by the nearness relations of real-world space. Why make the brain use a tedious learning process when the final outcome seems so clear? Why not build in the answer genetically? One reason could be that learning is more economical. It would require an enormous store of genetic information to force each separate nerve cell to make precisely the right connections, whereas it would require much less information to specify the construction of a learning machine designed to unscramble whatever irregularities result from a less constrained design.\nThis is why it isn't sensible to ask, Is the child's conception of space acquired or inherited? We acquire our conceptions of space by using agencies that learn in accord with processes determined by inheritance. These agencies proceed to learn from experience \u2014 but the outcomes of their learning processes are virtually predestined by the spatial geometry of our body parts. This kind of mixture of adaptation and predestination is quite common in biology, not only in the brain's development but in that of the rest of the body as well. How, for example, do our genes control the shapes and sizes of our bones? They may begin with some relatively precise specification of the types and location of certain early cells. But that alone would not be adequate for animals that themselves have to adapt to different conditions; therefore those early cells must themselves be programmed to adapt to the various chemical and mechanical influences that may later be imposed on them. Such systems are essential for our development, since our organs must become able to perform various tightly constrained activities, yet also be able to adapt to changing circumstances.\nPerhaps the growth of the Society-of-More is another instance of predestined learning, for it seems to develop in every normal child without much outside help. It seems clear that this complex agency is not built directly by inborn genes; instead, we each discover our own ways to represent comparisons \u2014 yet we all arrive at much the same final outcome. Presumably, genetic hints must help with this by supplying new layers of agents at roughly the right times and places.",
    "type": "article",
    "title": "11.7 predestined learning",
    "docId": 113478336912,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 117789000097,
    "gburl": "http://aurellem.org/society-of-mind/som-11.7.html-diffbotxyz436764933",
    "lastCrawlTimeUTC": 1588763409,
    "timestamp": "Wed, 06 May 2020 11:10:09 GMT"
  },
  {
    "sentiment": -0.368,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1757713230",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.1.html",
    "html": "<p>What happens when a child reads a story that begins like this?</p>\n<p>Mary was invited to Jack's party. She wondered if he would like a kite.</p>\n<p>If you asked what that kite was for, most people would answer that it must be a birthday present for Jack. How amazing it is that every normal person can make such complicated inferences so rapidly &mdash; considering that the idea of a gift was never mentioned at all! Could any machine do such remarkable things? Consider all the other assumptions and conclusions that almost everyone will make:</p>\n<p>The <em>party</em> is a birthday party. Jack and Mary are children. <em>She</em> is Mary. <em>He</em> is Jack.</p>\n<p>She is considering giving Jack a kite. She wonders if he would like the kite.</p>\n<p>We call these understandings <em>common sense.</em> They're made so swiftly that they're often ready in our minds before a sentence is complete! But how is this done? In order to realize that the kite is a present, one has to use such knowledge as that parties involve presents, that presents for children are usually toys, and that kites are appropriate toys to be given as presents. None of this is mentioned in the story itself. How do we bring together all that scattered knowledge so quickly? Here's what I think must happen. Somehow the words <em>Mary was invited to Jack's party</em> arouses a <em>party- invitation</em> frame in the reader's mind &mdash; and attached to the terminals of that frame are certain memories of various concerns. Who is the host? Who will attend? What present should I bring? What clothing shall I wear? Each of those concerns, in turn, is represented by a frame to whose terminals are already attached, as default assignments, the most usual solutions to that particular kind of problem.</p>\n<p>Such knowledge comes from previous experience. I was raised in a culture in which an invitation to a party carries the obligation to arrive well dressed and to bring a birthday present. Accordingly, when I read or hear that Mary was invited to a party, I attribute to Mary the same sorts of subjective reactions and concerns that I would have in such a situation. Therefore, although the story never mentions clothes or gifts at all, to expect their possible involvement seems only simple common sense. But though it is common, it is not simple. The next few sections speculate about how story understanding works.</p>",
    "text": "What happens when a child reads a story that begins like this?\nMary was invited to Jack's party. She wondered if he would like a kite.\nIf you asked what that kite was for, most people would answer that it must be a birthday present for Jack. How amazing it is that every normal person can make such complicated inferences so rapidly \u2014 considering that the idea of a gift was never mentioned at all! Could any machine do such remarkable things? Consider all the other assumptions and conclusions that almost everyone will make:\nThe party is a birthday party. Jack and Mary are children. She is Mary. He is Jack.\nShe is considering giving Jack a kite. She wonders if he would like the kite.\nWe call these understandings common sense. They're made so swiftly that they're often ready in our minds before a sentence is complete! But how is this done? In order to realize that the kite is a present, one has to use such knowledge as that parties involve presents, that presents for children are usually toys, and that kites are appropriate toys to be given as presents. None of this is mentioned in the story itself. How do we bring together all that scattered knowledge so quickly? Here's what I think must happen. Somehow the words Mary was invited to Jack's party arouses a party- invitation frame in the reader's mind \u2014 and attached to the terminals of that frame are certain memories of various concerns. Who is the host? Who will attend? What present should I bring? What clothing shall I wear? Each of those concerns, in turn, is represented by a frame to whose terminals are already attached, as default assignments, the most usual solutions to that particular kind of problem.\nSuch knowledge comes from previous experience. I was raised in a culture in which an invitation to a party carries the obligation to arrive well dressed and to bring a birthday present. Accordingly, when I read or hear that Mary was invited to a party, I attribute to Mary the same sorts of subjective reactions and concerns that I would have in such a situation. Therefore, although the story never mentions clothes or gifts at all, to expect their possible involvement seems only simple common sense. But though it is common, it is not simple. The next few sections speculate about how story understanding works.",
    "type": "article",
    "title": "26.1 understanding words",
    "tags": [
      {
        "score": 0.7260013222694397,
        "sentiment": 0.997,
        "count": 6,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.7153427600860596,
        "sentiment": 0.995,
        "count": 4,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6169215440750122,
        "sentiment": 0.994,
        "count": 7,
        "label": "party",
        "uri": "https://diffbot.com/entity/X7FemNyybOPS4CAzOu9lbJA"
      },
      {
        "score": 0.5918310880661011,
        "sentiment": 0.996,
        "count": 2,
        "label": "Jack Shephard",
        "uri": "https://diffbot.com/entity/XPb-cc-yAPbK9HeP1VhSLpQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5524377822875977,
        "sentiment": -0.364,
        "count": 4,
        "label": "narrative",
        "uri": "https://diffbot.com/entity/X-yV-ag8aOfmhY9T-yRt-wg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.522997260093689,
        "sentiment": 0,
        "count": 2,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 211298222521,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 22262284689,
    "gburl": "http://aurellem.org/society-of-mind/som-26.1.html-diffbotxyz2891074759",
    "lastCrawlTimeUTC": 1588763334,
    "timestamp": "Wed, 06 May 2020 11:08:54 GMT"
  },
  {
    "sentiment": 0.512,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-2042996446",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-1.3.html",
    "html": "<p>You know that everything you think and do is thought and done by you. But what's a <em>you</em>? What kinds of smaller entities cooperate inside your mind to do your work? To start to see how minds are like societies, try this: pick up a cup of tea!</p>\n<p>Your GRASPING agents want to keep hold of the cup. Your BALANCING agents want to keep the tea from spilling out. Your THIRST agents want you to drink the tea. Your MOVING agents want to get the cup to your lips.</p>\n<p>Yet none of these consume your mind as you roam about the room talking to your friends. You scarcely think at all about Balance; Balance has no concern with Grasp; Grasp has no interest in Thirst; and Thirst is not involved with your social problems. Why not? Because they can depend on one another. If each does its own little job, the really big job will get done by all of them together: drinking tea.</p>\n<p>How many processes are going on, to keep that teacup level in your grasp? There must be at least a hundred of them, just to shape your wrist and palm and hand. Another thousand muscle systems must work to manage all the moving bones and joints that make your body walk around. And to keep everything in balance, each of those processes has to communicate with some of the others. What if you stumble and start to fall? Then many other processes quickly try to get things straight. Some of them are concerned with how you lean and where you place your feet. Others are occupied with what to do about the tea: you wouldn't want to burn your own hand, but neither would you want to scald someone else. You need ways to make quick decisions.</p>\n<p>All this happens while you talk, and none of it appears to need much thought. But when you come to think of it, neither does your talk itself. What kinds of agents choose your words so that you can express the things you mean? How do those words get arranged into phrases and sentences, each connected to the next? What agencies inside your mind keep track of all the things you've said &mdash; and, also, whom you've said them to? How foolish it can make you feel when you repeat &mdash; unless you're sure your audience is new.</p>\n<p>We're always doing several things at once, like planning and walking and talking, and this all seems so natural that we take it for granted. But these processes actually involve more machinery than anyone can understand all at once. So, in the next few sections of this book, we'll focus on just one ordinary activity &mdash; making things with children's building-blocks. First we'll break this process into smaller parts, and then we'll see how each of them relates to all the other parts.</p>\n<p>In doing this, we'll try to imitate how Galileo and Newton learned so much by studying the simplest kinds of pendulums and weights, mirrors and prisms. Our study of how to build with blocks will be like focusing a microscope on the simplest objects we can find, to open up a great and unexpected universe. It is the same reason why so many biologists today devote more attention to tiny germs and viruses than to magnificent lions and tigers. For me and a whole generation of students, the world of work with children's blocks has been the prism and the pendulum for studying intelligence. In science, one can learn the most by studying what seems the least.</p>",
    "text": "You know that everything you think and do is thought and done by you. But what's a you? What kinds of smaller entities cooperate inside your mind to do your work? To start to see how minds are like societies, try this: pick up a cup of tea!\nYour GRASPING agents want to keep hold of the cup. Your BALANCING agents want to keep the tea from spilling out. Your THIRST agents want you to drink the tea. Your MOVING agents want to get the cup to your lips.\nYet none of these consume your mind as you roam about the room talking to your friends. You scarcely think at all about Balance; Balance has no concern with Grasp; Grasp has no interest in Thirst; and Thirst is not involved with your social problems. Why not? Because they can depend on one another. If each does its own little job, the really big job will get done by all of them together: drinking tea.\nHow many processes are going on, to keep that teacup level in your grasp? There must be at least a hundred of them, just to shape your wrist and palm and hand. Another thousand muscle systems must work to manage all the moving bones and joints that make your body walk around. And to keep everything in balance, each of those processes has to communicate with some of the others. What if you stumble and start to fall? Then many other processes quickly try to get things straight. Some of them are concerned with how you lean and where you place your feet. Others are occupied with what to do about the tea: you wouldn't want to burn your own hand, but neither would you want to scald someone else. You need ways to make quick decisions.\nAll this happens while you talk, and none of it appears to need much thought. But when you come to think of it, neither does your talk itself. What kinds of agents choose your words so that you can express the things you mean? How do those words get arranged into phrases and sentences, each connected to the next? What agencies inside your mind keep track of all the things you've said \u2014 and, also, whom you've said them to? How foolish it can make you feel when you repeat \u2014 unless you're sure your audience is new.\nWe're always doing several things at once, like planning and walking and talking, and this all seems so natural that we take it for granted. But these processes actually involve more machinery than anyone can understand all at once. So, in the next few sections of this book, we'll focus on just one ordinary activity \u2014 making things with children's building-blocks. First we'll break this process into smaller parts, and then we'll see how each of them relates to all the other parts.\nIn doing this, we'll try to imitate how Galileo and Newton learned so much by studying the simplest kinds of pendulums and weights, mirrors and prisms. Our study of how to build with blocks will be like focusing a microscope on the simplest objects we can find, to open up a great and unexpected universe. It is the same reason why so many biologists today devote more attention to tiny germs and viruses than to magnificent lions and tigers. For me and a whole generation of students, the world of work with children's blocks has been the prism and the pendulum for studying intelligence. In science, one can learn the most by studying what seems the least.",
    "type": "article",
    "title": "1.3 The society of mind",
    "tags": [
      {
        "score": 0.7099220156669617,
        "sentiment": 0.503,
        "count": 2,
        "label": "Thirst",
        "uri": "https://diffbot.com/entity/X0lItTEtCNHaCPw6VMDZDeg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      },
      {
        "score": 0.5929576754570007,
        "sentiment": 0.539,
        "count": 4,
        "label": "tea",
        "uri": "https://diffbot.com/entity/Xj0MZGNoMMPi-GZEZ2jlypA"
      },
      {
        "score": 0.5800184011459351,
        "sentiment": -0.206,
        "count": 3,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5315985083580017,
        "sentiment": -0.191,
        "count": 2,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 220135408024,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 47966208418,
    "gburl": "http://aurellem.org/society-of-mind/som-1.3.html-diffbotxyz2104000313",
    "lastCrawlTimeUTC": 1588763447,
    "timestamp": "Wed, 06 May 2020 11:10:47 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-2055177129",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-10.7.html",
    "html": "<p>In learning their Societies-of-More, children learn various skills for comparing different qualities and quantities, like number and extent. It is tempting to try to summarize all that by saying that the children are learning something; we could call it the concept of quantity. But why do we feel we have to think of what we learn as things or concepts? Why must we <em>thingify</em> everything?</p>\n<p>What is a thing? No one doubts that a child's building-block is a thing. But is a child's love for its mother also a <em>thing</em>? We're imprisoned by our poverty of words because even though we have good ways to describe objects and actions, we lack methods for describing dispositions and processes. We can scarcely speak of what minds do except as though they were filled with things that one could see or touch; that's why we cling to terms like <em>concepts</em> and <em>ideas.</em> I don't mean to say that this is always bad, for <em>thing-ifying</em> is indeed a splendid mental instrument. But for our present purpose, it is disastrous to assume that our minds contain some single <em>concept of quantity.</em> At different times, a word like <em>more</em> can mean many different kinds of things. Think about each of these expressions.</p>\n<p>More colorful. More loud. More swift. More valuable. More complicated.</p>\n<p>We speak as though these were similar, yet each of them involves a different, hard-earned web of ways to think! The phrase <em>more loud</em> might seem at first to be merely a matter of magnitude. But consider how the sound of a distant gong seems louder than a whisper near the ear &mdash; no matter that its actual intensity is less. Your reaction to what you hear depends not only on its physical intensity, but also on what your agencies conclude about the character of its source. Thus you can usually tell whether a gong is loud but distant, rather than soft but close, by unconsciously making assumptions about the origin of that sound. And all those other kinds of <em>more</em> engage equally subtle sorts of expertise.</p>\n<p>Instead of assuming that our children come to crystallize a single <em>concept of quantity,</em> we must try to discover how our children accumulate and classify their many methods for comparing things. How do agents like Tall, Thin, Short, and Wide get formed into subagencies? To an adult, it seems natural to associate both being taller and being wider with being larger. But what prevents the child from inventing senseless <em>concepts</em> such as <em>being Green and Tall and having recently been touched</em>? No child has the time to generate and test all possible combinations to find which ones are sensible. Life is too short to do that many bad experiments! The secret is: always try to combine related agents first. Tall, Thin, Short, and Wide are all closely related, because they are all concerned with making comparisons between spatial qualities. In fact, they probably involve agencies that are close to one another in the brain and share so many agents in common that they'll naturally seem similar.</p>",
    "text": "In learning their Societies-of-More, children learn various skills for comparing different qualities and quantities, like number and extent. It is tempting to try to summarize all that by saying that the children are learning something; we could call it the concept of quantity. But why do we feel we have to think of what we learn as things or concepts? Why must we thingify everything?\nWhat is a thing? No one doubts that a child's building-block is a thing. But is a child's love for its mother also a thing? We're imprisoned by our poverty of words because even though we have good ways to describe objects and actions, we lack methods for describing dispositions and processes. We can scarcely speak of what minds do except as though they were filled with things that one could see or touch; that's why we cling to terms like concepts and ideas. I don't mean to say that this is always bad, for thing-ifying is indeed a splendid mental instrument. But for our present purpose, it is disastrous to assume that our minds contain some single concept of quantity. At different times, a word like more can mean many different kinds of things. Think about each of these expressions.\nMore colorful. More loud. More swift. More valuable. More complicated.\nWe speak as though these were similar, yet each of them involves a different, hard-earned web of ways to think! The phrase more loud might seem at first to be merely a matter of magnitude. But consider how the sound of a distant gong seems louder than a whisper near the ear \u2014 no matter that its actual intensity is less. Your reaction to what you hear depends not only on its physical intensity, but also on what your agencies conclude about the character of its source. Thus you can usually tell whether a gong is loud but distant, rather than soft but close, by unconsciously making assumptions about the origin of that sound. And all those other kinds of more engage equally subtle sorts of expertise.\nInstead of assuming that our children come to crystallize a single concept of quantity, we must try to discover how our children accumulate and classify their many methods for comparing things. How do agents like Tall, Thin, Short, and Wide get formed into subagencies? To an adult, it seems natural to associate both being taller and being wider with being larger. But what prevents the child from inventing senseless concepts such as being Green and Tall and having recently been touched? No child has the time to generate and test all possible combinations to find which ones are sensible. Life is too short to do that many bad experiments! The secret is: always try to combine related agents first. Tall, Thin, Short, and Wide are all closely related, because they are all concerned with making comparisons between spatial qualities. In fact, they probably involve agencies that are close to one another in the brain and share so many agents in common that they'll naturally seem similar.",
    "type": "article",
    "title": "10.7 the concept of concept",
    "docId": 102840189319,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 221744841147,
    "gburl": "http://aurellem.org/society-of-mind/som-10.7.html-diffbotxyz3754126518",
    "lastCrawlTimeUTC": 1588763220,
    "timestamp": "Wed, 06 May 2020 11:07:00 GMT"
  },
  {
    "sentiment": 0.745,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1409803818",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-14.html",
    "text": "",
    "type": "article",
    "title": "14 reformulation",
    "docId": 183120855441,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 155907178943,
    "gburl": "http://aurellem.org/society-of-mind/som-14.html-diffbotxyz3523384958",
    "lastCrawlTimeUTC": 1588763294,
    "timestamp": "Wed, 06 May 2020 11:08:14 GMT"
  },
  {
    "sentiment": 0.798,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-921665934",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-16.2.html",
    "html": "<h2><em>16.2</em> mental growth</h2>\n<p>In ancient times it was believed that the newborn mind started out just like a full-grown mind, except for not yet being filled with ideas. Thus children were seen as ignorant adults, conceived with all their future aptitudes. Today, there are many different views. Some modern theories see a baby's mind as starting with a single Self whose problem is to learn to distinguish itself from the rest of the world. Others see the infant's mind as a place containing a horde of mind-fragments, mixed together in a disconnected and incoherent confusion in which each must learn to interact and cooperate with the others so that they can grow together to form a more coherent whole. Yet another image sees the child's mind as growing through a series of layerlike construction stages in which new levels of machinery are based and built upon the older ones.</p>\n<p>How do our minds form? Is every person born containing a hidden, built-in intellect just waiting to reveal itself? Or must minds grow in little steps from emptiness? The theories of the next few sections will combine ingredients from both these conceptions. We'll start by envisioning a simple brain composed of separate <em>proto-specialists,</em> each concerned with some important requirement, goal, or instinct, like food, drink, shelter, comfort, or defense. But there are reasons why those systems must be merged. On one side, we need administrative agencies to resolve conflicts between the separate specialists. On the other side, each specialist must be able to exploit whatever knowledge the others gain.</p>\n<p>For a relatively simple animal, a loose-knit league of nearly separate agencies with built-in goals might suffice for surviving in a suitable environment. But human minds don't merely learn new ways to reach old goals; we can also learn new kinds of goals. This enables us to live within a broader range of possible environments, but that versatility comes with its own dangers. If we could learn new goals without constraint, we'd soon fall prey to accidents &mdash; both in the world and inside our own minds. At the simplest levels, we have to be protected against such accidents as learning not to breathe. On higher levels, we need protection against acquiring lethal goals like learning to suppress our other goals entirely &mdash; the way that certain saints and mystics do. What sorts of built-in self-constraints could guide a mind toward goals that will not cause it to destroy itself?</p>\n<p>No possible inheritance of built-in genes can tell us what is good for us &mdash; because, unlike all other animals, we humans make for ourselves most of the problems we face. Accordingly, each human individual must learn new goals from what we call the traditions and heritages of our peers and predecessors. Consequently our genes must build some sort of <em>general-purpose</em> machinery through which individuals can acquire and transmit goals and values from one generation to another. How could brain-machines transfer things like values and goals? The next few sections suggest that this is done by exploiting the kinds of personal relationships we call emotional, such as fear and affection, attachment and dependency, or hate and love.</p>",
    "text": "16.2 mental growth\nIn ancient times it was believed that the newborn mind started out just like a full-grown mind, except for not yet being filled with ideas. Thus children were seen as ignorant adults, conceived with all their future aptitudes. Today, there are many different views. Some modern theories see a baby's mind as starting with a single Self whose problem is to learn to distinguish itself from the rest of the world. Others see the infant's mind as a place containing a horde of mind-fragments, mixed together in a disconnected and incoherent confusion in which each must learn to interact and cooperate with the others so that they can grow together to form a more coherent whole. Yet another image sees the child's mind as growing through a series of layerlike construction stages in which new levels of machinery are based and built upon the older ones.\nHow do our minds form? Is every person born containing a hidden, built-in intellect just waiting to reveal itself? Or must minds grow in little steps from emptiness? The theories of the next few sections will combine ingredients from both these conceptions. We'll start by envisioning a simple brain composed of separate proto-specialists, each concerned with some important requirement, goal, or instinct, like food, drink, shelter, comfort, or defense. But there are reasons why those systems must be merged. On one side, we need administrative agencies to resolve conflicts between the separate specialists. On the other side, each specialist must be able to exploit whatever knowledge the others gain.\nFor a relatively simple animal, a loose-knit league of nearly separate agencies with built-in goals might suffice for surviving in a suitable environment. But human minds don't merely learn new ways to reach old goals; we can also learn new kinds of goals. This enables us to live within a broader range of possible environments, but that versatility comes with its own dangers. If we could learn new goals without constraint, we'd soon fall prey to accidents \u2014 both in the world and inside our own minds. At the simplest levels, we have to be protected against such accidents as learning not to breathe. On higher levels, we need protection against acquiring lethal goals like learning to suppress our other goals entirely \u2014 the way that certain saints and mystics do. What sorts of built-in self-constraints could guide a mind toward goals that will not cause it to destroy itself?\nNo possible inheritance of built-in genes can tell us what is good for us \u2014 because, unlike all other animals, we humans make for ourselves most of the problems we face. Accordingly, each human individual must learn new goals from what we call the traditions and heritages of our peers and predecessors. Consequently our genes must build some sort of general-purpose machinery through which individuals can acquire and transmit goals and values from one generation to another. How could brain-machines transfer things like values and goals? The next few sections suggest that this is done by exploiting the kinds of personal relationships we call emotional, such as fear and affection, attachment and dependency, or hate and love.",
    "type": "article",
    "title": "16.2 mental growth",
    "docId": 262786433412,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 194035286401,
    "gburl": "http://aurellem.org/society-of-mind/som-16.2.html-diffbotxyz2727602058",
    "lastCrawlTimeUTC": 1588763256,
    "timestamp": "Wed, 06 May 2020 11:07:36 GMT"
  },
  {
    "sentiment": 0.473,
    "humanLanguage": "en",
    "diffbotUri": "article|3|531091862",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-30.3.html",
    "html": "<h2><em>30.3</em> mental models</h2>\n<p>Does a book know what is written inside it? Clearly, no. Does a book contain knowledge? Clearly, yes. But how could anything contain knowledge, yet not know it? We've seen how saying that a person or machine possesses knowledge amounts to saying that some observer could employ that person or machine to answer certain kinds of questions. Here is another view of what it means to know.</p>\n<p><em>Jack knows about A</em> means that there is a <em>model</em> M of A inside Jack's head.</p>\n<p>But what does it mean to say that one thing is a model of another and how could one have a model in one's head? Again, we have to specify some standard or authority. Let's make Jack be the judge of that:</p>\n<p>Jack considers M to be a good model of A to the extent that he finds M useful for answering questions about A.</p>\n<p>For example, suppose that A is a real automobile, and M is the kind of object we call a <em>toy</em> or <em>model</em> car. Then Jack will be able to use M to answer certain questions about A. However, we would think it strange to say that M is Jack's <em>knowledge</em> about A &mdash; because we reserve the word <em>knowledge</em> for something inside a head, and Jack can't keep a toy inside his head. But we never said that a model must be an ordinary physical object. Our definition allows a model to be anything that helps a person answer questions. Accordingly, a person could possess a <em>mental model,</em> too &mdash; in the form of some machinery or subsociety of agents inside the brain. This provides us with a simple explanation of what we mean by knowledge: Jack's knowledge about A is simply whichever mental models, processes, or agencies Jack's other agencies can use to answer questions about A. Thus, a person's mental model of a car need not itself resemble an actual car in any obvious way. It need not itself be heavy, fast, or consume gasoline to be able to answer questions about a car like <em>How heavy is it?</em> or <em>How fast can it go?</em></p>\n<p>Our mental models also work in social realms to answer questions like <em>Who owns that car?</em> or <em>Who permitted you to park it there?</em> However, to understand questions like these, we have to ask what people mean by <em>who</em> &mdash; and the answer is that we make mental models of people, too. In order for Mary to <em>know</em> about Jack's dispositions, motives, and possessions, Mary has to build inside her head some structure to help answer those kinds of questions &mdash; and that structure will constitute her mental model of Jack. Just think of all the different things our person-models do for us! If Mary knows Jack well enough, she'll be able to reply not only to physical questions like <em>How tall is Jack?</em> but also to social inquiries such as <em>Does he like me?</em> and even to psychological queries like <em>What are Jack's ideals?</em> Quite possibly, Mary's model of Jack will be able to produce more accurate answers to such questions than Jack himself could produce. People's mental models of their friends are often better, in certain respects, than their mental models of themselves.</p>\n<p>We all make models of ourselves and use them to predict which sorts of things we'll later be disposed to do. Naturally, our models of ourselves will often provide us with wrong answers because they are not faultless ways to see ourselves, but merely self-made answering machines.</p>",
    "text": "30.3 mental models\nDoes a book know what is written inside it? Clearly, no. Does a book contain knowledge? Clearly, yes. But how could anything contain knowledge, yet not know it? We've seen how saying that a person or machine possesses knowledge amounts to saying that some observer could employ that person or machine to answer certain kinds of questions. Here is another view of what it means to know.\nJack knows about A means that there is a model M of A inside Jack's head.\nBut what does it mean to say that one thing is a model of another and how could one have a model in one's head? Again, we have to specify some standard or authority. Let's make Jack be the judge of that:\nJack considers M to be a good model of A to the extent that he finds M useful for answering questions about A.\nFor example, suppose that A is a real automobile, and M is the kind of object we call a toy or model car. Then Jack will be able to use M to answer certain questions about A. However, we would think it strange to say that M is Jack's knowledge about A \u2014 because we reserve the word knowledge for something inside a head, and Jack can't keep a toy inside his head. But we never said that a model must be an ordinary physical object. Our definition allows a model to be anything that helps a person answer questions. Accordingly, a person could possess a mental model, too \u2014 in the form of some machinery or subsociety of agents inside the brain. This provides us with a simple explanation of what we mean by knowledge: Jack's knowledge about A is simply whichever mental models, processes, or agencies Jack's other agencies can use to answer questions about A. Thus, a person's mental model of a car need not itself resemble an actual car in any obvious way. It need not itself be heavy, fast, or consume gasoline to be able to answer questions about a car like How heavy is it? or How fast can it go?\nOur mental models also work in social realms to answer questions like Who owns that car? or Who permitted you to park it there? However, to understand questions like these, we have to ask what people mean by who \u2014 and the answer is that we make mental models of people, too. In order for Mary to know about Jack's dispositions, motives, and possessions, Mary has to build inside her head some structure to help answer those kinds of questions \u2014 and that structure will constitute her mental model of Jack. Just think of all the different things our person-models do for us! If Mary knows Jack well enough, she'll be able to reply not only to physical questions like How tall is Jack? but also to social inquiries such as Does he like me? and even to psychological queries like What are Jack's ideals? Quite possibly, Mary's model of Jack will be able to produce more accurate answers to such questions than Jack himself could produce. People's mental models of their friends are often better, in certain respects, than their mental models of themselves.\nWe all make models of ourselves and use them to predict which sorts of things we'll later be disposed to do. Naturally, our models of ourselves will often provide us with wrong answers because they are not faultless ways to see ourselves, but merely self-made answering machines.",
    "type": "article",
    "title": "30.3 mental models",
    "docId": 31760400803,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 185649152420,
    "gburl": "http://aurellem.org/society-of-mind/som-30.3.html-diffbotxyz3753352859",
    "lastCrawlTimeUTC": 1588763123,
    "timestamp": "Wed, 06 May 2020 11:05:23 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|65678993",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.1.html",
    "html": "<blockquote> The wind blows where it will, and you hear the sound of it, but you do not know whence it comes or whither it goes; so it is with every one who is born of the Spirit. &mdash;St. John </blockquote>\n<p>Language builds things in our minds. Yet words themselves can't be the substance of our thoughts. They have no meanings by themselves; they're only special sorts of marks or sounds. If we're to understand how language works, we must discard the usual view that words denote or represent, or designate; instead, their function is control: each word makes various agents change what various other agents do. If we want to understand how language works, we must never forget that our thinking-in-words reveals only a fragment of the mind's activity.</p>\n<p>We often seem to think in words. Yet we do this with no conscious sense of where and why those words originate or how they then proceed to influence our further thoughts and what we subsequently do. Our inner monologues and dialogues proceed without any effort, deliberation, or sense of how they're done. Now you might argue that you do know what brings those words to mind &mdash; in the sense that they are how you <em>express</em> your intentions and ideas. But that amounts to the same thing &mdash; since your intentions, too, appear to come and go in ways you do not understand. Suppose, for example, that at a certain moment you find you want to leave the room. Then, naturally, you'd look for the door. And this involves two mysteries:</p>\n<p>What made you want to leave the room? Was it simply that you became tired of staying in that room? Was it because you remembered something else you had to do? Whatever reasons come to mind, you still must ask what led to them. The further back you trace your thoughts, the vaguer seem those causal chains.</p>\n<p>The other side of the mystery is that we are equally ignorant of</p>\n<p>how we respond to our own intentions. Given a desire to leave the room, what led you to the thought of <em>door</em>? You only know that first you thought, <em>It's time to go,</em> and then you thought, <em>Where is the door?</em></p>\n<p>We're all so used to this that we regard it as completely natural. Yet we have barely any sense of why each thought follows the last. What connects the idea of leaving with the idea of door? Does this result from some direct connection between two partial states of mind, of leaving and of door? Does it involve some sort of less direct connection, not between those states themselves, but only between some signals that somehow represent those states? Or is it the product of yet more complex mechanisms?</p>\n<p>Our introspective abilities are too weak to answer such questions. The words we think seem to hover in some insubstantial interface wherein we understand neither the origins of the symbol-signs that seem to express our desires nor the destinations wherein they lead to actions and accomplishments. This is why words and images seem so magical: they work without our knowing how or why. At one moment a word can seem enormously meaningful; at the next moment it can seem no more than a sequence of sounds. And this is as it should be. It is the underlying emptiness of words that gives them their potential versatility. The less there is in a treasure chest, the more you'll be able to put in it.</p>",
    "text": "The wind blows where it will, and you hear the sound of it, but you do not know whence it comes or whither it goes; so it is with every one who is born of the Spirit. \u2014St. John\nLanguage builds things in our minds. Yet words themselves can't be the substance of our thoughts. They have no meanings by themselves; they're only special sorts of marks or sounds. If we're to understand how language works, we must discard the usual view that words denote or represent, or designate; instead, their function is control: each word makes various agents change what various other agents do. If we want to understand how language works, we must never forget that our thinking-in-words reveals only a fragment of the mind's activity.\nWe often seem to think in words. Yet we do this with no conscious sense of where and why those words originate or how they then proceed to influence our further thoughts and what we subsequently do. Our inner monologues and dialogues proceed without any effort, deliberation, or sense of how they're done. Now you might argue that you do know what brings those words to mind \u2014 in the sense that they are how you express your intentions and ideas. But that amounts to the same thing \u2014 since your intentions, too, appear to come and go in ways you do not understand. Suppose, for example, that at a certain moment you find you want to leave the room. Then, naturally, you'd look for the door. And this involves two mysteries:\nWhat made you want to leave the room? Was it simply that you became tired of staying in that room? Was it because you remembered something else you had to do? Whatever reasons come to mind, you still must ask what led to them. The further back you trace your thoughts, the vaguer seem those causal chains.\nThe other side of the mystery is that we are equally ignorant of\nhow we respond to our own intentions. Given a desire to leave the room, what led you to the thought of door? You only know that first you thought, It's time to go, and then you thought, Where is the door?\nWe're all so used to this that we regard it as completely natural. Yet we have barely any sense of why each thought follows the last. What connects the idea of leaving with the idea of door? Does this result from some direct connection between two partial states of mind, of leaving and of door? Does it involve some sort of less direct connection, not between those states themselves, but only between some signals that somehow represent those states? Or is it the product of yet more complex mechanisms?\nOur introspective abilities are too weak to answer such questions. The words we think seem to hover in some insubstantial interface wherein we understand neither the origins of the symbol-signs that seem to express our desires nor the destinations wherein they lead to actions and accomplishments. This is why words and images seem so magical: they work without our knowing how or why. At one moment a word can seem enormously meaningful; at the next moment it can seem no more than a sequence of sounds. And this is as it should be. It is the underlying emptiness of words that gives them their potential versatility. The less there is in a treasure chest, the more you'll be able to put in it.",
    "type": "article",
    "title": "19.1 the roots of intention",
    "docId": 95676891570,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 9859416501,
    "gburl": "http://aurellem.org/society-of-mind/som-19.1.html-diffbotxyz1452089977",
    "lastCrawlTimeUTC": 1588763147,
    "timestamp": "Wed, 06 May 2020 11:05:47 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-301282149",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-29.2.html",
    "html": "<p>To see that we can think in several mental realms at once, consider the role of the word <em>give</em> in this simple sentence:</p>\n<p>Mary gives Jack the kite.</p>\n<p>We can see at least three distinct meanings here. First, we could represent the idea of the kite's motion through physical space by using a Trans-frame whose Trajectory begins at Mary's hand and ends at Jack's.</p>\n<p>But quite beyond that realm of space, we also find a different significance in what Mary did &mdash; in another realm that we'll call</p>\n<p><em>estates.</em> This involves a different sense of <em>give,</em> in which the object need not actually move at all! Instead, what happens is the transfer of its ownership.</p>\n<p>Each of us has an <em>estate</em> &mdash; the collection of possessions we control. And this <em>realm of estate</em> is more important than it might seem, because it lies between the realms of objects and ideas. In order to carry out our plans, it is not enough only to know what things or ideas are required and how to adapt them to our purposes. We must also be able to take possession of those objects or ideas, either by right or by might.</p>\n<p>Possession plays essential roles in all our plans, because we can't use any materials, tools, or ideas until we gain control of them.</p>\n<p>We can also interpret Mary's act within a social realm, in which we understand that giving gifts involves yet other kinds of relationships. No sooner do you hear of Mary's gift than certain parts of your mind become concerned with why she was so generous and how this involved her affections and obligations.</p>\n<p>How can so many different thoughts proceed at the same time, without interfering with one another? I suspect that it is for the same reason that we have no trouble imagining an apple as both round and red at the same time: in that case, the processes for color and shape use agents that do not compete. Similarly, the different processes involved with ideas like <em>give</em> may operate in agencies so different that they rarely need to compete for the same resources.</p>",
    "text": "To see that we can think in several mental realms at once, consider the role of the word give in this simple sentence:\nMary gives Jack the kite.\nWe can see at least three distinct meanings here. First, we could represent the idea of the kite's motion through physical space by using a Trans-frame whose Trajectory begins at Mary's hand and ends at Jack's.\nBut quite beyond that realm of space, we also find a different significance in what Mary did \u2014 in another realm that we'll call\nestates. This involves a different sense of give, in which the object need not actually move at all! Instead, what happens is the transfer of its ownership.\nEach of us has an estate \u2014 the collection of possessions we control. And this realm of estate is more important than it might seem, because it lies between the realms of objects and ideas. In order to carry out our plans, it is not enough only to know what things or ideas are required and how to adapt them to our purposes. We must also be able to take possession of those objects or ideas, either by right or by might.\nPossession plays essential roles in all our plans, because we can't use any materials, tools, or ideas until we gain control of them.\nWe can also interpret Mary's act within a social realm, in which we understand that giving gifts involves yet other kinds of relationships. No sooner do you hear of Mary's gift than certain parts of your mind become concerned with why she was so generous and how this involved her affections and obligations.\nHow can so many different thoughts proceed at the same time, without interfering with one another? I suspect that it is for the same reason that we have no trouble imagining an apple as both round and red at the same time: in that case, the processes for color and shape use agents that do not compete. Similarly, the different processes involved with ideas like give may operate in agencies so different that they rarely need to compete for the same resources.",
    "type": "article",
    "title": "29.2 several thoughts at once",
    "docId": 232468824457,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 181598863781,
    "gburl": "http://aurellem.org/society-of-mind/som-29.2.html-diffbotxyz2669805196",
    "lastCrawlTimeUTC": 1588763088,
    "timestamp": "Wed, 06 May 2020 11:04:48 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1364140997",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.7.html",
    "html": "<p>Let's make a dumbbell theory of some people's personalities.</p>\n<p>Uniframers disregard discrepancies in favor of imagined regularities. They tend to be perfectionists but also tend to think in terms of stereotypes. This sometimes leads to recklessness because they have to reject some evidence in order to make their uniframes. Accumulators are less extreme. They keep collecting evidence and hence are much less prone to make mistakes. But then they're also slower to make discoveries.</p>\n<p>Of course these imaginary personalities are only caricatures, and everyone blends both extremes. Most people find some reasonable compromise, though a few of us lean more in one direction than the other. I'm sure we all use mixtures of different learning strategies</p>\n<p>&mdash; accumulations of descriptions, K-lines, uniframes, or whatever. On the surface, it might seem easier to make accumulations than to make uniframes &mdash; but choosing what to accumulate may require deeper insight. In any case, whenever an accumulation becomes too large and clumsy, we try to replace some groups of its members with a uniframe. But even when we succeed in finding a suitably compact uniframe, we can expect it, too, to accumulate exceptions eventually, since first descriptions rarely work for all our later purposes.</p>\n<p>For example, when a child first encounters dogs, an attempt might be made to create a uniframe that catalogs those animals' parts &mdash; eyes, ears, teeth, head, body, tail, legs, and so on. But the child will eventually have to learn that even here there are exceptions.</p>\n<p>Furthermore, that uniframe won't help answer the child's most urgent questions about any one dog in particular: Is it friendly? Does it have a loud bark? Is it the kind that tends to bite? Each such concern could require building a different kind of hierarchy-tree.</p>\n<p>This leads to an inescapable difficulty. Our various motives and concerns are likely to require incompatible ways to classify things. You can't predict a dog's bite from its bark. Each of the classifications we build must embody different kinds of knowledge, and we can rarely use more than a few of them at once. When we have a goal that is simple and clear, we may be able to select one particular kind of description that makes the problem easy to solve. But when goals of several types conflict, it is harder to know just what to do.</p>",
    "text": "Let's make a dumbbell theory of some people's personalities.\nUniframers disregard discrepancies in favor of imagined regularities. They tend to be perfectionists but also tend to think in terms of stereotypes. This sometimes leads to recklessness because they have to reject some evidence in order to make their uniframes. Accumulators are less extreme. They keep collecting evidence and hence are much less prone to make mistakes. But then they're also slower to make discoveries.\nOf course these imaginary personalities are only caricatures, and everyone blends both extremes. Most people find some reasonable compromise, though a few of us lean more in one direction than the other. I'm sure we all use mixtures of different learning strategies\n\u2014 accumulations of descriptions, K-lines, uniframes, or whatever. On the surface, it might seem easier to make accumulations than to make uniframes \u2014 but choosing what to accumulate may require deeper insight. In any case, whenever an accumulation becomes too large and clumsy, we try to replace some groups of its members with a uniframe. But even when we succeed in finding a suitably compact uniframe, we can expect it, too, to accumulate exceptions eventually, since first descriptions rarely work for all our later purposes.\nFor example, when a child first encounters dogs, an attempt might be made to create a uniframe that catalogs those animals' parts \u2014 eyes, ears, teeth, head, body, tail, legs, and so on. But the child will eventually have to learn that even here there are exceptions.\nFurthermore, that uniframe won't help answer the child's most urgent questions about any one dog in particular: Is it friendly? Does it have a loud bark? Is it the kind that tends to bite? Each such concern could require building a different kind of hierarchy-tree.\nThis leads to an inescapable difficulty. Our various motives and concerns are likely to require incompatible ways to classify things. You can't predict a dog's bite from its bark. Each of the classifications we build must embody different kinds of knowledge, and we can rarely use more than a few of them at once. When we have a goal that is simple and clear, we may be able to select one particular kind of description that makes the problem easy to solve. But when goals of several types conflict, it is harder to know just what to do.",
    "type": "article",
    "title": "12.7 accumulation strategies",
    "docId": 270357479833,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 182089187757,
    "gburl": "http://aurellem.org/society-of-mind/som-12.7.html-diffbotxyz1666384221",
    "lastCrawlTimeUTC": 1588763192,
    "timestamp": "Wed, 06 May 2020 11:06:32 GMT"
  },
  {
    "sentiment": 0.451,
    "images": [
      {
        "naturalHeight": 113,
        "width": 334,
        "diffbotUri": "image|3|-1310343673",
        "url": "http://aurellem.org/society-of-mind/illus/ch3/3-3.png",
        "naturalWidth": 334,
        "primary": true,
        "height": 113
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-600436053",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-3.4.html",
    "html": "<h2><em>3.4</em> Heterarchies</h2>\n<p>A hierarchical society is like a tree in which the agent at each branch is exclusively responsible for the agents on the twigs that branch from it. This pattern is found in every field, because dividing work into parts like that is usually the easiest way to start solving a problem. It is easy to construct and understand such organizations because each agent has only a single job to do: it needs only to <em>look up</em> for instructions from its supervisor, then <em>look down</em> to get help from its subordinates.</p>\n<p>But hierarchies do not always work. Consider that when two agents need to use each other's skills, then neither one can be <em>on top.</em> Notice what happens, for example, when you ask your vision-system to decide whether the following left-side scene depicts three blocks &mdash; or only two.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch3/3-3.png\"/></figure>\n<p>The agent See could answer that if it could Move the front block out of the line of view. But, in the course of doing that, Move might have to See if there were any obstacles that might interfere with the arm's trajectory. At such a moment, Move would be working for See, and See would be working for Move, both at the same time. This would be impossible inside a simple hierarchy.</p>\n<p>Most of the diagrams in the early parts of this book depict simple hierarchies. Later, we'll see more cross-connected rings and loops &mdash; when we are forced to consider the need for memory, which will become a constant subject of concern in this book. People often think of memory in terms of keeping records of the past, for recollecting things that happened in earlier times. But agencies also need other kinds of memory as well. See, for example, requires some sort of temporary memory in order to keep track of what next to do, when it starts one job before its previous job is done. If each of See's agents could do only one thing at a time, it would soon run out of resources and be unable to solve complicated problems. But if we have enough memory, we can arrange our agents into circular loops and thus use the same agents over and over again to do parts of several different jobs at the same time.</p>",
    "text": "3.4 Heterarchies\nA hierarchical society is like a tree in which the agent at each branch is exclusively responsible for the agents on the twigs that branch from it. This pattern is found in every field, because dividing work into parts like that is usually the easiest way to start solving a problem. It is easy to construct and understand such organizations because each agent has only a single job to do: it needs only to look up for instructions from its supervisor, then look down to get help from its subordinates.\nBut hierarchies do not always work. Consider that when two agents need to use each other's skills, then neither one can be on top. Notice what happens, for example, when you ask your vision-system to decide whether the following left-side scene depicts three blocks \u2014 or only two.\nThe agent See could answer that if it could Move the front block out of the line of view. But, in the course of doing that, Move might have to See if there were any obstacles that might interfere with the arm's trajectory. At such a moment, Move would be working for See, and See would be working for Move, both at the same time. This would be impossible inside a simple hierarchy.\nMost of the diagrams in the early parts of this book depict simple hierarchies. Later, we'll see more cross-connected rings and loops \u2014 when we are forced to consider the need for memory, which will become a constant subject of concern in this book. People often think of memory in terms of keeping records of the past, for recollecting things that happened in earlier times. But agencies also need other kinds of memory as well. See, for example, requires some sort of temporary memory in order to keep track of what next to do, when it starts one job before its previous job is done. If each of See's agents could do only one thing at a time, it would soon run out of resources and be unable to solve complicated problems. But if we have enough memory, we can arrange our agents into circular loops and thus use the same agents over and over again to do parts of several different jobs at the same time.",
    "type": "article",
    "title": "3.4 Heterarchies",
    "tags": [
      {
        "score": 0.5480462312698364,
        "sentiment": 0.855,
        "count": 6,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 94426038684,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 100051534220,
    "gburl": "http://aurellem.org/society-of-mind/som-3.4.html-diffbotxyz703147545",
    "lastCrawlTimeUTC": 1588762993,
    "timestamp": "Wed, 06 May 2020 11:03:13 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1759019714",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.2.html",
    "html": "<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<p>We've all heard jokes about how stupid present-day computers are. They send us bills and checks for zero dollars and zero cents. They don't mind working in endless loops, repeating the same thing a billion times. Their total lack of common sense is another reason people think that no machine could have a mind.</p>\n<p>It is interesting to note that some of the earliest computer programs excelled at what people consider to be <em>expert</em> skills. A 1956 program solved hard problems in mathematical logic, and a 1961 program solved college-level problems in calculus. Yet not till the 1970s could we construct robot programs that could see and move well enough to arrange children's building-blocks into simple towers and playhouses. Why could we make programs do grown-up things before we could make them do childish things? The answer may seem paradoxical: much of <em>expert</em> adult thinking is actually simpler than what is involved when ordinary children play! Why is it easier to program what experts do than what children do?</p>\n<p>What people vaguely call common sense is actually more intricate than most of the technical expertise we admire. Neither that <em>expert</em> program for logic nor the one for calculus embodied more than a hundred or so <em>facts</em> &mdash; and most of them were rather similar to one another. Yet these were enough to solve college-level problems. In contrast, think of all the different kinds of things a child must know merely to build a house of blocks &mdash; a process that involves knowledge of shapes and colors, space and time, support and balance, and an ability to keep track of what one is doing.</p>\n<p>To be considered an <em>expert,</em> one needs a large amount of knowledge of only a relatively few varieties. In contrast, an ordinary person's <em>common sense</em> involves a much larger variety of different types of knowledge &mdash; and this requires more complicated management systems.</p>\n<p>There is a simple reason why it is easier to acquire specialized knowledge than commonsense knowledge. Each type of knowledge needs some form of <em>representation</em> and a body of skills adapted to using that style of representation. Once that investment has been made, it is relatively easy for a specialist to accumulate further knowledge, provided the additional expertise is uniform enough to suit the same style of representation. A lawyer, doctor, architect, or composer who has learned to deal with a range of cases in some particular field finds it relatively easy to acquire more knowledge of a similar character. Think how much longer it would take a single person to learn to deal competently with a few diseases and several kinds of law cases and a small variety of architectural blueprints and a few orchestral scores. The greater variety of representations would make it much harder to acquire the <em>same amount</em> of knowledge. For each new domain, our novice would have to learn another type of representation and new skills for using it. It would be like learning many different languages, each with its own grammar, lexicon, and idioms. When seen this way, what children do seems all the more remarkable, since so many of their actions are based upon their own inventions and discoveries.</p>",
    "text": "Your browser does not support the video tag. Your browser does not support the video tag.\nWe've all heard jokes about how stupid present-day computers are. They send us bills and checks for zero dollars and zero cents. They don't mind working in endless loops, repeating the same thing a billion times. Their total lack of common sense is another reason people think that no machine could have a mind.\nIt is interesting to note that some of the earliest computer programs excelled at what people consider to be expert skills. A 1956 program solved hard problems in mathematical logic, and a 1961 program solved college-level problems in calculus. Yet not till the 1970s could we construct robot programs that could see and move well enough to arrange children's building-blocks into simple towers and playhouses. Why could we make programs do grown-up things before we could make them do childish things? The answer may seem paradoxical: much of expert adult thinking is actually simpler than what is involved when ordinary children play! Why is it easier to program what experts do than what children do?\nWhat people vaguely call common sense is actually more intricate than most of the technical expertise we admire. Neither that expert program for logic nor the one for calculus embodied more than a hundred or so facts \u2014 and most of them were rather similar to one another. Yet these were enough to solve college-level problems. In contrast, think of all the different kinds of things a child must know merely to build a house of blocks \u2014 a process that involves knowledge of shapes and colors, space and time, support and balance, and an ability to keep track of what one is doing.\nTo be considered an expert, one needs a large amount of knowledge of only a relatively few varieties. In contrast, an ordinary person's common sense involves a much larger variety of different types of knowledge \u2014 and this requires more complicated management systems.\nThere is a simple reason why it is easier to acquire specialized knowledge than commonsense knowledge. Each type of knowledge needs some form of representation and a body of skills adapted to using that style of representation. Once that investment has been made, it is relatively easy for a specialist to accumulate further knowledge, provided the additional expertise is uniform enough to suit the same style of representation. A lawyer, doctor, architect, or composer who has learned to deal with a range of cases in some particular field finds it relatively easy to acquire more knowledge of a similar character. Think how much longer it would take a single person to learn to deal competently with a few diseases and several kinds of law cases and a small variety of architectural blueprints and a few orchestral scores. The greater variety of representations would make it much harder to acquire the same amount of knowledge. For each new domain, our novice would have to learn another type of representation and new skills for using it. It would be like learning many different languages, each with its own grammar, lexicon, and idioms. When seen this way, what children do seems all the more remarkable, since so many of their actions are based upon their own inventions and discoveries.",
    "type": "article",
    "title": "7.2 uncommon sense",
    "docId": 141239730616,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 18590826910,
    "gburl": "http://aurellem.org/society-of-mind/som-7.2.html-diffbotxyz1931064704",
    "lastCrawlTimeUTC": 1588762965,
    "timestamp": "Wed, 06 May 2020 11:02:45 GMT"
  },
  {
    "date": "Fri, 27 Mar 2020 00:00:00 GMT",
    "sentiment": -0.709,
    "humanLanguage": "en",
    "estimatedDate": "Fri, 27 Mar 2020 00:00:00 GMT",
    "diffbotUri": "article|3|-1099682701",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-27.3.html",
    "html": "<p>To see what suppressors and censors have to do, we must consider not only the mental states that actually occur, but others that might occur under slightly different circumstances.</p>\n<p>Suppressors work by interceding to prevent actions just before they would be performed. This leads to a certain loss of time, because nothing can be done until acceptable alternatives can be found. Censors avoid this waste of time by interceding earlier. Instead of waiting until an action is about to occur, and then shutting it off, a censor operates earlier, when there still remains time to select alternatives. Then, instead of blocking the course of thought, the censor can merely deflect it into an acceptable direction. Accordingly, no time is lost.</p>\n<p>Clearly, censors can be more efficient than suppressors, but we have to pay a price for this. The farther back we go in time, the larger the variety of ways to reach each unwanted state of mind. Accordingly, to prevent a particular mental state from occurring, an early-acting censor must learn to recognize all the states of mind that might precede it. Thus, each censor may, in time, require a substantial memory bank. For all we know, each person accumulates millions of censor memories, to avoid the thought-patterns found to be ineffectual or harmful.</p>\n<p>Why not move farther back in time, to deflect those undesired actions even earlier? Then intercepting agents could have even larger effects with smaller efforts and, by selecting good paths early enough, we could solve complex problems without making any mistakes at all. Unfortunately, this cannot be accomplished only by using censors. This is because as we extend a censor's range back into time, the amount of inhibitory memory that would be needed (in order to prevent turns in every possible wrong direction) would grow exponentially. To solve a complex problem, it is not enough to know what might go wrong. One also needs some positive plan.</p>\n<p>As I mentioned before, it is easier to notice what your mind does than to notice what it doesn't do, and this means that we can't use introspection to perceive the work of these inhibitory agencies. I suspect that this effect has seriously distorted our conceptions of psychology and that once we recognize the importance of censors and other forms of <em>negative recognizers,</em> we'll find that they constitute large portions of our minds.</p>\n<p>Sometimes, though, our censors and suppressors must themselves be suppressed. In order to sketch out long-range plans, for example, we must adopt a style of thought that clears the mind of trivia and sets minor obstacles aside. But that could be very hard to do if too many censors remained on the scene; they'd make us shy away from strategies that aren't guaranteed to work, and tear apart our sketchy plans before we can start to accomplish them.</p>",
    "text": "To see what suppressors and censors have to do, we must consider not only the mental states that actually occur, but others that might occur under slightly different circumstances.\nSuppressors work by interceding to prevent actions just before they would be performed. This leads to a certain loss of time, because nothing can be done until acceptable alternatives can be found. Censors avoid this waste of time by interceding earlier. Instead of waiting until an action is about to occur, and then shutting it off, a censor operates earlier, when there still remains time to select alternatives. Then, instead of blocking the course of thought, the censor can merely deflect it into an acceptable direction. Accordingly, no time is lost.\nClearly, censors can be more efficient than suppressors, but we have to pay a price for this. The farther back we go in time, the larger the variety of ways to reach each unwanted state of mind. Accordingly, to prevent a particular mental state from occurring, an early-acting censor must learn to recognize all the states of mind that might precede it. Thus, each censor may, in time, require a substantial memory bank. For all we know, each person accumulates millions of censor memories, to avoid the thought-patterns found to be ineffectual or harmful.\nWhy not move farther back in time, to deflect those undesired actions even earlier? Then intercepting agents could have even larger effects with smaller efforts and, by selecting good paths early enough, we could solve complex problems without making any mistakes at all. Unfortunately, this cannot be accomplished only by using censors. This is because as we extend a censor's range back into time, the amount of inhibitory memory that would be needed (in order to prevent turns in every possible wrong direction) would grow exponentially. To solve a complex problem, it is not enough to know what might go wrong. One also needs some positive plan.\nAs I mentioned before, it is easier to notice what your mind does than to notice what it doesn't do, and this means that we can't use introspection to perceive the work of these inhibitory agencies. I suspect that this effect has seriously distorted our conceptions of psychology and that once we recognize the importance of censors and other forms of negative recognizers, we'll find that they constitute large portions of our minds.\nSometimes, though, our censors and suppressors must themselves be suppressed. In order to sketch out long-range plans, for example, we must adopt a style of thought that clears the mind of trivia and sets minor obstacles aside. But that could be very hard to do if too many censors remained on the scene; they'd make us shy away from strategies that aren't guaranteed to work, and tear apart our sketchy plans before we can start to accomplish them.",
    "type": "article",
    "title": "27.3 censors",
    "tags": [
      {
        "score": 0.868609607219696,
        "sentiment": -0.599,
        "count": 7,
        "label": "Roman censor",
        "uri": "https://diffbot.com/entity/rKf0jC588Pi2Duwn_6D0WtQ"
      }
    ],
    "docId": 181285290381,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 38479315341,
    "gburl": "http://aurellem.org/society-of-mind/som-27.3.html-diffbotxyz2064755498",
    "lastCrawlTimeUTC": 1588763026,
    "timestamp": "Wed, 06 May 2020 11:03:46 GMT"
  },
  {
    "sentiment": -0.433,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1131142105",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.3.html",
    "html": "<p>Many people reason that machines do only what they're programmed to do &mdash; and hence can never be creative or original. The trouble is that this argument presumes what it purports to show: that you can't program a machine to be creative! In fact, it is surprisingly easy to program a computer so that it will proceed to do more different things than any programmer could imagine in advance. This is possible because of what we'll call the <em>puzzle principle.</em></p>\n<p>Puzzle Principle: We can program a computer to solve any problem by trial and error, without knowing how to solve it in advance, provided only that we have a way to recognize when the problem is solved.</p>\n<p>By <em>trial and error</em> we mean programming the machine systematically to generate all possible structures within some universe of possibilities. For example, suppose you wished to have a robot machine that could build a bridge across a stream. The most efficient program for this would simply execute a specific procedure, planned out in advance, to precisely place some boards and nails. Of course, you couldn't write such a program unless you already knew how to build a bridge. But consider the alternative below, which is sometimes called the generate and test method. It consists of writing a two-part program.</p>\n<p>Generate. The first process simply produces, one after another, every possible arrangement of the boards and nails. At first, you might expect such a program to be hard to write. But it turns out to be surprisingly easy, once you appreciate that there is no requirement for each arrangement to make any sense whatsoever! Test. The second part of the process examines each arrangement to see whether the problem has been solved. If the goal were to build a dam, the test is simply whether it holds back the stream. If the goal were to build a bridge, the test is simply whether it spans the stream.</p>\n<p>This possibility makes us reexamine all our old ideas about intelligence and creativity, since it means that, in principle, at least, we can make machines solve any problems whose solutions we can recognize. This is rarely practical, however. Consider that there must be a thousand ways to attach two boards, a million ways to connect three of them, and a billion ways to nail four boards together. It would take inconceivably long before the puzzle principle produced a workable bridge. But it does help, philosophically, to replace our feeling of mystery about creativity by more specific and concrete questions about the efficiency of processes. The main problem with our bridge-building machine is the lack of connection between its generator and its test. Without some notion of progress toward a goal, it is hard to do better than mindless chance.</p>",
    "text": "Many people reason that machines do only what they're programmed to do \u2014 and hence can never be creative or original. The trouble is that this argument presumes what it purports to show: that you can't program a machine to be creative! In fact, it is surprisingly easy to program a computer so that it will proceed to do more different things than any programmer could imagine in advance. This is possible because of what we'll call the puzzle principle.\nPuzzle Principle: We can program a computer to solve any problem by trial and error, without knowing how to solve it in advance, provided only that we have a way to recognize when the problem is solved.\nBy trial and error we mean programming the machine systematically to generate all possible structures within some universe of possibilities. For example, suppose you wished to have a robot machine that could build a bridge across a stream. The most efficient program for this would simply execute a specific procedure, planned out in advance, to precisely place some boards and nails. Of course, you couldn't write such a program unless you already knew how to build a bridge. But consider the alternative below, which is sometimes called the generate and test method. It consists of writing a two-part program.\nGenerate. The first process simply produces, one after another, every possible arrangement of the boards and nails. At first, you might expect such a program to be hard to write. But it turns out to be surprisingly easy, once you appreciate that there is no requirement for each arrangement to make any sense whatsoever! Test. The second part of the process examines each arrangement to see whether the problem has been solved. If the goal were to build a dam, the test is simply whether it holds back the stream. If the goal were to build a bridge, the test is simply whether it spans the stream.\nThis possibility makes us reexamine all our old ideas about intelligence and creativity, since it means that, in principle, at least, we can make machines solve any problems whose solutions we can recognize. This is rarely practical, however. Consider that there must be a thousand ways to attach two boards, a million ways to connect three of them, and a billion ways to nail four boards together. It would take inconceivably long before the puzzle principle produced a workable bridge. But it does help, philosophically, to replace our feeling of mystery about creativity by more specific and concrete questions about the efficiency of processes. The main problem with our bridge-building machine is the lack of connection between its generator and its test. Without some notion of progress toward a goal, it is hard to do better than mindless chance.",
    "type": "article",
    "title": "7.3 the puzzle principle",
    "tags": [
      {
        "score": 0.8383072018623352,
        "sentiment": 0.184,
        "count": 4,
        "label": "puzzle",
        "uri": "https://diffbot.com/entity/XXkUD7AHIPyWa0yFOt86fCA"
      },
      {
        "score": 0.6101826429367065,
        "sentiment": 0.582,
        "count": 3,
        "label": "Results May Vary",
        "uri": "https://diffbot.com/entity/XSoUItxxLObehi0BEgYXtvQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5297342538833618,
        "sentiment": 0.543,
        "count": 1,
        "label": "How to Solve It",
        "uri": "https://diffbot.com/entity/XTK3yCMwgMQGKGhHOuvCXpQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      }
    ],
    "docId": 238089028014,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 251559068075,
    "gburl": "http://aurellem.org/society-of-mind/som-7.3.html-diffbotxyz2763183514",
    "lastCrawlTimeUTC": 1588763055,
    "timestamp": "Wed, 06 May 2020 11:04:15 GMT"
  },
  {
    "date": "Thu, 26 Dec 2019 00:00:00 GMT",
    "humanLanguage": "en",
    "estimatedDate": "Thu, 26 Dec 2019 00:00:00 GMT",
    "diffbotUri": "article|3|-1493548926",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.12.html",
    "html": "<p>Every discourse works on several scales. Each word you hear can change your state in a way that depends upon all the structures you have built while listening to the words that came before. Most of those structures are themselves mere transient things, which persist for only a few moments before you rearrange some of their parts and perhaps discard the rest entirely. Thus, a car might first appear as the subject of a sentence, then become a mere vehicle or instrument in the next sentence; finally, the whole scenario might be used merely to modify a personal trait of some actor in a larger scene. As a discourse proceeds, details on each scale become absorbed into larger-scale representation networks whose outlines become increasingly remote from the individual words that were used to construct them.</p>\n<p>It would be wonderful to have a compact, self-contained theory that explains all our language-forms. But that ideal cannot be realized because words are merely the external signs of very complex processes, and there is no clear boundary between language and all the rest of what we call thinking. To be sure, the boundaries of words themselves are relatively clear, and when they have multiple meanings, our grammar-tactics can often help us to assign the proper senses to various terminals and other structures. These tactics include all sorts of inflections, prepositions, word orderings, and signals that indicate how to include one phrase inside another. We also combine words into larger expressions that range in vagueness of boundaries from compact clich&eacute;s like <em>hot dog</em> to diffuse signals that are scarcely linked to specific words at all; these include our hard-to-describe nuances of phrasing, rhythm, intonation, and shifts of style and flow.</p>\n<p>We're normally quite unaware of how our grammar-tactics constrain us in our choices of words. We're often somewhat more aware of other language-tactics we use to guide our listeners' minds &mdash; to change the focus from one theme to another, to adjust the levels of detail, to shift between foreground and setting. We learn to use phrases like <em>by the way</em> to change the topic of concern, to say <em>for example</em> to shift to a finer level of detail, to say <em>but</em> to modify an expectation or to interrupt the usual flow, or to say <em>in any case</em> or <em>in spite of that</em> to indicate the end of an interruption or elaboration.</p>\n<p>But even all this is only a small part of language. To understand what people say, we also exploit our vast stores of common knowledge, not only about how specific words are related to the subjects of concern, but also about how to express and discuss those subjects. Every human community evolves a great array of discourse-forms to shape its stories, explanations, conversations, discussions, and styles of argument. Just as we learn grammar-forms for fitting words to sentence-frames, we also build up stocks of <em>plots</em> to organize our story-tales, and standard personalities to fill the roles of their protagonists &mdash; and every child must learn these forms.</p>",
    "text": "Every discourse works on several scales. Each word you hear can change your state in a way that depends upon all the structures you have built while listening to the words that came before. Most of those structures are themselves mere transient things, which persist for only a few moments before you rearrange some of their parts and perhaps discard the rest entirely. Thus, a car might first appear as the subject of a sentence, then become a mere vehicle or instrument in the next sentence; finally, the whole scenario might be used merely to modify a personal trait of some actor in a larger scene. As a discourse proceeds, details on each scale become absorbed into larger-scale representation networks whose outlines become increasingly remote from the individual words that were used to construct them.\nIt would be wonderful to have a compact, self-contained theory that explains all our language-forms. But that ideal cannot be realized because words are merely the external signs of very complex processes, and there is no clear boundary between language and all the rest of what we call thinking. To be sure, the boundaries of words themselves are relatively clear, and when they have multiple meanings, our grammar-tactics can often help us to assign the proper senses to various terminals and other structures. These tactics include all sorts of inflections, prepositions, word orderings, and signals that indicate how to include one phrase inside another. We also combine words into larger expressions that range in vagueness of boundaries from compact clichés like hot dog to diffuse signals that are scarcely linked to specific words at all; these include our hard-to-describe nuances of phrasing, rhythm, intonation, and shifts of style and flow.\nWe're normally quite unaware of how our grammar-tactics constrain us in our choices of words. We're often somewhat more aware of other language-tactics we use to guide our listeners' minds \u2014 to change the focus from one theme to another, to adjust the levels of detail, to shift between foreground and setting. We learn to use phrases like by the way to change the topic of concern, to say for example to shift to a finer level of detail, to say but to modify an expectation or to interrupt the usual flow, or to say in any case or in spite of that to indicate the end of an interruption or elaboration.\nBut even all this is only a small part of language. To understand what people say, we also exploit our vast stores of common knowledge, not only about how specific words are related to the subjects of concern, but also about how to express and discuss those subjects. Every human community evolves a great array of discourse-forms to shape its stories, explanations, conversations, discussions, and styles of argument. Just as we learn grammar-forms for fitting words to sentence-frames, we also build up stocks of plots to organize our story-tales, and standard personalities to fill the roles of their protagonists \u2014 and every child must learn these forms.",
    "type": "article",
    "title": "26.12 coherent discourse",
    "docId": 42342941056,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 212271939990,
    "gburl": "http://aurellem.org/society-of-mind/som-26.12.html-diffbotxyz2056514295",
    "lastCrawlTimeUTC": 1588762935,
    "timestamp": "Wed, 06 May 2020 11:02:15 GMT"
  },
  {
    "sentiment": -0.819,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-2018408100",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-27.8.html",
    "html": "<p>Some readers might object that the censor-learning theory of jokes is too narrow to be an explanation of humor in general. What of all the other roles that humor plays in occasions of enjoyment and companionship? Our answer is the same as usual: we can't expect any single, simple theory to explain adult psychology. To ask how humor works in a grown-up person is to ask how everything works in a grown-up person, since humor gets involved with so many other things. I didn't mean to suggest that every aspect of humor is involved in making censors learn. When humor evolved, as when any other mechanism develops in biology, it must have been built upon other mechanisms that already existed, and embodied mixtures of those other functions. Just as the voice is used for many social purposes, the mechanisms involved in humor are also used for other effects that are less involved with memory. In later life the effect of <em>functional autonomy</em> can make it hard to recognize the original function not only of humor, but of many other aspects of adult psychology. To understand how feelings work, we need to understand both their evolutionary and their individual histories.</p>\n<p>We've seen how important it is for us to learn about mistakes. To keep from making old mistakes ourselves, we learn about them from our families and friends. But a peculiar problem arises when we tell another person that something is wrong, for if this is interpreted as an expression of disapproval and rejection, it can evoke a sense of pain and loss &mdash; and lead to withdrawal and avoidance. Accordingly, to point out mistakes to someone whose loyalty and love we want to keep, we must adopt some pleasant or conciliatory form. Thus humor has evolved its graciously disarming ways to do its basically distasteful job! You don't want the recipient to <em>kill the messenger who brings bad news</em> &mdash; especially when you're the messenger.</p>\n<p>Many people seem genuinely surprised when shown that humor is so concerned with unpleasant, painful, and disgusting subjects. In a certain sense, there's really nothing humorous about most jokes &mdash; except, perhaps, in the skill and subtlety with which their dreadful content is disguised; frequently, the thought itself is little more than <em>See what happened to somebody else; now, aren't you glad it wasn't you?</em> In this sense most jokes are not actually frivolous at all but reflect the most serious of concerns. Why, by the way, are jokes usually less funny when heard again? Because the censors learn some more each time and prepare to act more quickly and effectively.</p>\n<p>Why, then, do certain kinds of jokes, particularly those about forbidden sexual subjects, seem to remain persistently funny to so many people? Why do those censors remain unchanged for so long? Here we can reuse our explanation of the prolonged persistence of attachment, infatuation, sexuality, and mourning-grief; because these areas relate to self-ideals, their memories, once formed, are slow to change. Thus the peculiar robustness of sexual humor may mean only that the censors of human sexuality are among the <em>slow learners</em> of the mind, like retarded children. In fact, we could argue that they literally are retarded children &mdash; that is, they are among the frozen remnants of our earlier selves.</p>",
    "text": "Some readers might object that the censor-learning theory of jokes is too narrow to be an explanation of humor in general. What of all the other roles that humor plays in occasions of enjoyment and companionship? Our answer is the same as usual: we can't expect any single, simple theory to explain adult psychology. To ask how humor works in a grown-up person is to ask how everything works in a grown-up person, since humor gets involved with so many other things. I didn't mean to suggest that every aspect of humor is involved in making censors learn. When humor evolved, as when any other mechanism develops in biology, it must have been built upon other mechanisms that already existed, and embodied mixtures of those other functions. Just as the voice is used for many social purposes, the mechanisms involved in humor are also used for other effects that are less involved with memory. In later life the effect of functional autonomy can make it hard to recognize the original function not only of humor, but of many other aspects of adult psychology. To understand how feelings work, we need to understand both their evolutionary and their individual histories.\nWe've seen how important it is for us to learn about mistakes. To keep from making old mistakes ourselves, we learn about them from our families and friends. But a peculiar problem arises when we tell another person that something is wrong, for if this is interpreted as an expression of disapproval and rejection, it can evoke a sense of pain and loss \u2014 and lead to withdrawal and avoidance. Accordingly, to point out mistakes to someone whose loyalty and love we want to keep, we must adopt some pleasant or conciliatory form. Thus humor has evolved its graciously disarming ways to do its basically distasteful job! You don't want the recipient to kill the messenger who brings bad news \u2014 especially when you're the messenger.\nMany people seem genuinely surprised when shown that humor is so concerned with unpleasant, painful, and disgusting subjects. In a certain sense, there's really nothing humorous about most jokes \u2014 except, perhaps, in the skill and subtlety with which their dreadful content is disguised; frequently, the thought itself is little more than See what happened to somebody else; now, aren't you glad it wasn't you? In this sense most jokes are not actually frivolous at all but reflect the most serious of concerns. Why, by the way, are jokes usually less funny when heard again? Because the censors learn some more each time and prepare to act more quickly and effectively.\nWhy, then, do certain kinds of jokes, particularly those about forbidden sexual subjects, seem to remain persistently funny to so many people? Why do those censors remain unchanged for so long? Here we can reuse our explanation of the prolonged persistence of attachment, infatuation, sexuality, and mourning-grief; because these areas relate to self-ideals, their memories, once formed, are slow to change. Thus the peculiar robustness of sexual humor may mean only that the censors of human sexuality are among the slow learners of the mind, like retarded children. In fact, we could argue that they literally are retarded children \u2014 that is, they are among the frozen remnants of our earlier selves.",
    "type": "article",
    "title": "27.8 good humor",
    "tags": [
      {
        "score": 0.6314859986305237,
        "sentiment": 0.206,
        "count": 1,
        "label": "Good Humor",
        "uri": "https://diffbot.com/entity/XeHmYtlknPqmXtHQnfQx01w",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5964369773864746,
        "sentiment": -0.299,
        "count": 5,
        "label": "joke",
        "uri": "https://diffbot.com/entity/XvJyT8Od1M821ttxhfdz4SQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5539677143096924,
        "sentiment": 0,
        "count": 2,
        "label": "psychology",
        "uri": "https://diffbot.com/entity/XvmWcmJMyOt6Vha1szxrZXQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 122274103737,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 85879636380,
    "gburl": "http://aurellem.org/society-of-mind/som-27.8.html-diffbotxyz408902714",
    "lastCrawlTimeUTC": 1588762856,
    "timestamp": "Wed, 06 May 2020 11:00:56 GMT"
  },
  {
    "sentiment": 0.997,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1398384585",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-30.8.html",
    "html": "<p>How could anything as complex as a human mind work so well for so many years? We all appreciate those splendid feats of writing plays and symphonies. But we rarely recognize how wonderful it is that a person can traverse an entire lifetime without making a single really serious mistake &mdash; like putting a fork in one's eye or using a window instead of a door. How do we do such amazing feats as to imagine things we've never seen before, to overcome obstacles, to repair things that are broken, to speak to one another, to have new ideas? What magical trick makes us intelligent? The trick is that there is no trick. The power of intelligence stems from our vast diversity, not from any single, perfect principle. Our species has evolved many effective although imperfect methods, and each of us individually develops more on our own. Eventually, very few of our actions and decisions come to depend on any single mechanism. Instead, they emerge from conflicts and negotiations among societies of processes that constantly challenge one another. In this book we've seen many such dimensions of diversity:</p>\n<p>The accumulation of myriad subagents. We learn many different ways to achieve each kind of goal. The many realms of ordinary thought. When one viewpoint fails to solve a problem, we can adopt other perspectives. The endowment of several <em>instinctive</em> protominds. We embody different kinds of organizations for achieving many kinds of goals. The hierarchies of administration grown in accord with Papert's principle. When simple methods fail, we can build new levels of organization. The evolutionary vestiges of animals that still remain inside our brains. We use machinery evolved from fish, amphibia, reptiles, and earlier mammals. The sequence of stages of the growing child's personality. We accumulate different personalities that we can apply to</p>\n<p>different situations. The complex, ever-growing heritage of language and culture. We can use methods and ideas developed by millions of our ancestors. The subordination of thought processes to censors and suppressors. We do not need perfect methods, since we can remember how imperfect methods fail.</p>\n<p>Each of these dimensions gives you toughness and versatility. They offer alternative ways to proceed when any system fails. If part of your society of mind proposes to do what other parts find unacceptable, your agencies can usually find another way. Sometimes you merely need to turn to another branch of the same accumulation. When that fails, you can ascend to a higher level and engage a larger change in strategy. Then, even if an entire agency should fail, your brain retains earlier versions of it. This means that every facet of your personality may have the option to <em>regress</em> to an earlier stage, which already has proved itself competent to deal with the usual problems of life. Finally, when even that won't work, you can usually switch to an entirely different family of agencies. Whenever anything goes wrong, there are always other realms of thought.</p>",
    "text": "How could anything as complex as a human mind work so well for so many years? We all appreciate those splendid feats of writing plays and symphonies. But we rarely recognize how wonderful it is that a person can traverse an entire lifetime without making a single really serious mistake \u2014 like putting a fork in one's eye or using a window instead of a door. How do we do such amazing feats as to imagine things we've never seen before, to overcome obstacles, to repair things that are broken, to speak to one another, to have new ideas? What magical trick makes us intelligent? The trick is that there is no trick. The power of intelligence stems from our vast diversity, not from any single, perfect principle. Our species has evolved many effective although imperfect methods, and each of us individually develops more on our own. Eventually, very few of our actions and decisions come to depend on any single mechanism. Instead, they emerge from conflicts and negotiations among societies of processes that constantly challenge one another. In this book we've seen many such dimensions of diversity:\nThe accumulation of myriad subagents. We learn many different ways to achieve each kind of goal. The many realms of ordinary thought. When one viewpoint fails to solve a problem, we can adopt other perspectives. The endowment of several instinctive protominds. We embody different kinds of organizations for achieving many kinds of goals. The hierarchies of administration grown in accord with Papert's principle. When simple methods fail, we can build new levels of organization. The evolutionary vestiges of animals that still remain inside our brains. We use machinery evolved from fish, amphibia, reptiles, and earlier mammals. The sequence of stages of the growing child's personality. We accumulate different personalities that we can apply to\ndifferent situations. The complex, ever-growing heritage of language and culture. We can use methods and ideas developed by millions of our ancestors. The subordination of thought processes to censors and suppressors. We do not need perfect methods, since we can remember how imperfect methods fail.\nEach of these dimensions gives you toughness and versatility. They offer alternative ways to proceed when any system fails. If part of your society of mind proposes to do what other parts find unacceptable, your agencies can usually find another way. Sometimes you merely need to turn to another branch of the same accumulation. When that fails, you can ascend to a higher level and engage a larger change in strategy. Then, even if an entire agency should fail, your brain retains earlier versions of it. This means that every facet of your personality may have the option to regress to an earlier stage, which already has proved itself competent to deal with the usual problems of life. Finally, when even that won't work, you can usually switch to an entirely different family of agencies. Whenever anything goes wrong, there are always other realms of thought.",
    "type": "article",
    "title": "30.8 intelligence and resourcefulness",
    "tags": [
      {
        "score": 0.5328015685081482,
        "sentiment": 0.965,
        "count": 2,
        "label": "multiculturalism",
        "uri": "https://diffbot.com/entity/XNZVuR2zIOWCC8Ztc5GVLkA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Ideology",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 174256472477,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 207011299765,
    "gburl": "http://aurellem.org/society-of-mind/som-30.8.html-diffbotxyz3269749724",
    "lastCrawlTimeUTC": 1588762903,
    "timestamp": "Wed, 06 May 2020 11:01:43 GMT"
  },

  {
    "sentiment": 0.901,
    "images": [
      {
        "naturalHeight": 137,
        "width": 271,
        "diffbotUri": "image|3|256091136",
        "url": "http://aurellem.org/society-of-mind/illus/ch25/25-4.png",
        "naturalWidth": 271,
        "primary": true,
        "height": 137
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|492412746",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-25.2.html",
    "html": "<p>When we first discussed how Builder works, we assumed that it employed a vision-agent, See, to locate the various blocks it needs. However, we never discussed how See itself might work. A person simply <em>looks and sees</em> &mdash; but that's more complicated than it seems. For instance, even a simple cube looks different from each point of view, since as you move, the images it makes inside your eye keep changing in both shape and size.</p>\n<p>How strange and dangerous moving would be if every step made everything seem wholly new! But that's not how it seems to us. When we move to the right, so that A becomes invisible, we remember what we learned when we saw it, and it still seems part of what we're seeing now. How can this be? Here is a theory of why things seem to stay the same, even when what we see of them keeps changing as we move around.</p>\n<p>Frame-Arrays. When we move, our vision-systems switch among a family of different frames that all use the same terminals.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch25/25-4.png\"/></figure>\n<p>I'll use the term <em>frame-arrays</em> for these groups of frames that share the same terminals. When you represent a thing's details with a frame-array, you can continue to move around yet <em>keep in mind</em> all that you've observed from those different viewpoints, even though you've never seen them all at once. This gives us the wonderful ability to conceive of all of an object's different views as aspects of a single thing.</p>\n<p>I do not mean to suggest that every time you see a new object you build a brand-new frame-array for it. First, you try to match what you see to the frame-arrays in the memories you have accumulated and refined over periods of many years. How do frame-arrays originate? I would assume that this underlying pattern &mdash; of families of frames that all share common terminals &mdash; is built into the architectures of major sections of the brain. But although that pattern is <em>built in,</em> developing the skills for using it involves each child in more than a decade of predestined learning.</p>",
    "text": "When we first discussed how Builder works, we assumed that it employed a vision-agent, See, to locate the various blocks it needs. However, we never discussed how See itself might work. A person simply looks and sees \u2014 but that's more complicated than it seems. For instance, even a simple cube looks different from each point of view, since as you move, the images it makes inside your eye keep changing in both shape and size.\nHow strange and dangerous moving would be if every step made everything seem wholly new! But that's not how it seems to us. When we move to the right, so that A becomes invisible, we remember what we learned when we saw it, and it still seems part of what we're seeing now. How can this be? Here is a theory of why things seem to stay the same, even when what we see of them keeps changing as we move around.\nFrame-Arrays. When we move, our vision-systems switch among a family of different frames that all use the same terminals.\nI'll use the term frame-arrays for these groups of frames that share the same terminals. When you represent a thing's details with a frame-array, you can continue to move around yet keep in mind all that you've observed from those different viewpoints, even though you've never seen them all at once. This gives us the wonderful ability to conceive of all of an object's different views as aspects of a single thing.\nI do not mean to suggest that every time you see a new object you build a brand-new frame-array for it. First, you try to match what you see to the frame-arrays in the memories you have accumulated and refined over periods of many years. How do frame-arrays originate? I would assume that this underlying pattern \u2014 of families of frames that all share common terminals \u2014 is built into the architectures of major sections of the brain. But although that pattern is built in, developing the skills for using it involves each child in more than a decade of predestined learning.",
    "type": "article",
    "title": "25.2 frame-arrays",
    "docId": 243725713823,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 220792242614,
    "gburl": "http://aurellem.org/society-of-mind/som-25.2.html-diffbotxyz3860134130",
    "lastCrawlTimeUTC": 1588762834,
    "timestamp": "Wed, 06 May 2020 11:00:34 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1102409050",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-28.4.html",
    "html": "<p>It seems completely natural to us that we should feel pain when we're injured or hunger when we're deprived of food. Such feelings seem to us to be inherent in those predicaments. Then why doesn't a car feel pain when its tire is punctured or feel hungry when its fuel runs low? The answer is that pain and hunger are not inherent in being injured or starved: such feelings must be <em>engineered.</em> These physical circumstances do not directly produce the states of mind they arouse; on the contrary, this depends upon intricate networks of agencies and nerve-bundles that took millions of years to evolve. We have no conscious sense of that machinery. When your skin is touched, it seems as though it were your skin that feels &mdash; and not your brain &mdash; because you're unaware of everything that happens in between.</p>\n<p>In order for hunger to keep us fed, it must engage some agency that gives priority to food-acquiring goals. But unless such signals came before our fuel reserves were entirely gone, they'd arrive too late to have any use. This is why feeling hungry or tired is not the same as being genuinely starved or exhausted. To serve as useful <em>warning signs,</em> feelings like pain and hunger must be engineered not simply to indicate dangerous conditions, but to anticipate them and warn us before too much damage is done.</p>\n<p>But what about the feelings of depression and discouragement we get when stuck at boring jobs or with problems we cannot solve? Such feelings resemble those that accompany physical fatigue, but they do not signify genuine depletions because they often easily respond to changes of context, interest, and schedule. Nevertheless, the similarity would be no accident, for probably those feelings arise because our higher-level brain centers have evolved connections that exploit our ancient fuel-exhaustion warning systems. After all, the unproductive use of time is virtually equivalent to wasting hard- earned energy!</p>\n<p>Now what about those incidents in which some person seems to go beyond what we supposed were the normal bounds of endurance, strength, or tolerance of pain? We like to believe this demonstrates that the force of will can overrule the physical laws that govern the world. But a person's ability to persist in circumstances we hadn't thought were tolerable need not indicate anything supernatural. Since our feelings of pain, depression, exhaustion, and discouragement are themselves mere products of our minds' activities &mdash; and ones that are engineered to warn us before we reach our ultimate limits &mdash; we need no extraordinary power of mind over matter to overcome them. It is merely a matter of finding ways to rearrange our priorities.</p>\n<p>In any case, what hurts &mdash; and even what is <em>felt</em> at all &mdash; may, in the end, be more dependent upon culture than biology. Ask anyone who runs a marathon, or ask your favorite Amazon.</p>",
    "text": "It seems completely natural to us that we should feel pain when we're injured or hunger when we're deprived of food. Such feelings seem to us to be inherent in those predicaments. Then why doesn't a car feel pain when its tire is punctured or feel hungry when its fuel runs low? The answer is that pain and hunger are not inherent in being injured or starved: such feelings must be engineered. These physical circumstances do not directly produce the states of mind they arouse; on the contrary, this depends upon intricate networks of agencies and nerve-bundles that took millions of years to evolve. We have no conscious sense of that machinery. When your skin is touched, it seems as though it were your skin that feels \u2014 and not your brain \u2014 because you're unaware of everything that happens in between.\nIn order for hunger to keep us fed, it must engage some agency that gives priority to food-acquiring goals. But unless such signals came before our fuel reserves were entirely gone, they'd arrive too late to have any use. This is why feeling hungry or tired is not the same as being genuinely starved or exhausted. To serve as useful warning signs, feelings like pain and hunger must be engineered not simply to indicate dangerous conditions, but to anticipate them and warn us before too much damage is done.\nBut what about the feelings of depression and discouragement we get when stuck at boring jobs or with problems we cannot solve? Such feelings resemble those that accompany physical fatigue, but they do not signify genuine depletions because they often easily respond to changes of context, interest, and schedule. Nevertheless, the similarity would be no accident, for probably those feelings arise because our higher-level brain centers have evolved connections that exploit our ancient fuel-exhaustion warning systems. After all, the unproductive use of time is virtually equivalent to wasting hard- earned energy!\nNow what about those incidents in which some person seems to go beyond what we supposed were the normal bounds of endurance, strength, or tolerance of pain? We like to believe this demonstrates that the force of will can overrule the physical laws that govern the world. But a person's ability to persist in circumstances we hadn't thought were tolerable need not indicate anything supernatural. Since our feelings of pain, depression, exhaustion, and discouragement are themselves mere products of our minds' activities \u2014 and ones that are engineered to warn us before we reach our ultimate limits \u2014 we need no extraordinary power of mind over matter to overcome them. It is merely a matter of finding ways to rearrange our priorities.\nIn any case, what hurts \u2014 and even what is felt at all \u2014 may, in the end, be more dependent upon culture than biology. Ask anyone who runs a marathon, or ask your favorite Amazon.",
    "type": "article",
    "title": "28.4 mind over matter",
    "tags": [
      {
        "score": 0.7110418677330017,
        "sentiment": -0.731,
        "count": 4,
        "label": "hunger",
        "uri": "https://diffbot.com/entity/Xo7QCmaohNGCXAGXrhtotMA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill",
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject"
        ]
      }
    ],
    "docId": 57914737085,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 135954514304,
    "gburl": "http://aurellem.org/society-of-mind/som-28.4.html-diffbotxyz4057036507",
    "lastCrawlTimeUTC": 1588762706,
    "timestamp": "Wed, 06 May 2020 10:58:26 GMT"
  },
  {
    "sentiment": 0.361,
    "images": [
      {
        "naturalHeight": 155,
        "width": 367,
        "diffbotUri": "image|3|-1610512682",
        "url": "http://aurellem.org/society-of-mind/illus/ch8/8-8.png",
        "naturalWidth": 367,
        "primary": true,
        "height": 155
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-2062713881",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.9.html",
    "html": "<p>If each K-line can connect to other K-lines, which, in turn, connect to others, then K-lines can form societies. But how can we make sure that this can serve our purposes, instead of becoming a great, disordered mess? What could guide them into representing useful hierarchies like these?</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch8/8-8.png\"/></figure>\n<p>To keep things orderly, we'll now apply that level-band idea again. Remember that we first invented K-lines to link older agents together; then we invented level-bands to keep those K-lines from filling up with too much useless, unrelated stuff. Now we have the same problem again: when connecting new K-lines to old ones, we must keep them from including too much inappropriate detail. So why not try the same solution? Let's apply the level-band idea to the K-line trees themselves!</p>\n<p>When making a new K-line memory, do not connect it to all the K-lines active at the time but only to those that are active within a certain level-band.</p>\n<p>It might be supposed that this idea would be hard to apply unless we specify what <em>level</em> means. However, something like this will happen automatically, simply because the new K-line societies will tend to inherit whatever hierarchy already existed among the original agents that become connected to those K-lines. We've actually seen two different ideas about this. In our Kite example, we talked about a description's <em>level of detail.</em> That is, we regarded it as more elevated to talk about <em>a sheet stretched across a frame</em> than to discuss the paper or the sticks themselves. In our Builder example, we talked about goals and considered the Tower Builder agent itself to be a level above the agents it exploits to solve its subproblems &mdash; agents like Begin and Add and End.</p>\n<p>This policy of connecting new K-lines to old ones must be used in moderation. Otherwise, no new agents would ever be included in our memories. Furthermore, it should not always be required to produce simple, orderly hierarchy-trees; for example, in the case of Builder, we found that both Move and See will often need one another's help. Eventually, all of our knowledge-structures become entangled with various sorts of exceptions, shortcuts, and cross-connections. No matter: the level-band idea will still apply in general, since most of what we know will still be mainly hierarchical because of how our knowledge grows.</p>",
    "text": "If each K-line can connect to other K-lines, which, in turn, connect to others, then K-lines can form societies. But how can we make sure that this can serve our purposes, instead of becoming a great, disordered mess? What could guide them into representing useful hierarchies like these?\nTo keep things orderly, we'll now apply that level-band idea again. Remember that we first invented K-lines to link older agents together; then we invented level-bands to keep those K-lines from filling up with too much useless, unrelated stuff. Now we have the same problem again: when connecting new K-lines to old ones, we must keep them from including too much inappropriate detail. So why not try the same solution? Let's apply the level-band idea to the K-line trees themselves!\nWhen making a new K-line memory, do not connect it to all the K-lines active at the time but only to those that are active within a certain level-band.\nIt might be supposed that this idea would be hard to apply unless we specify what level means. However, something like this will happen automatically, simply because the new K-line societies will tend to inherit whatever hierarchy already existed among the original agents that become connected to those K-lines. We've actually seen two different ideas about this. In our Kite example, we talked about a description's level of detail. That is, we regarded it as more elevated to talk about a sheet stretched across a frame than to discuss the paper or the sticks themselves. In our Builder example, we talked about goals and considered the Tower Builder agent itself to be a level above the agents it exploits to solve its subproblems \u2014 agents like Begin and Add and End.\nThis policy of connecting new K-lines to old ones must be used in moderation. Otherwise, no new agents would ever be included in our memories. Furthermore, it should not always be required to produce simple, orderly hierarchy-trees; for example, in the case of Builder, we found that both Move and See will often need one another's help. Eventually, all of our knowledge-structures become entangled with various sorts of exceptions, shortcuts, and cross-connections. No matter: the level-band idea will still apply in general, since most of what we know will still be mainly hierarchical because of how our knowledge grows.",
    "type": "article",
    "title": "8.9 knowledge-trees",
    "tags": [
      {
        "score": 0.8007473349571228,
        "sentiment": 0,
        "count": 8,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.7824912071228027,
        "sentiment": 0.215,
        "count": 4,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      },
      {
        "score": 0.5644129514694214,
        "sentiment": 0,
        "count": 2,
        "label": "society",
        "uri": "https://diffbot.com/entity/X5-r2onDFMwqrJxIepxIeQw"
      }
    ],
    "docId": 94546543020,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 42133242297,
    "gburl": "http://aurellem.org/society-of-mind/som-8.9.html-diffbotxyz2598895220",
    "lastCrawlTimeUTC": 1588762729,
    "timestamp": "Wed, 06 May 2020 10:58:49 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|480452792",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.1.html",
    "html": "<p>We often find it hard to <em>express our thoughts</em> &mdash; to summarize our mental states or put our ideas into words. It is tempting to blame this on the ambiguity of words, but the problem is deeper than that.</p>\n<p>Thoughts themselves are ambiguous!</p>\n<p>At first, one might complain that that's impossible. <em>I'm thinking exactly what I'm thinking; there's no way it could be otherwise. And this has nothing to do with whether I can express it precisely.</em> But <em>what you're thinking now</em> is itself inherently ambiguous. If we interpret it to mean the states of all your agencies, that would include much that cannot be <em>expressed</em> simply because it is not accessible to your language-agency. A more modest interpretation of <em>what you're thinking now</em> would be a partial indication of the present states of some of your higher-level agencies. But the significance of any agency's state depends on how it is likely to affect the states of other agencies. This implies that in order to <em>express</em> your present state of mind, you have to partially anticipate what some of your agencies are about to do. Inevitably, by the time you've managed to express yourself, you're no longer in the state you were before; your thoughts were ambiguous to begin with, and you never did succeed in expressing them but merely replaced them with other thoughts.</p>\n<p>This is not just a matter of words. The problem is that our states of mind are usually subject to change. The properties of physical things tend to persist when their contexts are changed &mdash; but the <em>significance</em> of a thought, idea, or partial state of mind depends upon which other thoughts are active at the time and upon what eventually emerges from the conflicts and negotiations among one's agencies. It is an illusion to assume a clear and absolute distinction between <em>expressing</em> and <em>thinking,</em> since expressing is itself an active process that involves simplifying and reconstituting a mental state by detaching it from the more diffuse and variable parts of its context.</p>\n<p>The listener, too, must deal with ambiguity. You understand <em>I wrote a note to my sister,</em> despite the fact that the word <em>note</em> could mean a short letter or comment, a banknote, a musical sound, an observation, a distinction, or a notoriety. If all our separate words are ambiguous by themselves, why are sentences so clearly understood? Because the context of each separate word is sharpened by the other words, as well as by the context of the listener's recent past. We can tolerate the ambiguity of words because we are already so competent at coping with the ambiguity of thoughts.</p>",
    "text": "We often find it hard to express our thoughts \u2014 to summarize our mental states or put our ideas into words. It is tempting to blame this on the ambiguity of words, but the problem is deeper than that.\nThoughts themselves are ambiguous!\nAt first, one might complain that that's impossible. I'm thinking exactly what I'm thinking; there's no way it could be otherwise. And this has nothing to do with whether I can express it precisely. But what you're thinking now is itself inherently ambiguous. If we interpret it to mean the states of all your agencies, that would include much that cannot be expressed simply because it is not accessible to your language-agency. A more modest interpretation of what you're thinking now would be a partial indication of the present states of some of your higher-level agencies. But the significance of any agency's state depends on how it is likely to affect the states of other agencies. This implies that in order to express your present state of mind, you have to partially anticipate what some of your agencies are about to do. Inevitably, by the time you've managed to express yourself, you're no longer in the state you were before; your thoughts were ambiguous to begin with, and you never did succeed in expressing them but merely replaced them with other thoughts.\nThis is not just a matter of words. The problem is that our states of mind are usually subject to change. The properties of physical things tend to persist when their contexts are changed \u2014 but the significance of a thought, idea, or partial state of mind depends upon which other thoughts are active at the time and upon what eventually emerges from the conflicts and negotiations among one's agencies. It is an illusion to assume a clear and absolute distinction between expressing and thinking, since expressing is itself an active process that involves simplifying and reconstituting a mental state by detaching it from the more diffuse and variable parts of its context.\nThe listener, too, must deal with ambiguity. You understand I wrote a note to my sister, despite the fact that the word note could mean a short letter or comment, a banknote, a musical sound, an observation, a distinction, or a notoriety. If all our separate words are ambiguous by themselves, why are sentences so clearly understood? Because the context of each separate word is sharpened by the other words, as well as by the context of the listener's recent past. We can tolerate the ambiguity of words because we are already so competent at coping with the ambiguity of thoughts.",
    "type": "article",
    "title": "20.1 ambiguity",
    "docId": 100220174760,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 90889601447,
    "gburl": "http://aurellem.org/society-of-mind/som-20.1.html-diffbotxyz2614721071",
    "lastCrawlTimeUTC": 1588762802,
    "timestamp": "Wed, 06 May 2020 11:00:02 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1858426657",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-29.7.html",
    "html": "<p>We always try to use old memories to recollect how we solved problems in the past. But nothing's ever twice the same, so recollections rarely match. Then we must force our memories to fit &mdash; so we can see those different things as similar. To do this, we can either modify a memory or change how we represent the present scene. For example, suppose you need a hammer but can only find a stone. One way to turn that stone to your purposes would be to make it fit your memory of a hammer's appearance &mdash; for example, by making your description of the stone include an imaginary boundary that divides it into two parts, to serve as handle and as head. Another way would be to make your hammer frame accept the entire stone as a hammer without a handle. Either scheme will make the memory match the description, but both will lead to conflicts in other agencies.</p>\n<p>How hard it will be to make such a match depends both on which agents are now active in your mind and on the levels of their priorities &mdash; in short, upon the context already established. It will be easy for you to see two things as similar when you only need to change relatively weak attachments at the conceptual fringes of familiar things. But frequently the ease of comprehension will also depend upon how readily you can switch from one mental realm to another.</p>\n<p>Consider what must happen in our minds when poets speak about their loves in romantic, floral terms. We all have learned a certain common way to represent a woman's beauty in terms of flowers that are prone, alas, to fade. For centuries this formula has been established in our language and literature; however, at first it must have seemed bizarre. We cannot possibly match our descriptions of women and flowers if we insist on interpreting such phrases and poems <em>literally</em> &mdash; that is to say, <em>illiterately</em> &mdash; entirely within the physical realm of the appearance, composition, and behavior of a typical flower.</p>\n<p>To be sure, the colors, symmetries, and smells of flowers can certainly arouse the sorts of states we associate with things we've come to see as beautiful. But the more essential trick is in knowing how to turn entirely away from the physical realm and dwell instead upon the images and fantasies that flowers evoke in other spheres &mdash; such as the sense of a thing so sweet and innocent, so helpless and delicate, that it invites affection, nurture, and protection. Features like these must be made to fit the listener's private love ideal &mdash; only then can the metaphor match.</p>\n<p>This, Herrick's bitter verse defeats. By holding us so tightly to the usual frames for human shapes, he steers us into fantasies of vegetables with hands and feet.</p>",
    "text": "We always try to use old memories to recollect how we solved problems in the past. But nothing's ever twice the same, so recollections rarely match. Then we must force our memories to fit \u2014 so we can see those different things as similar. To do this, we can either modify a memory or change how we represent the present scene. For example, suppose you need a hammer but can only find a stone. One way to turn that stone to your purposes would be to make it fit your memory of a hammer's appearance \u2014 for example, by making your description of the stone include an imaginary boundary that divides it into two parts, to serve as handle and as head. Another way would be to make your hammer frame accept the entire stone as a hammer without a handle. Either scheme will make the memory match the description, but both will lead to conflicts in other agencies.\nHow hard it will be to make such a match depends both on which agents are now active in your mind and on the levels of their priorities \u2014 in short, upon the context already established. It will be easy for you to see two things as similar when you only need to change relatively weak attachments at the conceptual fringes of familiar things. But frequently the ease of comprehension will also depend upon how readily you can switch from one mental realm to another.\nConsider what must happen in our minds when poets speak about their loves in romantic, floral terms. We all have learned a certain common way to represent a woman's beauty in terms of flowers that are prone, alas, to fade. For centuries this formula has been established in our language and literature; however, at first it must have seemed bizarre. We cannot possibly match our descriptions of women and flowers if we insist on interpreting such phrases and poems literally \u2014 that is to say, illiterately \u2014 entirely within the physical realm of the appearance, composition, and behavior of a typical flower.\nTo be sure, the colors, symmetries, and smells of flowers can certainly arouse the sorts of states we associate with things we've come to see as beautiful. But the more essential trick is in knowing how to turn entirely away from the physical realm and dwell instead upon the images and fantasies that flowers evoke in other spheres \u2014 such as the sense of a thing so sweet and innocent, so helpless and delicate, that it invites affection, nurture, and protection. Features like these must be made to fit the listener's private love ideal \u2014 only then can the metaphor match.\nThis, Herrick's bitter verse defeats. By holding us so tightly to the usual frames for human shapes, he steers us into fantasies of vegetables with hands and feet.",
    "type": "article",
    "title": "29.7 likenesses and analogies",
    "docId": 116191789466,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 157904945577,
    "gburl": "http://aurellem.org/society-of-mind/som-29.7.html-diffbotxyz1966125101",
    "lastCrawlTimeUTC": 1588762756,
    "timestamp": "Wed, 06 May 2020 10:59:16 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|770813122",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.1.html",
    "html": "<p>To represent the action <em>Put the apple in the pail,</em> the Origin pronome must be assigned to an apple-neme, and the Destination pronome to a polyneme for pail. However, at another time, another process might need the Origin to represent a block and the Destination to represent a tower top. Each pronome must be assigned to different things at different times, and only for long enough to complete the task of the moment. In other words, a pronome is a type of short-term memory.</p>\n<p>This suggests a simple way to embody the idea of a pronome: each pronome is simply a <em>temporary K-line.</em> The basic difference, then, between a pronome and a K-line is that a pronome's connections are temporary rather than permanent. We can <em>assign</em> a pronome by temporarily connecting it to whichever currently active agents it reaches. Then, when we <em>activate</em> that pronome again, those same agents will be aroused. To make the Origin pronome represent an apple, first activate an apple-neme; this will arouse certain agents.</p>\n<p>Next, quickly <em>assign</em> the Origin pronome. Those agents will then become attached to that pronome and presumably remain attached until the pronome is reassigned.</p>\n<p>If we compare pronomes and polynemes from this point of view, we see that they are closely related.</p>\n<p>Polynemes are permanent K-lines. They are long-term memories. Pronomes are temporary K-lines. They are short-term memories.</p>\n<p>It is not yet known today how brains form long-term memories. One hypothesis would be that we don't really have temporary K-lines at all, but that after a pronome's K-line is used, it becomes permanent, and the pronome machinery gets connected to another, previously unused K-line. However this works, we know little about it except that it requires a substantial amount of time to form a permanent memory &mdash; a time on the order of half an hour. If there is any serious disturbance in that interval, no memory will be formed. There also is some evidence that we can form new long-term memories at rates on the order of no more than perhaps one every few seconds, but this is very imprecise because we have no good definition of what we mean by separate memories. In any case, this seems to suggest that we might have several hundred such processes going on at once.</p>\n<p>Why does the process take so long? Perhaps because it simply takes that long to synthesize chemicals used to make permanent connection bridges between agents. Perhaps most of that time is consumed in searching for an unused K-line agent, particularly for one that already has the required potential connections. Or perhaps the required connections could emerge from <em>distributed memories</em> like those we mentioned briefly in section 20.9.</p>",
    "text": "To represent the action Put the apple in the pail, the Origin pronome must be assigned to an apple-neme, and the Destination pronome to a polyneme for pail. However, at another time, another process might need the Origin to represent a block and the Destination to represent a tower top. Each pronome must be assigned to different things at different times, and only for long enough to complete the task of the moment. In other words, a pronome is a type of short-term memory.\nThis suggests a simple way to embody the idea of a pronome: each pronome is simply a temporary K-line. The basic difference, then, between a pronome and a K-line is that a pronome's connections are temporary rather than permanent. We can assign a pronome by temporarily connecting it to whichever currently active agents it reaches. Then, when we activate that pronome again, those same agents will be aroused. To make the Origin pronome represent an apple, first activate an apple-neme; this will arouse certain agents.\nNext, quickly assign the Origin pronome. Those agents will then become attached to that pronome and presumably remain attached until the pronome is reassigned.\nIf we compare pronomes and polynemes from this point of view, we see that they are closely related.\nPolynemes are permanent K-lines. They are long-term memories. Pronomes are temporary K-lines. They are short-term memories.\nIt is not yet known today how brains form long-term memories. One hypothesis would be that we don't really have temporary K-lines at all, but that after a pronome's K-line is used, it becomes permanent, and the pronome machinery gets connected to another, previously unused K-line. However this works, we know little about it except that it requires a substantial amount of time to form a permanent memory \u2014 a time on the order of half an hour. If there is any serious disturbance in that interval, no memory will be formed. There also is some evidence that we can form new long-term memories at rates on the order of no more than perhaps one every few seconds, but this is very imprecise because we have no good definition of what we mean by separate memories. In any case, this seems to suggest that we might have several hundred such processes going on at once.\nWhy does the process take so long? Perhaps because it simply takes that long to synthesize chemicals used to make permanent connection bridges between agents. Perhaps most of that time is consumed in searching for an unused K-line agent, particularly for one that already has the required potential connections. Or perhaps the required connections could emerge from distributed memories like those we mentioned briefly in section 20.9.",
    "type": "article",
    "title": "22.1 pronomes and polynemes",
    "tags": [
      {
        "score": 0.7590596675872803,
        "sentiment": 0.209,
        "count": 5,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      },
      {
        "score": 0.713290810585022,
        "sentiment": 0.272,
        "count": 3,
        "label": "Origin Systems",
        "uri": "https://diffbot.com/entity/Ccb_oHREFPvC2aSvyOr652A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.6084005832672119,
        "sentiment": 0.157,
        "count": 3,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5871763229370117,
        "sentiment": 0,
        "count": 2,
        "label": "apple",
        "uri": "https://diffbot.com/entity/XgrXdplpfMRWDa_VBAOd2oA"
      },
      {
        "score": 0.5765987038612366,
        "sentiment": -0.285,
        "count": 1,
        "label": "Destination",
        "uri": "https://diffbot.com/entity/X3ku3PmMfMl2oaH-0f2-_Fg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5573928952217102,
        "sentiment": 0,
        "count": 1,
        "label": "Fly Me to the Moon",
        "uri": "https://diffbot.com/entity/XRZrQ-OahMnaptNVys6VVhw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5433865785598755,
        "sentiment": 0.645,
        "count": 4,
        "label": "temporary work",
        "uri": "https://diffbot.com/entity/X0Ooi5RBjNpS7zNFMtbPc4w"
      }
    ],
    "docId": 248348901780,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 55518429617,
    "gburl": "http://aurellem.org/society-of-mind/som-22.1.html-diffbotxyz3876717588",
    "lastCrawlTimeUTC": 1588762783,
    "timestamp": "Wed, 06 May 2020 10:59:43 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|364887311",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.2.html",
    "html": "<h2><em>12.2</em> learning meaning</h2>\n<p>What is learning, anyway? That word is certainly hard to define. The child in our Block-Arch scenario has found one way to learn one sense of what some adults mean by <em>arch.</em> But we can't assume that the same kinds of processes are involved when we learn to recite a poem, to use a spoon, and to tie a shoe. What happens when a person learns to read, learns to add numbers, learns a new language, learns to anticipate the dispositions of a friend, or learns to build a tower that will stand? If we tried to find a single definition for <em>learning</em> to span so many kinds of processes, we'd end up with some phrase too broad to have much use &mdash; like this:</p>\n<p><em>Learning</em> is making useful changes in the workings of our minds.</p>\n<p>The problem is that we use the single word <em>learning</em> to cover too diverse a society of ideas. Such a word can be useful in the title of a book, or in the name of an institution. But when it comes to studying the subject itself, we need more distinctive terms for important, different ways to learn. Even that one Block-Arch scene reveals at least four different ways to learn. We'll give them these new names:</p>\n<p>Uniframing combining several descriptions into one, for example, by observing that all the arches have certain common parts.</p>\n<p>Accumulating collecting incompatible descriptions, for example, by forming the phrase <em>block or wedge.</em></p>\n<p>Reformulating modifying a description's character, for example, by describing the separate blocks rather than the overall shape.</p>\n<p>Trans-framing bridging between structures and functions or actions, for example, by relating the concept of arch to the act of changing hands.</p>\n<p>These words will be explained in the sections that follow. It seems to me that the older words used in psychology &mdash; such as generalizing, practicing, conditioning, memorizing, or associating &mdash; are either too vague to be useful or have become connected to theories that simply aren't sound. In the meantime, the revolutions of computer science and Artificial Intelligence have led to new ideas about how various kinds of learning might work, and these new ideas deserve new names.</p>\n<p>Our Block-Arch scenario is based on a computer program developed by Patrick Winston in 1970. Winston's program required an external teacher to provide the examples and to say which of them were arches and which were not. In my unprogrammed version of this, the teacher has been replaced by the concern of some agency inside the child to account for the emergence of that mysterious Hand-Change phenomenon: why do certain structures force you to let go of the toy car, while other structures don't? We thus assume that the child is led to learn for itself in order to account for strange events. One might complain that it only makes learning harder to explain, to make it depend upon the child's curiosity. But if we are ever really to understand how our minds grow, we must first face reality: people just don't learn so well unless they're interested or concerned. The older theories of learning and remembering never got very far because in trying to oversimplify, they lost essential aspects of the context. It wouldn't be much use to have a theory in which knowledge is somehow stored away &mdash; without a corresponding theory of how later to put that knowledge back to work.</p>",
    "text": "12.2 learning meaning\nWhat is learning, anyway? That word is certainly hard to define. The child in our Block-Arch scenario has found one way to learn one sense of what some adults mean by arch. But we can't assume that the same kinds of processes are involved when we learn to recite a poem, to use a spoon, and to tie a shoe. What happens when a person learns to read, learns to add numbers, learns a new language, learns to anticipate the dispositions of a friend, or learns to build a tower that will stand? If we tried to find a single definition for learning to span so many kinds of processes, we'd end up with some phrase too broad to have much use \u2014 like this:\nLearning is making useful changes in the workings of our minds.\nThe problem is that we use the single word learning to cover too diverse a society of ideas. Such a word can be useful in the title of a book, or in the name of an institution. But when it comes to studying the subject itself, we need more distinctive terms for important, different ways to learn. Even that one Block-Arch scene reveals at least four different ways to learn. We'll give them these new names:\nUniframing combining several descriptions into one, for example, by observing that all the arches have certain common parts.\nAccumulating collecting incompatible descriptions, for example, by forming the phrase block or wedge.\nReformulating modifying a description's character, for example, by describing the separate blocks rather than the overall shape.\nTrans-framing bridging between structures and functions or actions, for example, by relating the concept of arch to the act of changing hands.\nThese words will be explained in the sections that follow. It seems to me that the older words used in psychology \u2014 such as generalizing, practicing, conditioning, memorizing, or associating \u2014 are either too vague to be useful or have become connected to theories that simply aren't sound. In the meantime, the revolutions of computer science and Artificial Intelligence have led to new ideas about how various kinds of learning might work, and these new ideas deserve new names.\nOur Block-Arch scenario is based on a computer program developed by Patrick Winston in 1970. Winston's program required an external teacher to provide the examples and to say which of them were arches and which were not. In my unprogrammed version of this, the teacher has been replaced by the concern of some agency inside the child to account for the emergence of that mysterious Hand-Change phenomenon: why do certain structures force you to let go of the toy car, while other structures don't? We thus assume that the child is led to learn for itself in order to account for strange events. One might complain that it only makes learning harder to explain, to make it depend upon the child's curiosity. But if we are ever really to understand how our minds grow, we must first face reality: people just don't learn so well unless they're interested or concerned. The older theories of learning and remembering never got very far because in trying to oversimplify, they lost essential aspects of the context. It wouldn't be much use to have a theory in which knowledge is somehow stored away \u2014 without a corresponding theory of how later to put that knowledge back to work.",
    "type": "article",
    "title": "12.2 learning meaning",
    "docId": 31985369491,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 158622286241,
    "gburl": "http://aurellem.org/society-of-mind/som-12.2.html-diffbotxyz3215890409",
    "lastCrawlTimeUTC": 1588762663,
    "timestamp": "Wed, 06 May 2020 10:57:43 GMT"
  },
  {
    "sentiment": -0.643,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1247807096",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-10.2.html",
    "html": "<p>What do those egg and water jar experiments say about our growth from infancy? Let's consider several explanations.</p>\n<p>QUANTITY: Perhaps the younger children simply don't yet understand the basic concept of quantity: that the amount of liquid remains the same.</p>\n<p>In the next few sections I'll argue that we don not learn one single, underlying <em>concept of quantity.</em> Instead, each person must construct a multileveled agency, which we'll call the Society-of-More, that finds different ways to deal with quantities.</p>\n<p>EXTENT: The younger children seem unduly influenced by the larger extent of space taken up by the spread-out eggs and taller water column.</p>\n<p>That cannot be the whole story because most adults, too, judge that there's more water in the taller jar &mdash; if they merely see the final scene, without knowing from where the water was poured! Here are a few other theories about the younger child's judgment:</p>\n<p>REVERSIBILITY: The older children pay more attention to what they think remains the same &mdash; while younger ones are more concerned with what has changed. CONFINEMENT: An older child knows that the amount of water stays the same, if none was ever added or removed or lost or spilled.</p>\n<p>LOGIC: Perhaps younger children have not yet learned to apply the kinds of reasoning that one would need to understand the concept of quantity.</p>\n<p>Every one of these explanations has some truth in it, but none reach the heart of the issue. It is clear that the older children know more about such matters and can do more complex kinds of reasoning. But there is ample evidence that most younger children also possess enough of the required abilities. For example, we can describe the experiment without actually doing it at all or we can perform it out of the child's sight, behind a cardboard screen. Then, when we explain what is happening, quite a few of the younger children will say, <em>Of course they'll be the same.</em></p>\n<p>Then what is the difficulty? Evidently, the younger children possess the ideas they need but don't know when to apply them! One might say that they lack adequate knowledge about their knowledge, or that they have not acquired the checks and balances required to select or override their hordes of agents with different perceptions and priorities. It is not enough to be able to use many kinds of reasoning;</p>\n<p>one also must know which to use in different circumstances! Learning is more than the mere accumulation of skills. Whatever we learn, there is always more to learn &mdash; about how to use what was already learned.</p>",
    "text": "What do those egg and water jar experiments say about our growth from infancy? Let's consider several explanations.\nQUANTITY: Perhaps the younger children simply don't yet understand the basic concept of quantity: that the amount of liquid remains the same.\nIn the next few sections I'll argue that we don not learn one single, underlying concept of quantity. Instead, each person must construct a multileveled agency, which we'll call the Society-of-More, that finds different ways to deal with quantities.\nEXTENT: The younger children seem unduly influenced by the larger extent of space taken up by the spread-out eggs and taller water column.\nThat cannot be the whole story because most adults, too, judge that there's more water in the taller jar \u2014 if they merely see the final scene, without knowing from where the water was poured! Here are a few other theories about the younger child's judgment:\nREVERSIBILITY: The older children pay more attention to what they think remains the same \u2014 while younger ones are more concerned with what has changed. CONFINEMENT: An older child knows that the amount of water stays the same, if none was ever added or removed or lost or spilled.\nLOGIC: Perhaps younger children have not yet learned to apply the kinds of reasoning that one would need to understand the concept of quantity.\nEvery one of these explanations has some truth in it, but none reach the heart of the issue. It is clear that the older children know more about such matters and can do more complex kinds of reasoning. But there is ample evidence that most younger children also possess enough of the required abilities. For example, we can describe the experiment without actually doing it at all or we can perform it out of the child's sight, behind a cardboard screen. Then, when we explain what is happening, quite a few of the younger children will say, Of course they'll be the same.\nThen what is the difficulty? Evidently, the younger children possess the ideas they need but don't know when to apply them! One might say that they lack adequate knowledge about their knowledge, or that they have not acquired the checks and balances required to select or override their hordes of agents with different perceptions and priorities. It is not enough to be able to use many kinds of reasoning;\none also must know which to use in different circumstances! Learning is more than the mere accumulation of skills. Whatever we learn, there is always more to learn \u2014 about how to use what was already learned.",
    "type": "article",
    "title": "10.2 reasoning about amounts",
    "tags": [
      {
        "score": 0.7254915833473206,
        "sentiment": 0,
        "count": 4,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      },
      {
        "score": 0.6125932335853577,
        "sentiment": -0.691,
        "count": 10,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5767315030097961,
        "sentiment": -0.77,
        "count": 5,
        "label": "water",
        "uri": "https://diffbot.com/entity/XZkGOXW6APzmZ3qAuzoh-aw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/ChemicalSubstance",
          "http://dbpedia.org/ontology/ChemicalCompound",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 91888632220,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 262713983415,
    "gburl": "http://aurellem.org/society-of-mind/som-10.2.html-diffbotxyz1658000490",
    "lastCrawlTimeUTC": 1588762685,
    "timestamp": "Wed, 06 May 2020 10:58:05 GMT"
  },
  {
    "sentiment": -0.739,
    "humanLanguage": "en",
    "diffbotUri": "article|3|37701953",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.1.html",
    "html": "<p>We normally assume that consciousness is knowing what happens in our minds right at the present time. In the next few sections, I'll argue that consciousness does not concern the present, but the past: it has to do with how we think about the records of our recent thoughts. But how can thinking about thoughts be possible at all?</p>\n<p>There's something queer about describing consciousness: whatever people mean to say, they just can't seem to make it clear. It's not like feeling confused or ignorant. Instead, we feel we know what's going on but can't describe it properly. How could anything seem so close, yet always keep beyond our reach?</p>\n<p>There is a simple sense in which thinking about a thought is not so different from thinking about an ordinary thing. We know that certain agencies must learn to recognize &mdash; and even name &mdash; the feel of touching a hand or an ear. Similarly, there must be other agencies that learn to recognize events inside the brain &mdash; for example, the activities of the agencies that manage memories. And those, I claim, are the bases of the awarenesses we recognize as consciousness.</p>\n<p>There is nothing peculiar about the idea of sensing events inside the brain. Agents are agents &mdash; and it is as easy for an agent to be wired to detect a brain-caused brain-event, as to detect a world-caused brain-event. Indeed, only a small minority of our agents are connected directly to sensors in the outer world, like those that send signals from the eye or skin; most of the agents in the brain detect events inside the brain. But here we're especially concerned with agents that are engaged in using and changing our most recent memories. These lie at the roots of consciousness.</p>\n<p>Why, for example, do we become less conscious of some things when we become more conscious of others? Surely this is because some resource is approaching some limitation &mdash; and I'll argue that it is our limited capacity to keep good records of our recent thoughts. Why, for example, do thoughts so often seem to flow in serial streams? It is because whenever we run out of room, the records of our recent thoughts must then displace those of our older ones. And why are we so unaware of how we get our new ideas? Because whenever we solve hard problems, our short-term memories become so involved with doing that that they have neither time nor space for keeping detailed records of what they themselves have done.</p>\n<p>What happens when we try to think about our most recent thoughts? We examine our recent memories. But these were already involved in what we were <em>thinking</em> about &mdash; and any self-inspecting probe is prone to change just what it's looking at. Then the system is likely to break down. It is hard enough to describe something with a stable shape; it is even harder to describe something that changes its shape before our eyes; and it is virtually impossible to speak of the shapes of things that change into something else each time we try to think of them. And that's what happens when we try to think about our present thoughts &mdash; since each such thought must change our mental state! Would any process not become confused that alters what it's looking at? In such a fix, how could one ever hope to be articulate?</p>",
    "text": "We normally assume that consciousness is knowing what happens in our minds right at the present time. In the next few sections, I'll argue that consciousness does not concern the present, but the past: it has to do with how we think about the records of our recent thoughts. But how can thinking about thoughts be possible at all?\nThere's something queer about describing consciousness: whatever people mean to say, they just can't seem to make it clear. It's not like feeling confused or ignorant. Instead, we feel we know what's going on but can't describe it properly. How could anything seem so close, yet always keep beyond our reach?\nThere is a simple sense in which thinking about a thought is not so different from thinking about an ordinary thing. We know that certain agencies must learn to recognize \u2014 and even name \u2014 the feel of touching a hand or an ear. Similarly, there must be other agencies that learn to recognize events inside the brain \u2014 for example, the activities of the agencies that manage memories. And those, I claim, are the bases of the awarenesses we recognize as consciousness.\nThere is nothing peculiar about the idea of sensing events inside the brain. Agents are agents \u2014 and it is as easy for an agent to be wired to detect a brain-caused brain-event, as to detect a world-caused brain-event. Indeed, only a small minority of our agents are connected directly to sensors in the outer world, like those that send signals from the eye or skin; most of the agents in the brain detect events inside the brain. But here we're especially concerned with agents that are engaged in using and changing our most recent memories. These lie at the roots of consciousness.\nWhy, for example, do we become less conscious of some things when we become more conscious of others? Surely this is because some resource is approaching some limitation \u2014 and I'll argue that it is our limited capacity to keep good records of our recent thoughts. Why, for example, do thoughts so often seem to flow in serial streams? It is because whenever we run out of room, the records of our recent thoughts must then displace those of our older ones. And why are we so unaware of how we get our new ideas? Because whenever we solve hard problems, our short-term memories become so involved with doing that that they have neither time nor space for keeping detailed records of what they themselves have done.\nWhat happens when we try to think about our most recent thoughts? We examine our recent memories. But these were already involved in what we were thinking about \u2014 and any self-inspecting probe is prone to change just what it's looking at. Then the system is likely to break down. It is hard enough to describe something with a stable shape; it is even harder to describe something that changes its shape before our eyes; and it is virtually impossible to speak of the shapes of things that change into something else each time we try to think of them. And that's what happens when we try to think about our present thoughts \u2014 since each such thought must change our mental state! Would any process not become confused that alters what it's looking at? In such a fix, how could one ever hope to be articulate?",
    "type": "article",
    "title": "15.1 momentary mental state",
    "tags": [
      {
        "score": 0.5723080635070801,
        "sentiment": 0.785,
        "count": 3,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5655295848846436,
        "sentiment": 0,
        "count": 1,
        "label": "I'll",
        "uri": "https://diffbot.com/entity/XKpTgx9VLPNSLDS43t-KlVg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/Comic",
          "http://dbpedia.org/ontology/Manga"
        ]
      },
      {
        "score": 0.5470897555351257,
        "sentiment": 0.309,
        "count": 1,
        "label": "Cozy Tapes Vol. 2",
        "uri": "https://diffbot.com/entity/XZnxl3DuAMbOAAlRkBXnVYg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5342638492584229,
        "sentiment": 0.821,
        "count": 2,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 156556509603,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 117771223462,
    "gburl": "http://aurellem.org/society-of-mind/som-15.1.html-diffbotxyz3477567413",
    "lastCrawlTimeUTC": 1588762595,
    "timestamp": "Wed, 06 May 2020 10:56:35 GMT"
  },
  {
    "sentiment": 0.98,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1447766514",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-4.8.html",
    "html": "<p>We usually reserve the word <em>ideals</em> to refer to how we think we ought to conduct our ethical affairs. But I'll use the term in a broader sense, to include the standards we maintain &mdash; consciously or otherwise &mdash; for how we ought to think about ordinary matters.</p>\n<p>We're always involved with goals of varying spans and scales. What happens when a transient inclination clashes with a long-term self-ideal? What happens, for that matter, when our ideals disagree among themselves, as when there is an inconsistency between the things we want to do and those we feel we ought to do? These disparities give rise to feelings of discomfort, guilt, and shame. To lessen such disturbances, we must either change the things we do &mdash; or change the ways we feel. Which should we try to modify &mdash; our immediate wants or our ideals? Such conflicts must be settled by the multilayered agencies that are formed in the early years of the growth of our personalities.</p>\n<p>In childhood, our agencies acquire various types of goals. Then we grow in overlapping waves, in which our older agencies affect the making of the new. This way, the older agencies can influence how our later ones will behave. Outside the individual, similar processes go on in every human community; we find children <em>taking after</em> persons other than themselves by absorbing values from their parents, families, and peers, even from the heroes and villains of mythology.</p>\n<p>Without enduring self-ideals, our lives would lack coherence. As individuals, we'd never be able to trust ourselves to carry out our personal plans. In a social group, no one person would be able to trust the others. A working society must evolve mechanisms that stabilize ideals &mdash; and many of the social principles that each of us regards as personal are really <em>long-term memories</em> in which our cultures store what they have learned across the centuries.</p>",
    "text": "We usually reserve the word ideals to refer to how we think we ought to conduct our ethical affairs. But I'll use the term in a broader sense, to include the standards we maintain \u2014 consciously or otherwise \u2014 for how we ought to think about ordinary matters.\nWe're always involved with goals of varying spans and scales. What happens when a transient inclination clashes with a long-term self-ideal? What happens, for that matter, when our ideals disagree among themselves, as when there is an inconsistency between the things we want to do and those we feel we ought to do? These disparities give rise to feelings of discomfort, guilt, and shame. To lessen such disturbances, we must either change the things we do \u2014 or change the ways we feel. Which should we try to modify \u2014 our immediate wants or our ideals? Such conflicts must be settled by the multilayered agencies that are formed in the early years of the growth of our personalities.\nIn childhood, our agencies acquire various types of goals. Then we grow in overlapping waves, in which our older agencies affect the making of the new. This way, the older agencies can influence how our later ones will behave. Outside the individual, similar processes go on in every human community; we find children taking after persons other than themselves by absorbing values from their parents, families, and peers, even from the heroes and villains of mythology.\nWithout enduring self-ideals, our lives would lack coherence. As individuals, we'd never be able to trust ourselves to carry out our personal plans. In a social group, no one person would be able to trust the others. A working society must evolve mechanisms that stabilize ideals \u2014 and many of the social principles that each of us regards as personal are really long-term memories in which our cultures store what they have learned across the centuries.",
    "type": "article",
    "title": "4.8 ideals",
    "tags": [
      {
        "score": 0.5657626986503601,
        "sentiment": 0.721,
        "count": 1,
        "label": "I'll",
        "uri": "https://diffbot.com/entity/XKpTgx9VLPNSLDS43t-KlVg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/Comic",
          "http://dbpedia.org/ontology/Manga"
        ]
      }
    ],
    "docId": 167247118730,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 206077559177,
    "gburl": "http://aurellem.org/society-of-mind/som-4.8.html-diffbotxyz1485869258",
    "lastCrawlTimeUTC": 1588762629,
    "timestamp": "Wed, 06 May 2020 10:57:09 GMT"
  },
  {
    "sentiment": -0.161,
    "images": [
      {
        "naturalHeight": 130,
        "width": 276,
        "diffbotUri": "image|3|1005839941",
        "url": "http://aurellem.org/society-of-mind/illus/ch16/16-9.png",
        "naturalWidth": 276,
        "primary": true,
        "height": 130
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-131546252",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-16.7.html",
    "html": "<p>How could any specialist cooperate when it doesn't understand how the others work? We manage to do our worldly work despite that same predicament; we deal with people and machines without knowing how their insides work. It's just the same inside the head; each part of the mind exploits the rest, not knowing how the other parts work but only what they seem to do.</p>\n<p>Suppose Thirst knows that water can be found in cups &mdash; but does not know how to find or reach for a cup; these are things only Find and Get can do. Then Thirst must have some way to exploit the abilities of those other agents. Builder, too, has a similar problem because most of its subagents cannot communicate directly with one another. It would be easy for Thirst or Builder simply to turn on other agents like Find and Get. But how will those subordinates know what to find or get? Must Thirst transmit to Find a picture of a cup? Must Builder send a picture of a brick? The trouble is that neither Builder nor Thirst is the sort of agent to contain the kind of knowledge required by Find &mdash; namely, the visual appearances of things. That kind of knowledge lies inside the memory-machinery of See. However, Thirst can achieve its drinking goal by activating two connections: one to cause See to <em>hallucinate</em> a cup and another connection to activate Find. Find itself can activate Get later. This should suffice for Thirst to locate and obtain a cup &mdash; if there is one in sight.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch16/16-9.png\"/></figure>\n<p>This scheme could be unreliable. If See became concerned with another object at that moment, Get would acquire the wrong object. Infants often disappoint themselves this way. Still, this scheme has the kind of simplicity one needs when starting to build any larger skill:</p>\n<p>one needs a process that sometimes works before one can proceed to improve it.</p>\n<p>This is merely a sketch of how to build an automatic <em>getting machine.</em> We'll return to this idea much later, when we discuss language, because what Thirst and Builder have to do resembles what people do when using words. When you say to another person, <em>Please pass the cup,</em> you don't emit a picture of a cup but merely send a signal that exploits the other person's memory.</p>\n<p>Achieving a goal by exploiting the abilities of other agencies might seem a shabby substitute for knowing how to do the work oneself. Yet this is the very source of the power of societies. No higher-level agency could ever achieve a complex goal if it had to be concerned with every small detail of what each nerve and muscle does. Unless most of its work were done by other agencies, no part of a society could do anything significant.</p>",
    "text": "How could any specialist cooperate when it doesn't understand how the others work? We manage to do our worldly work despite that same predicament; we deal with people and machines without knowing how their insides work. It's just the same inside the head; each part of the mind exploits the rest, not knowing how the other parts work but only what they seem to do.\nSuppose Thirst knows that water can be found in cups \u2014 but does not know how to find or reach for a cup; these are things only Find and Get can do. Then Thirst must have some way to exploit the abilities of those other agents. Builder, too, has a similar problem because most of its subagents cannot communicate directly with one another. It would be easy for Thirst or Builder simply to turn on other agents like Find and Get. But how will those subordinates know what to find or get? Must Thirst transmit to Find a picture of a cup? Must Builder send a picture of a brick? The trouble is that neither Builder nor Thirst is the sort of agent to contain the kind of knowledge required by Find \u2014 namely, the visual appearances of things. That kind of knowledge lies inside the memory-machinery of See. However, Thirst can achieve its drinking goal by activating two connections: one to cause See to hallucinate a cup and another connection to activate Find. Find itself can activate Get later. This should suffice for Thirst to locate and obtain a cup \u2014 if there is one in sight.\nThis scheme could be unreliable. If See became concerned with another object at that moment, Get would acquire the wrong object. Infants often disappoint themselves this way. Still, this scheme has the kind of simplicity one needs when starting to build any larger skill:\none needs a process that sometimes works before one can proceed to improve it.\nThis is merely a sketch of how to build an automatic getting machine. We'll return to this idea much later, when we discuss language, because what Thirst and Builder have to do resembles what people do when using words. When you say to another person, Please pass the cup, you don't emit a picture of a cup but merely send a signal that exploits the other person's memory.\nAchieving a goal by exploiting the abilities of other agencies might seem a shabby substitute for knowing how to do the work oneself. Yet this is the very source of the power of societies. No higher-level agency could ever achieve a complex goal if it had to be concerned with every small detail of what each nerve and muscle does. Unless most of its work were done by other agencies, no part of a society could do anything significant.",
    "type": "article",
    "title": "16.7 exploitation",
    "tags": [
      {
        "score": 0.7219439148902893,
        "sentiment": -0.339,
        "count": 4,
        "label": "construction worker",
        "uri": "https://diffbot.com/entity/XelVgOc5QN4uPgw8FXzWHiA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.688819944858551,
        "sentiment": 0.422,
        "count": 4,
        "label": "Thirst",
        "uri": "https://diffbot.com/entity/X0lItTEtCNHaCPw6VMDZDeg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      },
      {
        "score": 0.6659790277481079,
        "sentiment": -0.277,
        "count": 1,
        "label": "exploitation film",
        "uri": "https://diffbot.com/entity/XcnyrCdAGMNOvLGiy4lKTAA",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      }
    ],
    "docId": 220182233498,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 153119031693,
    "gburl": "http://aurellem.org/society-of-mind/som-16.7.html-diffbotxyz1288457745",
    "lastCrawlTimeUTC": 1588762565,
    "timestamp": "Wed, 06 May 2020 10:56:05 GMT"
  },
  {
    "sentiment": 0.163,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-790473928",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-30.4.html",
    "html": "<p>Now let's look at Mary's model of the world. (By <em>world</em> I mean the universe, not just the planet Earth.) This is simply all the structures in Mary's head that Mary's agencies can use to answer questions about things in the world.</p>\n<p>But what if we were to ask Mary a question not about any particular object, but one like <em>What sort of thing is the world itself?</em> That would put Mary in a curious predicament. She cannot answer this by using her world model, because each part of that model is designed only to answer questions about particular things. The trouble is that the world itself is not a particular thing inside the world.</p>\n<p>One way to deal with this (and a method that, surely, many children use) is adding to the model of the world an additional <em>object</em> &mdash; that represents the world itself. Then, since any object must have properties, the child might then assign to this the features, say, of an extremely large ball. Naturally this will lead to trouble should that child persist in asking ordinary questions about this extraordinary object &mdash; such as <em>What keeps the universe in place?</em> or <em>What's outside the universe?</em> &mdash; for these then lead to strange and inconsistent images. Eventually we learn some ways to deal with this &mdash; for example, by learning which questions to suppress. But as in the case of a perfect point, we may always feel uncomfortable with the thought of a thing that is unimaginably large in size, but has no shape at all.</p>\n<p>When you get right down to it, you can never really describe any worldly thing, either &mdash; that is, in any absolute sense. Whatever you purport to say about a thing, you're only expressing your own beliefs. Yet even that gloomy thought suggests an insight. Even if our models of the world cannot yield good answers about the world as a whole, and even though their other answers are frequently wrong, they can tell us something about ourselves. We can regard what we learn about our models of the world as constituting our models of our models of the world.</p>",
    "text": "Now let's look at Mary's model of the world. (By world I mean the universe, not just the planet Earth.) This is simply all the structures in Mary's head that Mary's agencies can use to answer questions about things in the world.\nBut what if we were to ask Mary a question not about any particular object, but one like What sort of thing is the world itself? That would put Mary in a curious predicament. She cannot answer this by using her world model, because each part of that model is designed only to answer questions about particular things. The trouble is that the world itself is not a particular thing inside the world.\nOne way to deal with this (and a method that, surely, many children use) is adding to the model of the world an additional object \u2014 that represents the world itself. Then, since any object must have properties, the child might then assign to this the features, say, of an extremely large ball. Naturally this will lead to trouble should that child persist in asking ordinary questions about this extraordinary object \u2014 such as What keeps the universe in place? or What's outside the universe? \u2014 for these then lead to strange and inconsistent images. Eventually we learn some ways to deal with this \u2014 for example, by learning which questions to suppress. But as in the case of a perfect point, we may always feel uncomfortable with the thought of a thing that is unimaginably large in size, but has no shape at all.\nWhen you get right down to it, you can never really describe any worldly thing, either \u2014 that is, in any absolute sense. Whatever you purport to say about a thing, you're only expressing your own beliefs. Yet even that gloomy thought suggests an insight. Even if our models of the world cannot yield good answers about the world as a whole, and even though their other answers are frequently wrong, they can tell us something about ourselves. We can regard what we learn about our models of the world as constituting our models of our models of the world.",
    "type": "article",
    "title": "30.4 world models",
    "tags": [
      {
        "score": 0.7426510453224182,
        "sentiment": 0.926,
        "count": 5,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6785269975662231,
        "sentiment": 0.563,
        "count": 4,
        "label": "model",
        "uri": "https://diffbot.com/entity/XcR6HArBoMQq9ruGasscqKw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6303886771202087,
        "sentiment": 0.503,
        "count": 2,
        "label": "DC Universe",
        "uri": "https://diffbot.com/entity/XsQhHJHXuOsOH94JbeVUjwA",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      }
    ],
    "docId": 67191767467,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 159116624259,
    "gburl": "http://aurellem.org/society-of-mind/som-30.4.html-diffbotxyz2179280645",
    "lastCrawlTimeUTC": 1588762537,
    "timestamp": "Wed, 06 May 2020 10:55:37 GMT"
  },
  {
    "sentiment": 0.928,
    "humanLanguage": "en",
    "diffbotUri": "article|3|2136141355",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-23.html",
    "text": "",
    "type": "article",
    "title": "23 comparisons",
    "docId": 84575584661,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 46864040370,
    "gburl": "http://aurellem.org/society-of-mind/som-23.html-diffbotxyz1118628404",
    "lastCrawlTimeUTC": 1588762505,
    "timestamp": "Wed, 06 May 2020 10:55:05 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-373048",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-2.5.html",
    "html": "<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<figure><video height=\"240\" width=\"320\"> Your browser does not support the video tag. </video></figure>\n<p>In the late 1960s Builder was embodied in the form of a computer program at the MIT Artificial Intelligence Laboratory. Both my collaborator, Seymour Papert, and I had long desired to combine a mechanical hand, a television eye, and a computer into a robot that could build with children's building-blocks. It took several years for us and our students to develop Move, See, Grasp, and hundreds of other little programs we needed to make a working Builder-agency. I like to think that this project gave us glimpses of what happens inside certain parts of children's minds when they learn to <em>play</em> with simple toys. The project left us wondering if even a thousand microskills would be enough to enable a child to fill a pail with sand. It was this body of experience, more than anything we'd learned about psychology, that led us to many ideas about societies of mind.</p>\n<p>To do those first experiments, we had to build a mechanical Hand, equipped with sensors for pressure and touch at its fingertips. Then we had to interface a television camera with our computer and write programs with which that Eye could discern the edges of the building-blocks. It also had to recognize the Hand itself. When those programs didn't work so well, we added more programs that used the fingers' feeling-sense to verify that things were where they visually seemed to be. Yet other programs were needed to enable the computer to move the Hand from place to place while using the eye to see that there was nothing in its way. We also had to write higher-level programs that the robot could use for planning what to do &mdash; and still more programs to make sure that those plans were actually carried out. To make this all work reliably, we needed programs to verify at every step (again by using Eye and Hand) that what had been planned inside the mind did actually take place outside &mdash; or else to correct the mistakes that occurred.</p>\n<p>In attempting to make our robot work, we found that many everyday problems were much more complicated than the sorts of problems, puzzles, and games adults consider hard. At every point, in that world of blocks, when we were forced to look more carefully than usual, we found an unexpected universe of complications. Consider just the seemingly simple problem of not reusing blocks already built into the tower. To a person, this seems simple common sense: <em>Don't use an object to satisfy a new goal if that object is already involved in accomplishing a prior goal.</em> No one knows exactly how human minds do this. Clearly we learn from experience to recognize the situations in which difficulties are likely to occur, and when we're older we learn to plan ahead to avoid such conflicts. But since we cannot be sure what will work, we must learn policies for dealing with uncertainty. Which strategies are best to try, and which will avoid the worst mistakes? Thousands and, perhaps, millions of little processes must be involved in how we anticipate, imagine, plan, predict, and prevent &mdash; and yet all this proceeds so automatically that we regard it as <em>ordinary common sense.</em> But if thinking is so complicated, what makes it seem so simple? At first it may seem incredible that our minds could use such intricate machinery and yet be unaware of it.</p>\n<p>In general, we're least aware of what our minds do best.</p>",
    "text": "Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag.\nIn the late 1960s Builder was embodied in the form of a computer program at the MIT Artificial Intelligence Laboratory. Both my collaborator, Seymour Papert, and I had long desired to combine a mechanical hand, a television eye, and a computer into a robot that could build with children's building-blocks. It took several years for us and our students to develop Move, See, Grasp, and hundreds of other little programs we needed to make a working Builder-agency. I like to think that this project gave us glimpses of what happens inside certain parts of children's minds when they learn to play with simple toys. The project left us wondering if even a thousand microskills would be enough to enable a child to fill a pail with sand. It was this body of experience, more than anything we'd learned about psychology, that led us to many ideas about societies of mind.\nTo do those first experiments, we had to build a mechanical Hand, equipped with sensors for pressure and touch at its fingertips. Then we had to interface a television camera with our computer and write programs with which that Eye could discern the edges of the building-blocks. It also had to recognize the Hand itself. When those programs didn't work so well, we added more programs that used the fingers' feeling-sense to verify that things were where they visually seemed to be. Yet other programs were needed to enable the computer to move the Hand from place to place while using the eye to see that there was nothing in its way. We also had to write higher-level programs that the robot could use for planning what to do \u2014 and still more programs to make sure that those plans were actually carried out. To make this all work reliably, we needed programs to verify at every step (again by using Eye and Hand) that what had been planned inside the mind did actually take place outside \u2014 or else to correct the mistakes that occurred.\nIn attempting to make our robot work, we found that many everyday problems were much more complicated than the sorts of problems, puzzles, and games adults consider hard. At every point, in that world of blocks, when we were forced to look more carefully than usual, we found an unexpected universe of complications. Consider just the seemingly simple problem of not reusing blocks already built into the tower. To a person, this seems simple common sense: Don't use an object to satisfy a new goal if that object is already involved in accomplishing a prior goal. No one knows exactly how human minds do this. Clearly we learn from experience to recognize the situations in which difficulties are likely to occur, and when we're older we learn to plan ahead to avoid such conflicts. But since we cannot be sure what will work, we must learn policies for dealing with uncertainty. Which strategies are best to try, and which will avoid the worst mistakes? Thousands and, perhaps, millions of little processes must be involved in how we anticipate, imagine, plan, predict, and prevent \u2014 and yet all this proceeds so automatically that we regard it as ordinary common sense. But if thinking is so complicated, what makes it seem so simple? At first it may seem incredible that our minds could use such intricate machinery and yet be unaware of it.\nIn general, we're least aware of what our minds do best.",
    "type": "article",
    "title": "2.5 easy things are hard",
    "tags": [
      {
        "score": 0.7838322520256042,
        "sentiment": 0,
        "count": 1,
        "label": "MIT Computer Science and Artificial Intelligence Laboratory",
        "uri": "https://diffbot.com/entity/OgvyA5O4DPjK5cXsUAwMn8g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/EducationalInstitution"
        ]
      },
      {
        "score": 0.6422429084777832,
        "sentiment": 0.32,
        "count": 1,
        "label": "Seymour Papert",
        "uri": "https://diffbot.com/entity/P8Ue3ofn7MP-xCyDxpkXe0A",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.6190242767333984,
        "sentiment": 0,
        "count": 0,
        "label": "construction worker",
        "uri": "https://diffbot.com/entity/XelVgOc5QN4uPgw8FXzWHiA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5808236002922058,
        "sentiment": -0.928,
        "count": 5,
        "label": "browser game",
        "uri": "https://diffbot.com/entity/XWqfGVRk5MNiKlDY006Ltbw"
      },
      {
        "score": 0.5704417824745178,
        "sentiment": -0.9,
        "count": 5,
        "label": "video art",
        "uri": "https://diffbot.com/entity/XQXzEmZu1MJKtizgHKbp4eg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5554991364479065,
        "sentiment": 0,
        "count": 2,
        "label": "children's literature",
        "uri": "https://diffbot.com/entity/XZZX2PiM6NXC3s1KFBNYheg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5081292986869812,
        "sentiment": -0.849,
        "count": 1,
        "label": "hard rock",
        "uri": "https://diffbot.com/entity/XDoRE3gmUNtiHdih9MUy7wg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Genre",
          "http://dbpedia.org/ontology/MusicGenre",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5060884356498718,
        "sentiment": 0.549,
        "count": 2,
        "label": "mechanics",
        "uri": "https://diffbot.com/entity/XMGWfrOjdPmiharqp9sLvlQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 163043869114,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 259415064997,
    "gburl": "http://aurellem.org/society-of-mind/som-2.5.html-diffbotxyz3741847923",
    "lastCrawlTimeUTC": 1588762475,
    "timestamp": "Wed, 06 May 2020 10:54:35 GMT"
  },
  {
    "sentiment": -0.12,
    "images": [
      {
        "naturalHeight": 120,
        "width": 384,
        "diffbotUri": "image|3|-433051998",
        "url": "http://aurellem.org/society-of-mind/illus/ch24/24-6.png",
        "naturalWidth": 384,
        "primary": true,
        "height": 120
      },
      {
        "naturalHeight": 173,
        "width": 362,
        "diffbotUri": "image|3|-432128477",
        "url": "http://aurellem.org/society-of-mind/illus/ch24/24-7.png",
        "naturalWidth": 362,
        "height": 173
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1319293354",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-24.9.html",
    "html": "<p>How do frames become activated? This amounts to asking how we recognize familiar situations or things. There is no limit to how complicated such a question can become, since there are no natural boundary lines between recognizing, remembering, and all the rest of how we think. For questions like this, with no place to start, we have to construct some boundary lines from our own imagination.</p>\n<p>We'll simply assume that every frame is activated by some set of recognizers. We can regard a recognizer as a type of agent that, in a sense, is the opposite of a K-line &mdash; since instead of arousing a certain state of mind, it has to recognize when a certain state of mind occurs. Accordingly, the recognizers of a frame are very much like the terminals of a frame, except that the connections to the terminals are reversed.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch24/24-6.png\"/></figure>\n<p>This suggests that not only frames but agencies in general might be organized in the form of agents sandwiched between recognizers and memorizers.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch24/24-7.png\"/></figure>\n<p>This sketch of how our agencies are organized is oversimplified. Each agent, be it a frame, a K-line, or whatever, must have some machinery for learning when it should become active &mdash; and that may require more than simply recognizing the presence of certain features. For example, to recognize an object as a car, it isn't enough that it include some assortment of parts like body, wheels, and license plate; the frame must also recognize that those parts are in suitable relationships &mdash; that the wheels be properly attached to the body, for example. Workers in the field of Artificial Intelligence have experimented with a variety of ways to make frame-recognizers, but the field is still in its infancy. The recognizers of our higher-level agencies might have to include mechanisms as complex as difference-engines in order to match their relational descriptions to actual situations.</p>",
    "text": "How do frames become activated? This amounts to asking how we recognize familiar situations or things. There is no limit to how complicated such a question can become, since there are no natural boundary lines between recognizing, remembering, and all the rest of how we think. For questions like this, with no place to start, we have to construct some boundary lines from our own imagination.\nWe'll simply assume that every frame is activated by some set of recognizers. We can regard a recognizer as a type of agent that, in a sense, is the opposite of a K-line \u2014 since instead of arousing a certain state of mind, it has to recognize when a certain state of mind occurs. Accordingly, the recognizers of a frame are very much like the terminals of a frame, except that the connections to the terminals are reversed.\nThis suggests that not only frames but agencies in general might be organized in the form of agents sandwiched between recognizers and memorizers.\nThis sketch of how our agencies are organized is oversimplified. Each agent, be it a frame, a K-line, or whatever, must have some machinery for learning when it should become active \u2014 and that may require more than simply recognizing the presence of certain features. For example, to recognize an object as a car, it isn't enough that it include some assortment of parts like body, wheels, and license plate; the frame must also recognize that those parts are in suitable relationships \u2014 that the wheels be properly attached to the body, for example. Workers in the field of Artificial Intelligence have experimented with a variety of ways to make frame-recognizers, but the field is still in its infancy. The recognizers of our higher-level agencies might have to include mechanisms as complex as difference-engines in order to match their relational descriptions to actual situations.",
    "type": "article",
    "title": "24.9 recognizers and memorizers",
    "tags": [
      {
        "score": 0.6471537351608276,
        "sentiment": 0.304,
        "count": 2,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      },
      {
        "score": 0.5677989721298218,
        "sentiment": 0,
        "count": 1,
        "label": "artificial intelligence",
        "uri": "https://diffbot.com/entity/X_lYDrjmAMlKKwXaDf958zg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 267114201515,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 157841785254,
    "gburl": "http://aurellem.org/society-of-mind/som-24.9.html-diffbotxyz1769883124",
    "lastCrawlTimeUTC": 1588762450,
    "timestamp": "Wed, 06 May 2020 10:54:10 GMT"
  },
  {
    "sentiment": 0.927,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1706057556",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.html",
    "text": "",
    "type": "article",
    "title": "7 problems and goals",
    "docId": 34660123008,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 79786295714,
    "gburl": "http://aurellem.org/society-of-mind/som-7.html-diffbotxyz2962167517",
    "lastCrawlTimeUTC": 1588762386,
    "timestamp": "Wed, 06 May 2020 10:53:06 GMT"
  },
  {
    "sentiment": -0.441,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1710884580",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.2.html",
    "html": "<p>The brain is imprisoned inside the skull, a silent, dark, and motionless place; how can it learn what it's like outside? The surface of the brain itself has not the slightest sense of touch; it has no skin with which to feel; it is only connected to skin. Nor can a brain see, for it has no eyes; it only is connected to eyes. The only paths from the world to the brain are bundles of nerves like those that come in from the eyes, ears, and skin. How do the signals that come through those nerves give rise to our sense of <em>being in</em> the outside world? The answer is that this sense is a complicated illusion. We never actually make any direct contact with the outside world. Instead, we work with models of the world that we build inside our brains. The next few sections try to sketch how that could come about.</p>\n<p>The surface of the skin contains countless little touch-sensing agents, and the retina of the eye includes a million tiny light detectors. Scientists know a good deal about how these sensors send signals to the brain. But we know much less about how those signals lead to sensations of touch and of sight. Try this simple experiment:</p>\n<p>Touch your ear.</p>\n<p>What did that feel like? It seems impossible to answer that because there's scarcely anything to say. Now try a different experiment:</p>\n<p>Touch your ear twice, in two different places, and also touch your nose.</p>\n<p>Which two touches feel most similar? That question seems much easier to answer: one might say that the two ear touches feel more similar. Evidently, there is scarcely anything that one can say about a <em>single sensation</em> by itself, but we can often say much more when we can make comparisons.</p>\n<p>Consider the analogy to how mathematics treats a <em>perfect point.</em> We shouldn't speak about its shape; it simply doesn't have a shape! But since we're used to things as having shapes,</p>\n<p>we can't help thinking of points as round, like <em>very tiny little dots.</em> Similarly, we're not supposed to talk about the size of a point &mdash; since mathematical points, by definition, have no size. Still, we can scarcely help but think, in any case, <em>they're very small.</em> In fact, there's absolutely nothing to be said about a single point, except how it relates to other points. This is not because such things are too complicated to explain, but because they are too simple to explain. One cannot even speak about where a point is, by itself &mdash; since <em>where</em> has meaning only in relation to other points in space. But once we know some pairs of points, we can relate these to the lines that connect them, and then we can define new, different points where various pairs of lines may intersect. Repeating this can generate entire worlds of geometry. Once we understand the terrifying fact that points are nothing by themselves but exist only in relation to other points, then we can ask, as Einstein did, whether time and space are anything more than vast societies of nearnesses.</p>\n<p>In the same way, there is little that one could say about any <em>single touch</em> &mdash; or about what any single sense-detecting agent does. However, there is much more to be said about the relations between two or more skin touches, because the closer together two skin spots are, the more frequently they'll both be touched at the same time.</p>",
    "text": "The brain is imprisoned inside the skull, a silent, dark, and motionless place; how can it learn what it's like outside? The surface of the brain itself has not the slightest sense of touch; it has no skin with which to feel; it is only connected to skin. Nor can a brain see, for it has no eyes; it only is connected to eyes. The only paths from the world to the brain are bundles of nerves like those that come in from the eyes, ears, and skin. How do the signals that come through those nerves give rise to our sense of being in the outside world? The answer is that this sense is a complicated illusion. We never actually make any direct contact with the outside world. Instead, we work with models of the world that we build inside our brains. The next few sections try to sketch how that could come about.\nThe surface of the skin contains countless little touch-sensing agents, and the retina of the eye includes a million tiny light detectors. Scientists know a good deal about how these sensors send signals to the brain. But we know much less about how those signals lead to sensations of touch and of sight. Try this simple experiment:\nTouch your ear.\nWhat did that feel like? It seems impossible to answer that because there's scarcely anything to say. Now try a different experiment:\nTouch your ear twice, in two different places, and also touch your nose.\nWhich two touches feel most similar? That question seems much easier to answer: one might say that the two ear touches feel more similar. Evidently, there is scarcely anything that one can say about a single sensation by itself, but we can often say much more when we can make comparisons.\nConsider the analogy to how mathematics treats a perfect point. We shouldn't speak about its shape; it simply doesn't have a shape! But since we're used to things as having shapes,\nwe can't help thinking of points as round, like very tiny little dots. Similarly, we're not supposed to talk about the size of a point \u2014 since mathematical points, by definition, have no size. Still, we can scarcely help but think, in any case, they're very small. In fact, there's absolutely nothing to be said about a single point, except how it relates to other points. This is not because such things are too complicated to explain, but because they are too simple to explain. One cannot even speak about where a point is, by itself \u2014 since where has meaning only in relation to other points in space. But once we know some pairs of points, we can relate these to the lines that connect them, and then we can define new, different points where various pairs of lines may intersect. Repeating this can generate entire worlds of geometry. Once we understand the terrifying fact that points are nothing by themselves but exist only in relation to other points, then we can ask, as Einstein did, whether time and space are anything more than vast societies of nearnesses.\nIn the same way, there is little that one could say about any single touch \u2014 or about what any single sense-detecting agent does. However, there is much more to be said about the relations between two or more skin touches, because the closer together two skin spots are, the more frequently they'll both be touched at the same time.",
    "type": "article",
    "title": "11.2 the shape of space",
    "tags": [
      {
        "score": 0.5812532305717468,
        "sentiment": 0.276,
        "count": 1,
        "label": "What It's Like",
        "uri": "https://diffbot.com/entity/XBneOA3n5Pm6zuEBbl9L8yA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Single"
        ]
      },
      {
        "score": 0.5329747796058655,
        "sentiment": 0,
        "count": 1,
        "label": "Direct Contact",
        "uri": "https://diffbot.com/entity/XL7LKK58qNJWEQPt5eXet6g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      }
    ],
    "docId": 39446249884,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 94073586093,
    "gburl": "http://aurellem.org/society-of-mind/som-11.2.html-diffbotxyz3474945760",
    "lastCrawlTimeUTC": 1588762324,
    "timestamp": "Wed, 06 May 2020 10:52:04 GMT"
  },
  {
    "sentiment": -0.111,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1629929355",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-27.4.html",
    "html": "<p>We spend our lives at learning things, yet always find exceptions and mistakes. Certainty seems always out of reach. This means that we have to take some risks to keep from being paralyzed by cowardice. But to keep from having accidents, we must accumulate two complementary types of knowledge:</p>\n<p>We search for <em>islands of consistency</em> within which ordinary reasoning seems safe. We work also to find and mark the unsafe boundaries of those domains.</p>\n<p>In civilized communities, appointed guardians post signs to warn about sharp turns, thin ice, and animals that bite. And so do our philosophers, when they report to us their paradoxical discoveries &mdash; those tales of the Liar who admits to lying and the Barber who shaves all the people who do not shave themselves. These valuable lessons teach us which thoughts we shouldn't think; they are the intellectual counterparts to Freud's emotion censors. It is interesting how frequently we find paradoxical nonsense to be funny, and when we come to the section on jokes, we'll see why this is so. When we look closely, we find that most jokes are concerned with taboos, injuries, and other ways of coming to harm &mdash; and logical absurdities can also lead to harm.</p>\n<p>We tell our children not to cross the road unless they are sure no car is coming. But what do we mean by <em>sure</em>? No one can ever really <em>prove</em> that no car is coming, since there is no way to rule out the possibility that some mad scientist has found a way to make cars invisible. In ordinary life we have to deal with <em>usual</em> instead of <em>true.</em> All we can really ask a child to do is <em>look both ways before you cross.</em> In the real world, it makes no sense to ask for absolute certainty.</p>\n<p>Unfortunately there are no simple, foolproof ways to get around the inconsistencies of common sense. Accordingly, we each must learn specific ways to keep from various mistakes. Why can't we do that logically? The answer is that perfect logic rarely works. One difficulty is finding foolproof rules for reasoning. But the more serious problem is that of finding foolproof bases for our arguments. It is virtually impossible to state any facts about the real world that actually are always true. We observed this when we discussed <em>Birds can Fly.</em> This statement applies to typical birds, but not to birds imprisoned in small cages, chained with leg irons, or under the influence of high-gravity fields. Similarly, when you're told, <em>Rover is a dog,</em> you'll assume that Rover has a tail, since your frame for a typical dog has a terminal for a tail. But should you learn that Rover lacks a tail, your mind won't self-destruct; instead, you'll change your Rover-frame &mdash; but still expect most other dogs to keep their tails.</p>\n<p>Exceptions are a fact of life because few <em>facts</em> are always true. Logic fails because it tries to find exceptions to this rule.</p>",
    "text": "We spend our lives at learning things, yet always find exceptions and mistakes. Certainty seems always out of reach. This means that we have to take some risks to keep from being paralyzed by cowardice. But to keep from having accidents, we must accumulate two complementary types of knowledge:\nWe search for islands of consistency within which ordinary reasoning seems safe. We work also to find and mark the unsafe boundaries of those domains.\nIn civilized communities, appointed guardians post signs to warn about sharp turns, thin ice, and animals that bite. And so do our philosophers, when they report to us their paradoxical discoveries \u2014 those tales of the Liar who admits to lying and the Barber who shaves all the people who do not shave themselves. These valuable lessons teach us which thoughts we shouldn't think; they are the intellectual counterparts to Freud's emotion censors. It is interesting how frequently we find paradoxical nonsense to be funny, and when we come to the section on jokes, we'll see why this is so. When we look closely, we find that most jokes are concerned with taboos, injuries, and other ways of coming to harm \u2014 and logical absurdities can also lead to harm.\nWe tell our children not to cross the road unless they are sure no car is coming. But what do we mean by sure? No one can ever really prove that no car is coming, since there is no way to rule out the possibility that some mad scientist has found a way to make cars invisible. In ordinary life we have to deal with usual instead of true. All we can really ask a child to do is look both ways before you cross. In the real world, it makes no sense to ask for absolute certainty.\nUnfortunately there are no simple, foolproof ways to get around the inconsistencies of common sense. Accordingly, we each must learn specific ways to keep from various mistakes. Why can't we do that logically? The answer is that perfect logic rarely works. One difficulty is finding foolproof rules for reasoning. But the more serious problem is that of finding foolproof bases for our arguments. It is virtually impossible to state any facts about the real world that actually are always true. We observed this when we discussed Birds can Fly. This statement applies to typical birds, but not to birds imprisoned in small cages, chained with leg irons, or under the influence of high-gravity fields. Similarly, when you're told, Rover is a dog, you'll assume that Rover has a tail, since your frame for a typical dog has a terminal for a tail. But should you learn that Rover lacks a tail, your mind won't self-destruct; instead, you'll change your Rover-frame \u2014 but still expect most other dogs to keep their tails.\nExceptions are a fact of life because few facts are always true. Logic fails because it tries to find exceptions to this rule.",
    "type": "article",
    "title": "27.4 exceptions to logic",
    "tags": [
      {
        "score": 0.5787825584411621,
        "sentiment": 0,
        "count": 2,
        "label": "Rover Group",
        "uri": "https://diffbot.com/entity/CiHCFFJpbPTuOb4QMRDrzpg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Corporation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5764229893684387,
        "sentiment": -0.576,
        "count": 1,
        "label": "barber",
        "uri": "https://diffbot.com/entity/Xh8U949EjPOiPoHtqzrKDNg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5398534536361694,
        "sentiment": 0,
        "count": 3,
        "label": "logic",
        "uri": "https://diffbot.com/entity/X2FR1oallObSTFA7yvuGb3g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5225154757499695,
        "sentiment": 0,
        "count": 2,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      },
      {
        "score": 0.5193563103675842,
        "sentiment": -0.437,
        "count": 2,
        "label": "joke",
        "uri": "https://diffbot.com/entity/XvJyT8Od1M821ttxhfdz4SQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5183797478675842,
        "sentiment": 0,
        "count": 1,
        "label": "Rover Company",
        "uri": "https://diffbot.com/entity/CoSPiqPI3OUOODwqifeIt8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      }
    ],
    "docId": 46431191429,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 28992176554,
    "gburl": "http://aurellem.org/society-of-mind/som-27.4.html-diffbotxyz2633776618",
    "lastCrawlTimeUTC": 1588762355,
    "timestamp": "Wed, 06 May 2020 10:52:35 GMT"
  },
  {
    "sentiment": 0.882,
    "images": [
      {
        "naturalHeight": 152,
        "width": 420,
        "diffbotUri": "image|3|-1966829153",
        "url": "http://aurellem.org/society-of-mind/illus/ch28/28-3.png",
        "naturalWidth": 420,
        "primary": true,
        "height": 152
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1663423545",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-28.8.html",
    "html": "<p>Consider the popular idea that a person is capable of two kinds of thinking at once &mdash; a <em>right brain</em> kind and a <em>left brain</em> kind &mdash; as though there were two different individuals inside each human brain. This raises some odd questions, since there are many other ways to draw imaginary boundaries through brains.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch28/28-3.png\"/></figure>\n<p>If you agree that each person has both a left-brain mind and right-brain mind, then you must also agree that each person also has a front-brain mind and a back-brain mind! Can a single large mind contain so many smaller ones, with overlapping boundaries? It makes sense to think of part of a structure as being a <em>thing</em> in its own right only when the relationships among parts of that structure have some significant type of coherency. Before you'd say that a certain arbitrary section of brain contains a mind, you'd want to have some evidence that what happens inside that boundary is something you would consider to be a mind.</p>\n<p>The less another entity resembles you, the less it means for you to say that it, like you, must have a mind. Do our very smallest agencies have minds? No, because it would make no more sense to say this than to say that two trees form a forest or that two bricks form a wall. But there are indeed some agencies inside our brains that do have humanlike abilities to solve, by themselves, some types of problems that we regard as hard. For example, your agencies for locomotion, vision, and language may contain within their boundaries some processes that are quite as intricate as those <em>you</em> use for your own conscious thought. Possibly, some of those processes are actually more <em>conscious</em> than you are yourself, in the sense that they maintain and use even more complete records of their own internal activities. Yet what happens in those agencies is so sealed off that you have no direct experience of how <em>you</em> distinguish a cat from a dog, retrace <em>your</em> last few steps, or listen and talk without knowing how <em>you</em> do it.</p>\n<p>All this suggests that it can make sense to think there exists, inside your brain, a society of different minds. Like members of a family, the different minds can work together to help each other, each still having its own mental experiences that the others never know about. Several such agencies could have many agents in common, yet still have no more sense of each other's interior activities than do people whose apartments share opposite sides of the same walls. Like tenants in a rooming house, the processes that share your brain need not share one another's mental lives.</p>\n<p>If each of us contains several such mini-minds, could any special exercise help put them all <em>in closer touch</em>? Certainly there are ways to become selectively aware of processes that are usually not conscious at all. But becoming aware of everything that happens in brains would leave no room for thought. And the reports of those who claim to have developed such skills seem singularly uninformative. If anything, they demonstrate that it's even harder than we think to penetrate those unresisting barriers.</p>",
    "text": "Consider the popular idea that a person is capable of two kinds of thinking at once \u2014 a right brain kind and a left brain kind \u2014 as though there were two different individuals inside each human brain. This raises some odd questions, since there are many other ways to draw imaginary boundaries through brains.\nIf you agree that each person has both a left-brain mind and right-brain mind, then you must also agree that each person also has a front-brain mind and a back-brain mind! Can a single large mind contain so many smaller ones, with overlapping boundaries? It makes sense to think of part of a structure as being a thing in its own right only when the relationships among parts of that structure have some significant type of coherency. Before you'd say that a certain arbitrary section of brain contains a mind, you'd want to have some evidence that what happens inside that boundary is something you would consider to be a mind.\nThe less another entity resembles you, the less it means for you to say that it, like you, must have a mind. Do our very smallest agencies have minds? No, because it would make no more sense to say this than to say that two trees form a forest or that two bricks form a wall. But there are indeed some agencies inside our brains that do have humanlike abilities to solve, by themselves, some types of problems that we regard as hard. For example, your agencies for locomotion, vision, and language may contain within their boundaries some processes that are quite as intricate as those you use for your own conscious thought. Possibly, some of those processes are actually more conscious than you are yourself, in the sense that they maintain and use even more complete records of their own internal activities. Yet what happens in those agencies is so sealed off that you have no direct experience of how you distinguish a cat from a dog, retrace your last few steps, or listen and talk without knowing how you do it.\nAll this suggests that it can make sense to think there exists, inside your brain, a society of different minds. Like members of a family, the different minds can work together to help each other, each still having its own mental experiences that the others never know about. Several such agencies could have many agents in common, yet still have no more sense of each other's interior activities than do people whose apartments share opposite sides of the same walls. Like tenants in a rooming house, the processes that share your brain need not share one another's mental lives.\nIf each of us contains several such mini-minds, could any special exercise help put them all in closer touch? Certainly there are ways to become selectively aware of processes that are usually not conscious at all. But becoming aware of everything that happens in brains would leave no room for thought. And the reports of those who claim to have developed such skills seem singularly uninformative. If anything, they demonstrate that it's even harder than we think to penetrate those unresisting barriers.",
    "type": "article",
    "title": "28.8 overlapping minds",
    "tags": [
      {
        "score": 0.596863865852356,
        "sentiment": 0,
        "count": 2,
        "label": "brain",
        "uri": "https://diffbot.com/entity/X_EH2MANnPh60z9fSfbgdHw"
      }
    ],
    "docId": 143364719033,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 45521273270,
    "gburl": "http://aurellem.org/society-of-mind/som-28.8.html-diffbotxyz1656277793",
    "lastCrawlTimeUTC": 1588762418,
    "timestamp": "Wed, 06 May 2020 10:53:38 GMT"
  },
  {
    "sentiment": -0.227,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-889788092",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-1.2.html",
    "html": "<blockquote> It was never supposed <em>[the poet Imlac said]</em> that cogitation is inherent in matter, or that every particle is a thinking being. Yet if any part of matter be devoid of thought, what part can we suppose to think? Matter can differ from matter only in form, density, bulk, motion, and direction of motion. To which of these, however varied or combined, can consciousness be annexed? To be round or square, to be solid or fluid, to be great or little, to be moved slowly or swiftly, one way or another, are modes of material existence all equally alien from the nature of cogitation. If matter be once without thought, it can only be made to think by some new modification; but all the modifications which it can admit are equally unconnected with cogitative powers. &mdash;Samuel Johnson</blockquote>\n<p>How could solid-seeming brains support such ghostly things as thoughts? This question troubled many thinkers of the past. The world of thoughts and the world of things appeared to be too far apart to interact in any way. So long as thoughts seemed so utterly different from everything else, there seemed to be no place to start.</p>\n<p>A few centuries ago it seemed equally impossible to explain Life, because living things appeared to be so different from anything else. Plants seemed to grow from nothing. Animals could move and learn. Both could reproduce themselves &mdash; while nothing else could do such things. But then that awesome gap began to close. Every living thing was found to be composed of smaller cells, and cells turned out to be composed of complex but comprehensible chemicals. Soon it was found that plants did not create any substance at all but simply extracted most of their material from gases in the air. Mysteriously pulsing hearts turned out to be no more than mechanical pumps, composed of networks of muscle cells. But it was not until the present century that John von Neumann showed theoretically how cell-machines could reproduce while, almost independently, James Watson and Francis Crick discovered how each cell actually makes copies of its own hereditary code. No longer does an educated person have to seek any special, vital force to animate each living thing.</p>\n<p>Similarly, a century ago, we had essentially no way to start to explain how thinking works. Then psychologists like Sigmund Freud and Jean Piaget produced their theories about child development. Somewhat later, on the mechanical side, mathematicians like Kurt G&ouml;del and Alan Turing began to reveal the hitherto unknown range of what machines could be made to do. These two streams of thought began to merge only in the 1940s, when Warren McCulloch and Walter Pitts began to show how machines might be made to see, reason, and remember. Research in the modern science of Artificial Intelligence started only in the 1950s, stimulated by the invention of modern computers. This inspired a flood of new ideas about how machines could do what only minds had done previously.</p>\n<p>Most people still believe that no machine could ever be conscious, or feel ambition, jealousy, humor, or have any other mental life-experience. To be sure, we are still far from being able to create machines that do all the things people do. But this only means that we need better theories about how thinking works. This book will show how the tiny machines that we'll call <em>agents of the mind</em> could be the long sought <em>particles</em> that those theories need.</p>",
    "text": "It was never supposed [the poet Imlac said] that cogitation is inherent in matter, or that every particle is a thinking being. Yet if any part of matter be devoid of thought, what part can we suppose to think? Matter can differ from matter only in form, density, bulk, motion, and direction of motion. To which of these, however varied or combined, can consciousness be annexed? To be round or square, to be solid or fluid, to be great or little, to be moved slowly or swiftly, one way or another, are modes of material existence all equally alien from the nature of cogitation. If matter be once without thought, it can only be made to think by some new modification; but all the modifications which it can admit are equally unconnected with cogitative powers. \u2014Samuel Johnson\nHow could solid-seeming brains support such ghostly things as thoughts? This question troubled many thinkers of the past. The world of thoughts and the world of things appeared to be too far apart to interact in any way. So long as thoughts seemed so utterly different from everything else, there seemed to be no place to start.\nA few centuries ago it seemed equally impossible to explain Life, because living things appeared to be so different from anything else. Plants seemed to grow from nothing. Animals could move and learn. Both could reproduce themselves \u2014 while nothing else could do such things. But then that awesome gap began to close. Every living thing was found to be composed of smaller cells, and cells turned out to be composed of complex but comprehensible chemicals. Soon it was found that plants did not create any substance at all but simply extracted most of their material from gases in the air. Mysteriously pulsing hearts turned out to be no more than mechanical pumps, composed of networks of muscle cells. But it was not until the present century that John von Neumann showed theoretically how cell-machines could reproduce while, almost independently, James Watson and Francis Crick discovered how each cell actually makes copies of its own hereditary code. No longer does an educated person have to seek any special, vital force to animate each living thing.\nSimilarly, a century ago, we had essentially no way to start to explain how thinking works. Then psychologists like Sigmund Freud and Jean Piaget produced their theories about child development. Somewhat later, on the mechanical side, mathematicians like Kurt Gödel and Alan Turing began to reveal the hitherto unknown range of what machines could be made to do. These two streams of thought began to merge only in the 1940s, when Warren McCulloch and Walter Pitts began to show how machines might be made to see, reason, and remember. Research in the modern science of Artificial Intelligence started only in the 1950s, stimulated by the invention of modern computers. This inspired a flood of new ideas about how machines could do what only minds had done previously.\nMost people still believe that no machine could ever be conscious, or feel ambition, jealousy, humor, or have any other mental life-experience. To be sure, we are still far from being able to create machines that do all the things people do. But this only means that we need better theories about how thinking works. This book will show how the tiny machines that we'll call agents of the mind could be the long sought particles that those theories need.",
    "type": "article",
    "title": "1.2 The mind and the brain",
    "tags": [
      {
        "score": 0.5562801361083984,
        "sentiment": 0,
        "count": 1,
        "label": "artificial intelligence",
        "uri": "https://diffbot.com/entity/X_lYDrjmAMlKKwXaDf958zg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.533203661441803,
        "sentiment": 0,
        "count": 1,
        "label": "John von Neumann",
        "uri": "https://diffbot.com/entity/PM1QaxG9GPSGEYdfYpcuzdA",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5298103094100952,
        "sentiment": 0,
        "count": 1,
        "label": "Francis Crick",
        "uri": "https://diffbot.com/entity/PGi6DSEmnNnGqSz-viToVXw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5263044238090515,
        "sentiment": 0,
        "count": 1,
        "label": "Warren Sturgis McCulloch",
        "uri": "https://diffbot.com/entity/PzUDb2uE8MKSbYUcPg_VwNQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.521742582321167,
        "sentiment": 0,
        "count": 1,
        "label": "James D. Watson",
        "uri": "https://diffbot.com/entity/PLL0HRJY7O_Csy9qcd8biVw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5055437088012695,
        "sentiment": 0.169,
        "count": 2,
        "label": "video game mod",
        "uri": "https://diffbot.com/entity/X3ozFQgs4Om2HKG25K8r2Fw"
      }
    ],
    "docId": 222150082980,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 231610319255,
    "gburl": "http://aurellem.org/society-of-mind/som-1.2.html-diffbotxyz2869012600",
    "lastCrawlTimeUTC": 1588762307,
    "timestamp": "Wed, 06 May 2020 10:51:47 GMT"
  },
  {
    "sentiment": -0.694,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1237439242",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.7.html",
    "html": "<p>Our everyday ideas about the progression of mental time are wrong: they leave no room for the fact that every agent has a different causal history. To be sure, those different pasts are intermixed over longer spans of time, and every agent is eventually influenced by what has happened in the common, remote history of its society. But that's not what one means by <em>now.</em> The problem is with the connections between the moment-to-moment activities of largely separate agencies.</p>\n<p>When a pin drops, you might say, <em>I just heard a pin drop.</em> But no one says, <em>I hear a pin dropping.</em> Our speaking agencies know from experience that the physical episode of pin dropping will be over before you can even start to speak. But you would say, <em>I am in love,</em> rather than <em>I was just in love,</em> because your speaking agencies know that the agencies involved with personal attachments work at a slower pace, with states that may persist for months or years. And, in between, when someone asks, <em>What sorts of feelings have you now?</em> we often find our half-formed answers wrong before they can be expressed, as other feelings intervene. What seems only a moment to one agency may seem like an era to another.</p>\n<p>Our memories are only indirectly linked to physical time. We have no absolute sense of when a memorable event <em>actually</em> happened. At best, we can only know some temporal relations between it and certain other events. You might be able to recall that X and Y occurred on different days but be unable to determine which of those days came earlier. And many memories seem not to be linked to intervals of time at all &mdash; like knowing that four comes after three, or that <em>I am myself.</em></p>\n<p>The slower an agency operates &mdash; that is, the longer the intervals between each change of state &mdash; the more external signals can arrive inside those intervals. Does this mean that the outside world will appear to move faster to a slow agency than to a faster agency? Does life seem swift to tortoises, but tedious to hummingbirds?</p>",
    "text": "Our everyday ideas about the progression of mental time are wrong: they leave no room for the fact that every agent has a different causal history. To be sure, those different pasts are intermixed over longer spans of time, and every agent is eventually influenced by what has happened in the common, remote history of its society. But that's not what one means by now. The problem is with the connections between the moment-to-moment activities of largely separate agencies.\nWhen a pin drops, you might say, I just heard a pin drop. But no one says, I hear a pin dropping. Our speaking agencies know from experience that the physical episode of pin dropping will be over before you can even start to speak. But you would say, I am in love, rather than I was just in love, because your speaking agencies know that the agencies involved with personal attachments work at a slower pace, with states that may persist for months or years. And, in between, when someone asks, What sorts of feelings have you now? we often find our half-formed answers wrong before they can be expressed, as other feelings intervene. What seems only a moment to one agency may seem like an era to another.\nOur memories are only indirectly linked to physical time. We have no absolute sense of when a memorable event actually happened. At best, we can only know some temporal relations between it and certain other events. You might be able to recall that X and Y occurred on different days but be unable to determine which of those days came earlier. And many memories seem not to be linked to intervals of time at all \u2014 like knowing that four comes after three, or that I am myself.\nThe slower an agency operates \u2014 that is, the longer the intervals between each change of state \u2014 the more external signals can arrive inside those intervals. Does this mean that the outside world will appear to move faster to a slow agency than to a faster agency? Does life seem swift to tortoises, but tedious to hummingbirds?",
    "type": "article",
    "title": "6.7 the causal now",
    "docId": 36186096048,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 70429999538,
    "gburl": "http://aurellem.org/society-of-mind/som-6.7.html-diffbotxyz1125504844",
    "lastCrawlTimeUTC": 1588762211,
    "timestamp": "Wed, 06 May 2020 10:50:11 GMT"
  },
  {
    "sentiment": -0.26,
    "humanLanguage": "en",
    "diffbotUri": "article|3|266842696",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.1.html",
    "html": "<p>We often say what we want to say in fewer words than might seem possible.</p>\n<p><em>Do you see the table over there?</em> <em>Yes.</em> <em>Do you see the red block on it?</em> <em>Yes.</em> <em>Good. Please bring it to me.</em></p>\n<p>That first <em>it</em> saves the speaker from having to say <em>that table</em> again. The second <em>it</em> does the same for <em>the red block.</em> Accordingly, many people consider a pronoun like <em>it</em> to be an abbreviation or substitute for another phrase used recently. But when we look at this more carefully, we see that a pronoun need not refer to any phrase at all. For instance, the word <em>this</em> in the previous sentence was not an abbreviation for any particular phrase. Instead, it served as a signal to you, the reader, to examine more carefully a certain partial state of mind &mdash; in this case, a certain theory about pronouns &mdash; that I assumed was aroused in your mind by the sentences that preceded it. In other words, pronouns do not signify objects or words; instead, they represent conceptions, ideas, or activities that the speaker assumes are going on inside the listener's mind. But how can the listener tell which one of the activities is signified when there are several possibilities?</p>\n<p><em>Do you remember the ring Jane liked?</em> <em>Yes.</em> <em>Good. Please buy it and give it to her.</em></p>\n<p>How do we know that <em>her</em> means Jane and that <em>it</em> means the ring &mdash; and not the other way around? We can tell that <em>her</em> is not the ring because English grammar usually restricts the pronoun <em>her</em> to apply only to a female person &mdash; though it could also mean an animal, a country, or a ship. But you would know that <em>it</em> means <em>the ring</em> in any case, because your Buy agency would not accept the thought of buying Jane, nor would your agency for Give accept the thought of giving gifts to rings. If someone said, <em>Buy Jane and give her to that ring,</em> both Buy and Give would have such strong conflicts that the problem would ascend to the listener's higher-level agencies, which would react with disbelief.</p>\n<p>Our language often uses pronounlike words to refer to mental activities &mdash; but we do not do this only in language: it happens in all the other higher-level functions of our minds. Later we'll see how Find will find a block &mdash; rather than, say, a toy giraffe &mdash; even though Builder has only said <em>find.</em> The trick is that Find will use whichever description happens to be available in its current context. Since it's already working for Builder, its subagents will assume that it should find a building-block.</p>\n<p>Whenever we talk or think, we use pronounlike devices to exploit whatever mental activities have already been aroused, to interlink the thoughts already active in the mind. To do this, though, we need to have machinery we can use as temporary <em>handles</em> for taking hold of, and moving around, those active fragments of mental states. To emphasize the analogy with the pronouns of our languages, I'll call such handles <em>pronomes.</em> The next few sections speculate about how pronomes work.</p>",
    "text": "We often say what we want to say in fewer words than might seem possible.\nDo you see the table over there? Yes. Do you see the red block on it? Yes. Good. Please bring it to me.\nThat first it saves the speaker from having to say that table again. The second it does the same for the red block. Accordingly, many people consider a pronoun like it to be an abbreviation or substitute for another phrase used recently. But when we look at this more carefully, we see that a pronoun need not refer to any phrase at all. For instance, the word this in the previous sentence was not an abbreviation for any particular phrase. Instead, it served as a signal to you, the reader, to examine more carefully a certain partial state of mind \u2014 in this case, a certain theory about pronouns \u2014 that I assumed was aroused in your mind by the sentences that preceded it. In other words, pronouns do not signify objects or words; instead, they represent conceptions, ideas, or activities that the speaker assumes are going on inside the listener's mind. But how can the listener tell which one of the activities is signified when there are several possibilities?\nDo you remember the ring Jane liked? Yes. Good. Please buy it and give it to her.\nHow do we know that her means Jane and that it means the ring \u2014 and not the other way around? We can tell that her is not the ring because English grammar usually restricts the pronoun her to apply only to a female person \u2014 though it could also mean an animal, a country, or a ship. But you would know that it means the ring in any case, because your Buy agency would not accept the thought of buying Jane, nor would your agency for Give accept the thought of giving gifts to rings. If someone said, Buy Jane and give her to that ring, both Buy and Give would have such strong conflicts that the problem would ascend to the listener's higher-level agencies, which would react with disbelief.\nOur language often uses pronounlike words to refer to mental activities \u2014 but we do not do this only in language: it happens in all the other higher-level functions of our minds. Later we'll see how Find will find a block \u2014 rather than, say, a toy giraffe \u2014 even though Builder has only said find. The trick is that Find will use whichever description happens to be available in its current context. Since it's already working for Builder, its subagents will assume that it should find a building-block.\nWhenever we talk or think, we use pronounlike devices to exploit whatever mental activities have already been aroused, to interlink the thoughts already active in the mind. To do this, though, we need to have machinery we can use as temporary handles for taking hold of, and moving around, those active fragments of mental states. To emphasize the analogy with the pronouns of our languages, I'll call such handles pronomes. The next few sections speculate about how pronomes work.",
    "type": "article",
    "title": "21.1 the pronouns of the mind",
    "tags": [
      {
        "score": 0.7254357933998108,
        "sentiment": 0.83,
        "count": 4,
        "label": "Jane Beale",
        "uri": "https://diffbot.com/entity/XLM-xUc81MxSrGE9iw6A0IQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6535167098045349,
        "sentiment": 0,
        "count": 0,
        "label": "Yes",
        "uri": "https://diffbot.com/entity/BHz1Dv5zaPVeJkkIyFApSnQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.6131614446640015,
        "sentiment": 0,
        "count": 2,
        "label": "construction worker",
        "uri": "https://diffbot.com/entity/XelVgOc5QN4uPgw8FXzWHiA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5846374034881592,
        "sentiment": 0,
        "count": 2,
        "label": "speaker",
        "uri": "https://diffbot.com/entity/rY_pyVgIkNpOp45OVKPq5_w"
      },
      {
        "score": 0.5524194240570068,
        "sentiment": 0,
        "count": 1,
        "label": "Fly Me to the Moon",
        "uri": "https://diffbot.com/entity/XRZrQ-OahMnaptNVys6VVhw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5408288240432739,
        "sentiment": -0.297,
        "count": 1,
        "label": "English grammar",
        "uri": "https://diffbot.com/entity/XGjH99PuHMniqVj1XE0VQ4A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 73444704647,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 265829810621,
    "gburl": "http://aurellem.org/society-of-mind/som-21.1.html-diffbotxyz1664349130",
    "lastCrawlTimeUTC": 1588762274,
    "timestamp": "Wed, 06 May 2020 10:51:14 GMT"
  },
  {
    "sentiment": 0.951,
    "images": [
      {
        "naturalHeight": 137,
        "width": 344,
        "diffbotUri": "image|3|1143798538",
        "url": "http://aurellem.org/society-of-mind/illus/ch5/5-2.png",
        "naturalWidth": 344,
        "primary": true,
        "height": 137
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1032572159",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-5.7.html",
    "html": "<p>What do we signify by words like <em>me,</em> <em>myself,</em> and <em>I</em>? What does a story mean that starts with <em>In my childhood</em>? What is that strange possession <em>you,</em> which stays the same throughout your life? Are you the same person you were before you learned to read? You scarcely can imagine, now, how words looked then. Just try to look at these words without reading them:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch5/5-2.png\"/></figure>\n<p>We find it almost impossible to separate the appearances of things from what they've come to mean to us. But if we cannot recollect how things appeared to us before we learned to link new meanings to those things, what makes us think we can recollect how we ourselves appeared to us in previous times? What would you say if someone asked questions like these:</p>\n<p><em>Are you the same person now that you once were, before you learned to talk?</em> <em>Of course I am. Why, who else could I be?</em> <em>Do you mean that you haven't changed at all?</em> <em>Of course not. I only mean I'm the same person &mdash; the same in some ways, different in others &mdash; but still the same me.</em> <em>But how can you be the same as the person you were before you had even learned to remember things? Can you even imagine what that was like?</em> <em>Perhaps I can't &mdash; yet still there must have been some continuity. Even if I can't remember it, I surely was that person, too.</em></p>\n<p>We all experience that sense of changelessness in spite of change, not only for the past but also for the future, too! Consider how you are generous to future self at present self's expense. Today, you put some money in the bank in order that sometime later you can take it out. Whenever did that future self do anything so good for you? Is <em>you</em> the body of those memories whose meanings change only slowly? Is it the never-ending side effects of all your previous experience? Or is it just whichever of your agents change the least as time and life proceed?</p>",
    "text": "What do we signify by words like me, myself, and I? What does a story mean that starts with In my childhood? What is that strange possession you, which stays the same throughout your life? Are you the same person you were before you learned to read? You scarcely can imagine, now, how words looked then. Just try to look at these words without reading them:\nWe find it almost impossible to separate the appearances of things from what they've come to mean to us. But if we cannot recollect how things appeared to us before we learned to link new meanings to those things, what makes us think we can recollect how we ourselves appeared to us in previous times? What would you say if someone asked questions like these:\nAre you the same person now that you once were, before you learned to talk? Of course I am. Why, who else could I be? Do you mean that you haven't changed at all? Of course not. I only mean I'm the same person \u2014 the same in some ways, different in others \u2014 but still the same me. But how can you be the same as the person you were before you had even learned to remember things? Can you even imagine what that was like? Perhaps I can't \u2014 yet still there must have been some continuity. Even if I can't remember it, I surely was that person, too.\nWe all experience that sense of changelessness in spite of change, not only for the past but also for the future, too! Consider how you are generous to future self at present self's expense. Today, you put some money in the bank in order that sometime later you can take it out. Whenever did that future self do anything so good for you? Is you the body of those memories whose meanings change only slowly? Is it the never-ending side effects of all your previous experience? Or is it just whichever of your agents change the least as time and life proceed?",
    "type": "article",
    "title": "5.7 permanent identity",
    "tags": [
      {
        "score": 0.5984936356544495,
        "sentiment": 0.713,
        "count": 1,
        "label": "cultural identity",
        "uri": "https://diffbot.com/entity/Xan9aqh0jOn-5YJhgTZXcoQ"
      }
    ],
    "docId": 184659411387,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 118605398446,
    "gburl": "http://aurellem.org/society-of-mind/som-5.7.html-diffbotxyz411362345",
    "lastCrawlTimeUTC": 1588762252,
    "timestamp": "Wed, 06 May 2020 10:50:52 GMT"
  },
  {
    "sentiment": 0.293,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1105499354",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.2.html",
    "html": "<p>We introduced the concept of a polyneme to explain how an agent could communicate with many other kinds of agencies. In order for a polyneme to work, each of its recipients must learn its own way to react. Now we've seen a second way, for a pronome is also an agent that can interact with many other agencies. The difference is that a pronome has essentially the same effect on each of its recipients &mdash; namely, to activate or to assign a certain short-term memory-unit. I'll introduce a new word &mdash; <em>isonome</em> &mdash; for any agent that has this sort of uniform effect on many agencies.</p>\n<p>An isonome has a similar, built-in effect on each of its recipients. It thus applies the same idea to many different things at once. A polyneme has different, learned effects on each of its</p>\n<p>recipients. It thus connects the same thing to many different ideas.</p>\n<p>Why should isonomes exist at all? Because our agencies have common genetic origins, they tend also to be architecturally similar. So they'll tend to lie in roughly parallel formations like the pages of a book, operate in generally similar ways, and have similar memory-control processes. Then any agent whose connections tend to run straight through the pages of that book from cover to cover will tend to have similar effects on all of them.</p>\n<p>Both isonomes and polynemes are involved with memories &mdash; but polynemes are essentially the memories themselves, while isonomes control how memories are used. Pronomes are a particular type of isonome; there must also be <em>interruption isonomes</em> that work similarly but manage memories on larger scales &mdash; for example, for storing away the several pronome memories of an entire Trans- frame all at once. (We'll see how something like this must be done whenever we encounter a grammar word like <em>who</em> or <em>which.</em>) Yet other types of isonomes must be involved whenever an agent is used to control the level-band of activity in another agency without concern for all the fine details of what happens inside that agency. So the power of polynemes stems from how they learn to arouse many different processes at once, while isonomes draw their power from exploiting abilities that are already common to many agencies.</p>",
    "text": "We introduced the concept of a polyneme to explain how an agent could communicate with many other kinds of agencies. In order for a polyneme to work, each of its recipients must learn its own way to react. Now we've seen a second way, for a pronome is also an agent that can interact with many other agencies. The difference is that a pronome has essentially the same effect on each of its recipients \u2014 namely, to activate or to assign a certain short-term memory-unit. I'll introduce a new word \u2014 isonome \u2014 for any agent that has this sort of uniform effect on many agencies.\nAn isonome has a similar, built-in effect on each of its recipients. It thus applies the same idea to many different things at once. A polyneme has different, learned effects on each of its\nrecipients. It thus connects the same thing to many different ideas.\nWhy should isonomes exist at all? Because our agencies have common genetic origins, they tend also to be architecturally similar. So they'll tend to lie in roughly parallel formations like the pages of a book, operate in generally similar ways, and have similar memory-control processes. Then any agent whose connections tend to run straight through the pages of that book from cover to cover will tend to have similar effects on all of them.\nBoth isonomes and polynemes are involved with memories \u2014 but polynemes are essentially the memories themselves, while isonomes control how memories are used. Pronomes are a particular type of isonome; there must also be interruption isonomes that work similarly but manage memories on larger scales \u2014 for example, for storing away the several pronome memories of an entire Trans- frame all at once. (We'll see how something like this must be done whenever we encounter a grammar word like who or which.) Yet other types of isonomes must be involved whenever an agent is used to control the level-band of activity in another agency without concern for all the fine details of what happens inside that agency. So the power of polynemes stems from how they learn to arouse many different processes at once, while isonomes draw their power from exploiting abilities that are already common to many agencies.",
    "type": "article",
    "title": "22.2 isonomes",
    "tags": [
      {
        "score": 0.5639864206314087,
        "sentiment": 0.209,
        "count": 6,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5034347772598267,
        "sentiment": 0,
        "count": 1,
        "label": "I'll",
        "uri": "https://diffbot.com/entity/XKpTgx9VLPNSLDS43t-KlVg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/Comic",
          "http://dbpedia.org/ontology/Manga"
        ]
      }
    ],
    "docId": 221048783242,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 236152897947,
    "gburl": "http://aurellem.org/society-of-mind/som-22.2.html-diffbotxyz2840225223",
    "lastCrawlTimeUTC": 1588762180,
    "timestamp": "Wed, 06 May 2020 10:49:40 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1166943477",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.2.html",
    "html": "<p>Many common words are ambiguous enough that even simple sentences can be understood in several ways.</p>\n<p>The astronomer married the star.</p>\n<p>It probably was a movie star &mdash; though the listener may have experienced a moment of confusion. The trouble is that the word <em>star</em> is linked to different polynemes for a celestial body, a theatrical celebrity, or an object with a certain shape. The momentary confusion comes because the word <em>astronomer</em> gives us an initial bias toward the celestial sense of <em>star.</em> But that inhuman meaning causes conflict in our marriage-agent, and this soon leads to another, more consistent interpretation. The problem is harder when a sentence contains two or more ambiguous words.</p>\n<p>John shot two bucks.</p>\n<p>Alone, the word <em>shot</em> could refer either to shooting a gun or, in American slang, to making a bet. By itself, the word <em>buck</em> could mean either a dollar or a male deer. These alternatives permit at least four conceivable interpretations. Two of them are quite implausible, because people rarely shoot bullets at dollars or bet deer. But the other two are possible, since, unfortunately, people do bet dollars and shoot at deer. Without more clues, we have no way to choose between these interpretations. Yet we wouldn't have the slightest doubt that <em>buck</em> means dollar if the previous context gave a hint that money or gambling was involved &mdash; rather than hunting, forestry, or outdoor life.</p>\n<p>How do <em>contexts</em> work to clarify such ambiguities? The activity of an outdoors polyneme would start by producing a small bias for arousing deer rather than dollar and gun instead of bet. Then the <em>ring-closing</em> effect would swiftly make that preference sharp. Other polynemes like those for hunting and killing will soon be engaged and will combine to activate the recognizers for yet other, related polynemes like those for forest and animal. Soon this will produce a collection of mutually supporting polynemes that establish a single, consistent interpretation.</p>\n<p>One might fear that this would lead, instead, to an avalanche that arouses all the agents of the mind. That would be less likely to happen if the different possible senses of each word are made to compete, by being assembled into cross-exclusion groups. Then, as the polynemes for deer and gun gain strength, they will weaken and suppress the competing nemes for money and for wagering &mdash; and that will weaken, in turn, the other polynemes that support the alternative context of making bets. The end effect of this will be almost instantaneous. In only a few cycles of the meaning ring, the agents associated with deer and gun will completely suppress their competitors.</p>",
    "text": "Many common words are ambiguous enough that even simple sentences can be understood in several ways.\nThe astronomer married the star.\nIt probably was a movie star \u2014 though the listener may have experienced a moment of confusion. The trouble is that the word star is linked to different polynemes for a celestial body, a theatrical celebrity, or an object with a certain shape. The momentary confusion comes because the word astronomer gives us an initial bias toward the celestial sense of star. But that inhuman meaning causes conflict in our marriage-agent, and this soon leads to another, more consistent interpretation. The problem is harder when a sentence contains two or more ambiguous words.\nJohn shot two bucks.\nAlone, the word shot could refer either to shooting a gun or, in American slang, to making a bet. By itself, the word buck could mean either a dollar or a male deer. These alternatives permit at least four conceivable interpretations. Two of them are quite implausible, because people rarely shoot bullets at dollars or bet deer. But the other two are possible, since, unfortunately, people do bet dollars and shoot at deer. Without more clues, we have no way to choose between these interpretations. Yet we wouldn't have the slightest doubt that buck means dollar if the previous context gave a hint that money or gambling was involved \u2014 rather than hunting, forestry, or outdoor life.\nHow do contexts work to clarify such ambiguities? The activity of an outdoors polyneme would start by producing a small bias for arousing deer rather than dollar and gun instead of bet. Then the ring-closing effect would swiftly make that preference sharp. Other polynemes like those for hunting and killing will soon be engaged and will combine to activate the recognizers for yet other, related polynemes like those for forest and animal. Soon this will produce a collection of mutually supporting polynemes that establish a single, consistent interpretation.\nOne might fear that this would lead, instead, to an avalanche that arouses all the agents of the mind. That would be less likely to happen if the different possible senses of each word are made to compete, by being assembled into cross-exclusion groups. Then, as the polynemes for deer and gun gain strength, they will weaken and suppress the competing nemes for money and for wagering \u2014 and that will weaken, in turn, the other polynemes that support the alternative context of making bets. The end effect of this will be almost instantaneous. In only a few cycles of the meaning ring, the agents associated with deer and gun will completely suppress their competitors.",
    "type": "article",
    "title": "20.2 negotiating ambiguity",
    "tags": [
      {
        "score": 0.5782094597816467,
        "sentiment": 0.433,
        "count": 2,
        "label": "astronomer",
        "uri": "https://diffbot.com/entity/XT3kF9aSQMVqkzILKL1JgZw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession"
        ]
      }
    ],
    "docId": 91614282166,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 202930258317,
    "gburl": "http://aurellem.org/society-of-mind/som-20.2.html-diffbotxyz1156970318",
    "lastCrawlTimeUTC": 1588762158,
    "timestamp": "Wed, 06 May 2020 10:49:18 GMT"
  },
  {
    "sentiment": 0.63,
    "humanLanguage": "en",
    "diffbotUri": "article|3|477892542",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.5.html",
    "html": "<p>There is an old and popular idea that we learn only what we are rewarded for. Some psychologists have claimed that human learning is based entirely on <em>reinforcement</em> by reward: that even when we train ourselves with no external inducements, we are still learning from reward &mdash; only now in the form of signals from inside ourselves. But we cannot trust an argument that assumes what it purports to prove, and in any case, when we try to use this idea to explain how people learn to solve hard problems, we encounter a deadly circularity. You first must be able to do something before you can be rewarded for doing it!</p>\n<p>This circularity was no great problem when Ivan Pavlov studied conditioned reflexes nearly a century ago, because in his experiments the animals never needed to produce new kinds of behavior; they only had to link new stimuli to old behaviors. Decades later, Pavlov's research was extended by the Harvard psychologist B. F. Skinner,</p>\n<p>who recognized that higher animals did indeed sometimes exhibit new forms of behavior, which he called <em>operants.</em> Skinner's experiments confirmed that when a certain operant is followed by a reward, it is likely to reappear more frequently on later occasions. He also discovered that this kind of learning has much larger effects if the animal cannot predict when it will be rewarded. Under names like <em>operant conditioning</em> and <em>behavior modification,</em> Skinner's discoveries had a wide influence in psychology and education, but never led to explaining how brains produce new operants. Further- more, few of these animal experiments shed much light on how humans learn to form and carry out their complex plans; the trouble is that other animals can scarcely learn such things at all. Those twin ideas &mdash; reward/success and punish/failure &mdash; do not explain enough about how people learn to produce the new ideas that enable them to solve difficult problems that could not otherwise be solved without many lifetimes of ineffectual trial and error.</p>\n<p>The answer must lie in learning better ways to learn. In order to discuss these things, we'll have to start by using many ordinary words like goal reward, learning, thinking, recognizing, liking, wanting,</p>\n<p>imagining, and remembering &mdash; all based on old and vague ideas. We'll find that most such words must be replaced by new distinctions and ideas. Still, there's something common to them all: in order to solve any hard problem, we must use various kinds of memories. At each moment, we must keep track of what we've just done &mdash; or else we might repeat the same steps over and over again. Also, we must somehow maintain our goals &mdash; or we'll end up doing pointless things. Finally, once our problem is solved, we need access to records of how it was done, for use when similar problems arise in the future.</p>\n<p>Much of this book will be concerned with memory &mdash; that is, with records of the mental past. Why, how, and when should such records be made? When the human brain solves a hard problem, many millions of agents and processes are involved. Which agents could be wise enough to guess what changes should then be made? The high-level agents can't know such things; they scarcely know which lower-level processes exist. Nor can lower-level agents know which of their actions helped us to reach our high-level goals; they scarcely know that higher-level goals exist. The agencies that move our legs aren't concerned with whether we are walking toward home or toward work &mdash; nor do the agents involved with such destinations know anything of controlling individual muscle units. Where in the mind are judgments made about which agents merit praise or blame?</p>",
    "text": "There is an old and popular idea that we learn only what we are rewarded for. Some psychologists have claimed that human learning is based entirely on reinforcement by reward: that even when we train ourselves with no external inducements, we are still learning from reward \u2014 only now in the form of signals from inside ourselves. But we cannot trust an argument that assumes what it purports to prove, and in any case, when we try to use this idea to explain how people learn to solve hard problems, we encounter a deadly circularity. You first must be able to do something before you can be rewarded for doing it!\nThis circularity was no great problem when Ivan Pavlov studied conditioned reflexes nearly a century ago, because in his experiments the animals never needed to produce new kinds of behavior; they only had to link new stimuli to old behaviors. Decades later, Pavlov's research was extended by the Harvard psychologist B. F. Skinner,\nwho recognized that higher animals did indeed sometimes exhibit new forms of behavior, which he called operants. Skinner's experiments confirmed that when a certain operant is followed by a reward, it is likely to reappear more frequently on later occasions. He also discovered that this kind of learning has much larger effects if the animal cannot predict when it will be rewarded. Under names like operant conditioning and behavior modification, Skinner's discoveries had a wide influence in psychology and education, but never led to explaining how brains produce new operants. Further- more, few of these animal experiments shed much light on how humans learn to form and carry out their complex plans; the trouble is that other animals can scarcely learn such things at all. Those twin ideas \u2014 reward/success and punish/failure \u2014 do not explain enough about how people learn to produce the new ideas that enable them to solve difficult problems that could not otherwise be solved without many lifetimes of ineffectual trial and error.\nThe answer must lie in learning better ways to learn. In order to discuss these things, we'll have to start by using many ordinary words like goal reward, learning, thinking, recognizing, liking, wanting,\nimagining, and remembering \u2014 all based on old and vague ideas. We'll find that most such words must be replaced by new distinctions and ideas. Still, there's something common to them all: in order to solve any hard problem, we must use various kinds of memories. At each moment, we must keep track of what we've just done \u2014 or else we might repeat the same steps over and over again. Also, we must somehow maintain our goals \u2014 or we'll end up doing pointless things. Finally, once our problem is solved, we need access to records of how it was done, for use when similar problems arise in the future.\nMuch of this book will be concerned with memory \u2014 that is, with records of the mental past. Why, how, and when should such records be made? When the human brain solves a hard problem, many millions of agents and processes are involved. Which agents could be wise enough to guess what changes should then be made? The high-level agents can't know such things; they scarcely know which lower-level processes exist. Nor can lower-level agents know which of their actions helped us to reach our high-level goals; they scarcely know that higher-level goals exist. The agencies that move our legs aren't concerned with whether we are walking toward home or toward work \u2014 nor do the agents involved with such destinations know anything of controlling individual muscle units. Where in the mind are judgments made about which agents merit praise or blame?",
    "type": "article",
    "title": "7.5 learning and memory",
    "docId": 137429664171,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 136535867798,
    "gburl": "http://aurellem.org/society-of-mind/som-7.5.html-diffbotxyz3470918590",
    "lastCrawlTimeUTC": 1588762101,
    "timestamp": "Wed, 06 May 2020 10:48:21 GMT"
  },
  {
    "sentiment": -0.156,
    "images": [
      {
        "naturalHeight": 152,
        "width": 327,
        "diffbotUri": "image|3|253320573",
        "url": "http://aurellem.org/society-of-mind/illus/ch25/25-1.png",
        "naturalWidth": 327,
        "primary": true,
        "height": 152
      },
      {
        "naturalHeight": 105,
        "width": 297,
        "diffbotUri": "image|3|254244094",
        "url": "http://aurellem.org/society-of-mind/illus/ch25/25-2.png",
        "naturalWidth": 297,
        "height": 105
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-233292102",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-25.1.html",
    "html": "<p>Each of the drawings below can be seen in at least two different ways.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch25/25-1.png\"/></figure>\n<p>The drawing on the left might represent either a single candlestick or two people facing each other. The drawing on the right looks like a cube &mdash; but first it looks like a cube as seen from above and then, suddenly, it looks like a cube as seen from below. Why does each drawing seem to change its character from time to time? Why can't we see both forms at once? Because, it seems, our agencies can tolerate just one interpretation at a time.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch25/25-2.png\"/></figure>\n<p>We must ask certain questions here. First, what enables us to see those pictures as composed of the features we call by names like edges, lines, corners, and areas? Our vision-systems seem virtually compelled to group the outputs of our sensors into entities like these. Next, what enables us to see those features as grouped together to form larger objects? Apparently, our vision-systems again are virtually compelled to represent each of those features, be it a corner, edge, or area, as belonging to one and only one larger object at a time. I won't discuss those questions in this book except to suggest a general hypothesis:</p>\n<p>Our vision-systems are born equipped, on each of several different levels, with some sort of <em>locking-in</em> machinery that at every moment permits each <em>part,</em> at each level, to be assigned to one and only one <em>whole</em> at the next level.</p>\n<p>We should also ask, how do we recognize those objects as examples of familiar things like faces, cubes, or candlesticks? And again we'll make the similar hypothesis that our memory-frame machinery also uses <em>locking-in</em> machinery that permits each <em>object</em> to be attached only to one frame at a time. The end result is that in every region of the picture, the frames must compete with one another to account for each feature.</p>",
    "text": "Each of the drawings below can be seen in at least two different ways.\nThe drawing on the left might represent either a single candlestick or two people facing each other. The drawing on the right looks like a cube \u2014 but first it looks like a cube as seen from above and then, suddenly, it looks like a cube as seen from below. Why does each drawing seem to change its character from time to time? Why can't we see both forms at once? Because, it seems, our agencies can tolerate just one interpretation at a time.\nWe must ask certain questions here. First, what enables us to see those pictures as composed of the features we call by names like edges, lines, corners, and areas? Our vision-systems seem virtually compelled to group the outputs of our sensors into entities like these. Next, what enables us to see those features as grouped together to form larger objects? Apparently, our vision-systems again are virtually compelled to represent each of those features, be it a corner, edge, or area, as belonging to one and only one larger object at a time. I won't discuss those questions in this book except to suggest a general hypothesis:\nOur vision-systems are born equipped, on each of several different levels, with some sort of locking-in machinery that at every moment permits each part, at each level, to be assigned to one and only one whole at the next level.\nWe should also ask, how do we recognize those objects as examples of familiar things like faces, cubes, or candlesticks? And again we'll make the similar hypothesis that our memory-frame machinery also uses locking-in machinery that permits each object to be attached only to one frame at a time. The end result is that in every region of the picture, the frames must compete with one another to account for each feature.",
    "type": "article",
    "title": "25.1 one frame at a time?",
    "tags": [
      {
        "score": 0.5813809037208557,
        "sentiment": 0,
        "count": 1,
        "label": "As Seen from Above",
        "uri": "https://diffbot.com/entity/XKQrDUB-rPB-IoJLM9MXVsA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 233973547449,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 40782856604,
    "gburl": "http://aurellem.org/society-of-mind/som-25.1.html-diffbotxyz3462137674",
    "lastCrawlTimeUTC": 1588762127,
    "timestamp": "Wed, 06 May 2020 10:48:47 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1998295879",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.2.html",
    "html": "<p>What do we mean by words like <em>sentience,</em> <em>consciousness,</em> or <em>self-awareness</em>? They all seem to refer to the sense of feeling one's mind at work &mdash; but beyond that, it is hard to say whether there are any differences in what they mean. For instance, suppose that you had just smiled, and someone asked if you had been conscious of this. It would scarcely matter how that question was posed:</p>\n<p><em>Did you just smile?</em> <em>Do you realize that you just smiled?</em> <em>Do you remember smiling?</em> <em>Were you conscious of doing so?</em> <em>Were you aware of it?</em></p>\n<p>Each of these questions really asks what you can say about your recent mental past. In order for you to reply truthfully, <em>Yes, I know I smiled,</em> your speaking-agencies must use some records about the recent activity of certain agents. But what about all the other activities involved in everything you say and do? If you were truly self-aware, wouldn't you know all those other things as well? There is a common myth that what we view as consciousness is measurelessly deep and powerful &mdash; yet actually, we scarcely know a thing about what happens in the great computers of our brains. How can we think, not knowing what it is to think? How can we get such good ideas, yet not be able to say what ideas are or how they're made?</p>\n<p>Why is it so hard to talk about our present state of mind? We've already seen several reasons for this. one is that the time-delays between the different parts of a mind mean that the concept of a <em>present state</em> is not psychologically sound. Another reason is that each attempt to reflect upon our mental state will change that state, and this means that trying to know our state is like photographing something that is moving too fast: such pictures will always be blurred. In any case, we aren't much concerned in the first place with learning how to describe our mental states; instead, we're more engaged with practical things, like making plans and carrying them out.</p>\n<p>How much genuine self-insight is possible for us? I'm sure our memory-machinery provides some useful clues, if only we could learn to interpret them. But it is unlikely that any part of the mind can ever obtain complete descriptions of what happens in the other parts, because, it seems, our memory-control systems have too little temporary memory even to represent their own activities in much detail.</p>",
    "text": "What do we mean by words like sentience, consciousness, or self-awareness? They all seem to refer to the sense of feeling one's mind at work \u2014 but beyond that, it is hard to say whether there are any differences in what they mean. For instance, suppose that you had just smiled, and someone asked if you had been conscious of this. It would scarcely matter how that question was posed:\nDid you just smile? Do you realize that you just smiled? Do you remember smiling? Were you conscious of doing so? Were you aware of it?\nEach of these questions really asks what you can say about your recent mental past. In order for you to reply truthfully, Yes, I know I smiled, your speaking-agencies must use some records about the recent activity of certain agents. But what about all the other activities involved in everything you say and do? If you were truly self-aware, wouldn't you know all those other things as well? There is a common myth that what we view as consciousness is measurelessly deep and powerful \u2014 yet actually, we scarcely know a thing about what happens in the great computers of our brains. How can we think, not knowing what it is to think? How can we get such good ideas, yet not be able to say what ideas are or how they're made?\nWhy is it so hard to talk about our present state of mind? We've already seen several reasons for this. one is that the time-delays between the different parts of a mind mean that the concept of a present state is not psychologically sound. Another reason is that each attempt to reflect upon our mental state will change that state, and this means that trying to know our state is like photographing something that is moving too fast: such pictures will always be blurred. In any case, we aren't much concerned in the first place with learning how to describe our mental states; instead, we're more engaged with practical things, like making plans and carrying them out.\nHow much genuine self-insight is possible for us? I'm sure our memory-machinery provides some useful clues, if only we could learn to interpret them. But it is unlikely that any part of the mind can ever obtain complete descriptions of what happens in the other parts, because, it seems, our memory-control systems have too little temporary memory even to represent their own activities in much detail.",
    "type": "article",
    "title": "15.2 self-examination",
    "tags": [
      {
        "score": 0.6566241979598999,
        "sentiment": 0,
        "count": 0,
        "label": "test  (assessment)",
        "uri": "https://diffbot.com/entity/XLrHiBgiYMbCGxx8RZt44mw"
      }
    ],
    "docId": 272712335800,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 143836381580,
    "gburl": "http://aurellem.org/society-of-mind/som-15.2.html-diffbotxyz1507093228",
    "lastCrawlTimeUTC": 1588762073,
    "timestamp": "Wed, 06 May 2020 10:47:53 GMT"
  },
  {
    "sentiment": 0.873,
    "images": [
      {
        "naturalHeight": 198,
        "width": 404,
        "diffbotUri": "image|3|-1909917566",
        "url": "http://aurellem.org/society-of-mind/illus/ch18/18-6.png",
        "naturalWidth": 404,
        "primary": true,
        "height": 198
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-504038440",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.4.html",
    "html": "<h2><em>18.4</em> logical chains</h2>\n<p><em>Logic</em> is the word we use for certain ways to chain ideas. But I doubt that pure deductive logic plays much of a role in ordinary thinking. Here is one way to contrast logical reasoning and ordinary thinking. Both build chainlike connections between ideas. The difference is that in logic there's no middle ground; a logic link is either there or not. Because of this, a logical argument cannot have any <em>weakest link.</em></p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch18/18-6.png\"/></figure>\n<p>Logic demands just one support for every link, a single, flawless deduction. Common sense asks, at every step, if all of what we've found so far is in accord with everyday experience. No sensible person ever trusts a long, thin chain of reasoning. In real life, when we listen to an argument, we do not merely check each separate step;</p>\n<p>we look to see if what has been described so far seems plausible. We look for other evidence beyond the reasons in that argument. Consider how often we speak of reasoning in terms of structural or architectural expressions, as though our arguments were like the towers Builder builds:</p>\n<p><em>Your argument is based on weak evidence.</em> <em>You must support that with more evidence.</em> <em>That argument is shaky. It will collapse.</em></p>\n<p>In this way, commonsense reasoning differs from <em>logical</em> reasoning. When an ordinary argument seems weak, we may be able to support it with more evidence. But there is no way for a link inside a logic chain to use additional support; if it's not quite right, then it's absolutely wrong. Indeed, this weakness is actually the source of logic's own peculiar strength, because the less we base our conclusions on, the fewer possibilities can exist for weaknesses in our arguments! This strategy serves mathematics well &mdash; but it doesn't help us much in dealing with uncertainties. We cannot afford to stake our lives on chains that fall apart so easily.</p>\n<p>I do not mean to say that there is anything wrong with logic; I only object to the assumption that ordinary reasoning is largely based on it. What, then, are the functions of logic? It rarely helps us get a new idea, but it often helps us to detect the weaknesses in old ideas. Sometimes it also helps us clarify our thoughts by refining messy networks into simpler chains. Thus, once we find a way to solve a certain problem, logical analysis can help us find the most essential steps. Then it becomes easier to explain what we've discovered to other people &mdash; and, also, we often benefit from explaining our ideas to ourselves. This is because, more often than not, instead of explaining what we actually did, we come up with a new formulation. Paradoxically, the moments in which we think we're being logical and methodical can be just the times at which we're most creative and original.</p>",
    "text": "18.4 logical chains\nLogic is the word we use for certain ways to chain ideas. But I doubt that pure deductive logic plays much of a role in ordinary thinking. Here is one way to contrast logical reasoning and ordinary thinking. Both build chainlike connections between ideas. The difference is that in logic there's no middle ground; a logic link is either there or not. Because of this, a logical argument cannot have any weakest link.\nLogic demands just one support for every link, a single, flawless deduction. Common sense asks, at every step, if all of what we've found so far is in accord with everyday experience. No sensible person ever trusts a long, thin chain of reasoning. In real life, when we listen to an argument, we do not merely check each separate step;\nwe look to see if what has been described so far seems plausible. We look for other evidence beyond the reasons in that argument. Consider how often we speak of reasoning in terms of structural or architectural expressions, as though our arguments were like the towers Builder builds:\nYour argument is based on weak evidence. You must support that with more evidence. That argument is shaky. It will collapse.\nIn this way, commonsense reasoning differs from logical reasoning. When an ordinary argument seems weak, we may be able to support it with more evidence. But there is no way for a link inside a logic chain to use additional support; if it's not quite right, then it's absolutely wrong. Indeed, this weakness is actually the source of logic's own peculiar strength, because the less we base our conclusions on, the fewer possibilities can exist for weaknesses in our arguments! This strategy serves mathematics well \u2014 but it doesn't help us much in dealing with uncertainties. We cannot afford to stake our lives on chains that fall apart so easily.\nI do not mean to say that there is anything wrong with logic; I only object to the assumption that ordinary reasoning is largely based on it. What, then, are the functions of logic? It rarely helps us get a new idea, but it often helps us to detect the weaknesses in old ideas. Sometimes it also helps us clarify our thoughts by refining messy networks into simpler chains. Thus, once we find a way to solve a certain problem, logical analysis can help us find the most essential steps. Then it becomes easier to explain what we've discovered to other people \u2014 and, also, we often benefit from explaining our ideas to ourselves. This is because, more often than not, instead of explaining what we actually did, we come up with a new formulation. Paradoxically, the moments in which we think we're being logical and methodical can be just the times at which we're most creative and original.",
    "type": "article",
    "title": "18.4 logical chains",
    "tags": [
      {
        "score": 0.8403827548027039,
        "sentiment": 0,
        "count": 9,
        "label": "logic",
        "uri": "https://diffbot.com/entity/X2FR1oallObSTFA7yvuGb3g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6751735806465149,
        "sentiment": 0,
        "count": 6,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      }
    ],
    "docId": 30591058323,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 58836255162,
    "gburl": "http://aurellem.org/society-of-mind/som-18.4.html-diffbotxyz476907958",
    "lastCrawlTimeUTC": 1588761964,
    "timestamp": "Wed, 06 May 2020 10:46:04 GMT"
  },
  {
    "sentiment": -0.867,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-515434998",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.1.html",
    "html": "<p>Our child, playing with some blocks and a toy car, happens to build this structure. Let's call it a Block-Arch.</p>\n<p>Block-Arch seems to cause a strange new phenomenon: when you push the car through it, your arm gets trapped! Then, in order to complete that action, you must release the car &mdash; and reach around to the other side of the arch, perhaps by changing hands. The child becomes interested in this <em>Hand-Change</em> phenomenon and wonders how Block-Arch causes it. Soon the child finds another structure that seems similar &mdash; except that Hand-Change disappears because you can't even push the car through it. Yet both structures fit the same description!</p>\n<p>But if Block-Arch causes Hand-Change, then this can't be a block-arch. So the child must find some way to change the mental description of Block-Arch so it won't apply to this. What is the difference between them? Perhaps this is because those standing blocks now touch one another, when they didn't touch before. We could adapt to this by changing our description of Block-Arch: <em>There must be two standing blocks and a lying block. The standing blocks must not touch.</em> But even this does not suffice, because the child soon finds yet another structure that matches this description. Here, too, the Hand-Change phenomenon has disappeared; now you can push the car through it without letting go!</p>\n<p>Again we must change our description to keep this from being considered a Block-Arch. Finally the child discovers another variation that does produce Hand-Change:</p>\n<p>Our child has constructed for itself a useful conception of an arch, based entirely upon its own experience.</p>",
    "text": "Our child, playing with some blocks and a toy car, happens to build this structure. Let's call it a Block-Arch.\nBlock-Arch seems to cause a strange new phenomenon: when you push the car through it, your arm gets trapped! Then, in order to complete that action, you must release the car \u2014 and reach around to the other side of the arch, perhaps by changing hands. The child becomes interested in this Hand-Change phenomenon and wonders how Block-Arch causes it. Soon the child finds another structure that seems similar \u2014 except that Hand-Change disappears because you can't even push the car through it. Yet both structures fit the same description!\nBut if Block-Arch causes Hand-Change, then this can't be a block-arch. So the child must find some way to change the mental description of Block-Arch so it won't apply to this. What is the difference between them? Perhaps this is because those standing blocks now touch one another, when they didn't touch before. We could adapt to this by changing our description of Block-Arch: There must be two standing blocks and a lying block. The standing blocks must not touch. But even this does not suffice, because the child soon finds yet another structure that matches this description. Here, too, the Hand-Change phenomenon has disappeared; now you can push the car through it without letting go!\nAgain we must change our description to keep this from being considered a Block-Arch. Finally the child discovers another variation that does produce Hand-Change:\nOur child has constructed for itself a useful conception of an arch, based entirely upon its own experience.",
    "type": "article",
    "title": "12.1 a block-arch scenario",
    "tags": [
      {
        "score": 0.871719479560852,
        "sentiment": 0,
        "count": 0,
        "label": "Arch",
        "uri": "https://diffbot.com/entity/Aj_olvZ7NOoi5_m7SLYR7UA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Settlement"
        ]
      },
      {
        "score": 0.7623432874679565,
        "sentiment": 0,
        "count": 0,
        "label": "Gateway Arch",
        "uri": "https://diffbot.com/entity/Li4ey-yQzN56wwgpdmAbFoA",
        "rdfTypes": ["http://dbpedia.org/ontology/Monument"]
      },
      {
        "score": 0.687735378742218,
        "sentiment": 0,
        "count": 0,
        "label": "Block Entertainment",
        "uri": "https://diffbot.com/entity/BWRd5DB_ONZmauoRna9Jkcw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company",
          "http://dbpedia.org/ontology/RecordLabel"
        ]
      },
      {
        "score": 0.6145540475845337,
        "sentiment": -0.552,
        "count": 7,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 61962715526,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 133014438283,
    "gburl": "http://aurellem.org/society-of-mind/som-12.1.html-diffbotxyz3902718795",
    "lastCrawlTimeUTC": 1588761940,
    "timestamp": "Wed, 06 May 2020 10:45:40 GMT"
  },
  {
    "sentiment": -0.842,
    "humanLanguage": "en",
    "diffbotUri": "article|3|987056383",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.7.html",
    "html": "<p>There are important variations on the theme of <em>weighing evidence.</em> Our first idea was just to count the bits of evidence in favor of an object's being a chair. But not all bits of evidence are equally valuable, so we can improve our scheme by giving different <em>weights</em> to different kinds of evidence.</p>\n<p>How could we prevent this chair recognizer from accepting a table that appeared to be composed of four legs and a seat? One approach would be to try to rearrange the weights to avoid this. But if we already possessed a table recognizer, we could use its output as evidence against there being a chair by adding it in with a negative weight! How should one decide what number weights to assign to each feature? In 1959, Frank Rosenblatt invented an ingenious evidence-weighing machine called a <em>Perceptron.</em> It was equipped with a procedure that automatically learned which weights to use from being told by a teacher which of the distinctions it made were unacceptable.</p>\n<p>All feature-weighing machines have serious limitations because, although they can measure the presence or absence of various features, they cannot take into account enough of the relations among those features. For example, in the book Perceptrons, Seymour Papert and I proved mathematically that no feature-weighing machine can distinguish between the two kinds of patterns drawn below &mdash; no matter how cleverly we choose the weights.</p>\n<p>Both drawings on the left depict patterns that are connected &mdash; that is, that can be drawn with a single line. The patterns on the right are disconnected in the sense of needing two separate lines. Here is a way to prove that no feature-weighing machine can recognize this sort of distinction. Suppose that you chopped each picture into a heap of tiny pieces. It would be impossible to say which heaps came from connected drawings and which came from disconnected drawings &mdash; simply because each heap would contain identical assortments of picture fragments! Every heap would contain exactly four pictures of right-angle turns, two <em>line endings,</em> and the same total lengths of horizontal and vertical line segments. It is therefore impossible to distinguish one heap from another by <em>adding up the evidence,</em> because all information about the relations between the bits of evidence has been lost.</p>",
    "text": "There are important variations on the theme of weighing evidence. Our first idea was just to count the bits of evidence in favor of an object's being a chair. But not all bits of evidence are equally valuable, so we can improve our scheme by giving different weights to different kinds of evidence.\nHow could we prevent this chair recognizer from accepting a table that appeared to be composed of four legs and a seat? One approach would be to try to rearrange the weights to avoid this. But if we already possessed a table recognizer, we could use its output as evidence against there being a chair by adding it in with a negative weight! How should one decide what number weights to assign to each feature? In 1959, Frank Rosenblatt invented an ingenious evidence-weighing machine called a Perceptron. It was equipped with a procedure that automatically learned which weights to use from being told by a teacher which of the distinctions it made were unacceptable.\nAll feature-weighing machines have serious limitations because, although they can measure the presence or absence of various features, they cannot take into account enough of the relations among those features. For example, in the book Perceptrons, Seymour Papert and I proved mathematically that no feature-weighing machine can distinguish between the two kinds of patterns drawn below \u2014 no matter how cleverly we choose the weights.\nBoth drawings on the left depict patterns that are connected \u2014 that is, that can be drawn with a single line. The patterns on the right are disconnected in the sense of needing two separate lines. Here is a way to prove that no feature-weighing machine can recognize this sort of distinction. Suppose that you chopped each picture into a heap of tiny pieces. It would be impossible to say which heaps came from connected drawings and which came from disconnected drawings \u2014 simply because each heap would contain identical assortments of picture fragments! Every heap would contain exactly four pictures of right-angle turns, two line endings, and the same total lengths of horizontal and vertical line segments. It is therefore impossible to distinguish one heap from another by adding up the evidence, because all information about the relations between the bits of evidence has been lost.",
    "type": "article",
    "title": "19.7 weighing evidence",
    "tags": [
      {
        "score": 0.7614810466766357,
        "sentiment": -0.27,
        "count": 5,
        "label": "evidence",
        "uri": "https://diffbot.com/entity/X6gaMuoO9OUWvHWkFlelruA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill",
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject"
        ]
      },
      {
        "score": 0.586962103843689,
        "sentiment": 0,
        "count": 1,
        "label": "Frank Rosenblatt",
        "uri": "https://diffbot.com/entity/Pbw8-CaIRNwGEAOvoOA0ZZw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5614656209945679,
        "sentiment": 0,
        "count": 1,
        "label": "Seymour Papert",
        "uri": "https://diffbot.com/entity/P8Ue3ofn7MP-xCyDxpkXe0A",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 153115722152,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 235530158483,
    "gburl": "http://aurellem.org/society-of-mind/som-19.7.html-diffbotxyz3974041302",
    "lastCrawlTimeUTC": 1588761979,
    "timestamp": "Wed, 06 May 2020 10:46:19 GMT"
  },
  {
    "sentiment": 0.867,
    "humanLanguage": "en",
    "diffbotUri": "article|3|667508447",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.8.html",
    "html": "<blockquote>Guilt is the gift that keeps on giving. &mdash;Jewish proverb </blockquote>\n<p>All people talk of goals and dreams, of personal priorities, of goods and bads, rights and wrongs, virtues and depravities. What makes our ethics and ideals develop in our children's minds?</p>\n<p>In one of the theories of Sigmund Freud, an infant becomes enamored of one or both parents, and somehow this leads the baby into absorbing or, as Freud put it, <em>introjecting</em> the goals and values of those love-objects. Thenceforth, throughout later life, those parent-images persist inside the grown-up child's mind, to influence whatever thoughts and goals are considered worthy of pursuit. We are not compelled to agree with all of Freud's account, but we have to explain why children develop models of their parents' values at all.</p>\n<p>So far as the child's safety is concerned, it would suffice for attachment to keep the child in the parents' physical vicinity. What could be the biological and psychological functions of developing complicated self-ideals?</p>\n<p>The answer seems quite clear to me. Consider that our models of ourselves are so complex that even adults can't explain them. How could a fragmentary infant mind know enough to build such a complicated thing &mdash; without some model upon which to base it? We aren't horn with built-in Selves &mdash; but most of us are fortunate enough to he born with human caretakers. Then, our attachment mechanisms force us to focus on our parents' ways, and this leads us to build crude images of what those parents themselves are like. That way, the values and goals of a culture pass from one generation to the next. They are not learned the way skills are learned. We learn our earliest values under the influence of attachment-related signals that represent, not our own success or failure, but our parents' love or rejection. When we maintain our standards, we feel virtuous rather than merely successful. When we violate those standards, we feel shame and guilt rather than mere disappointment. This is not just a matter of words: those things are not the same; it is like the difference between ends and means.</p>\n<p>How could coherence be imposed upon a multitude of mindless agencies? Freud may have been the first to see that this could emerge from the effects of infant attachment. It was several more decades before psychologists recognized that separating children from their attachments can have devastating effects on the growth of their personalities. Freud also observed that children frequently reject one parent in favor of the other, in a process that suggests the cross-exclusiveness of sexual jealousy; he called this the Oedipus complex. It seems plausible that something of this sort ought to happen regardless of any connection between attachment and sexuality. If a developing identity is based upon that of another person, it must become confusing to be attached to two dissimilar adult <em>models.</em> This might lead a child to try to simplify the situation by rejecting or removing one of them from the scene.</p>\n<p>Many people dislike the thought of being dominated from within by the image of a parent's wish. Yet, in exchange, that slavery is just what makes us relatively free (as compared with other animals) from being forced to obey so many other kinds of unlearned, built-in instinct-goals.</p>",
    "text": "Guilt is the gift that keeps on giving. \u2014Jewish proverb\nAll people talk of goals and dreams, of personal priorities, of goods and bads, rights and wrongs, virtues and depravities. What makes our ethics and ideals develop in our children's minds?\nIn one of the theories of Sigmund Freud, an infant becomes enamored of one or both parents, and somehow this leads the baby into absorbing or, as Freud put it, introjecting the goals and values of those love-objects. Thenceforth, throughout later life, those parent-images persist inside the grown-up child's mind, to influence whatever thoughts and goals are considered worthy of pursuit. We are not compelled to agree with all of Freud's account, but we have to explain why children develop models of their parents' values at all.\nSo far as the child's safety is concerned, it would suffice for attachment to keep the child in the parents' physical vicinity. What could be the biological and psychological functions of developing complicated self-ideals?\nThe answer seems quite clear to me. Consider that our models of ourselves are so complex that even adults can't explain them. How could a fragmentary infant mind know enough to build such a complicated thing \u2014 without some model upon which to base it? We aren't horn with built-in Selves \u2014 but most of us are fortunate enough to he born with human caretakers. Then, our attachment mechanisms force us to focus on our parents' ways, and this leads us to build crude images of what those parents themselves are like. That way, the values and goals of a culture pass from one generation to the next. They are not learned the way skills are learned. We learn our earliest values under the influence of attachment-related signals that represent, not our own success or failure, but our parents' love or rejection. When we maintain our standards, we feel virtuous rather than merely successful. When we violate those standards, we feel shame and guilt rather than mere disappointment. This is not just a matter of words: those things are not the same; it is like the difference between ends and means.\nHow could coherence be imposed upon a multitude of mindless agencies? Freud may have been the first to see that this could emerge from the effects of infant attachment. It was several more decades before psychologists recognized that separating children from their attachments can have devastating effects on the growth of their personalities. Freud also observed that children frequently reject one parent in favor of the other, in a process that suggests the cross-exclusiveness of sexual jealousy; he called this the Oedipus complex. It seems plausible that something of this sort ought to happen regardless of any connection between attachment and sexuality. If a developing identity is based upon that of another person, it must become confusing to be attached to two dissimilar adult models. This might lead a child to try to simplify the situation by rejecting or removing one of them from the scene.\nMany people dislike the thought of being dominated from within by the image of a parent's wish. Yet, in exchange, that slavery is just what makes us relatively free (as compared with other animals) from being forced to obey so many other kinds of unlearned, built-in instinct-goals.",
    "type": "article",
    "title": "17.8 attachment-images",
    "tags": [
      {
        "score": 0.7811416387557983,
        "sentiment": 0.681,
        "count": 5,
        "label": "Sigmund Freud",
        "uri": "https://diffbot.com/entity/PuzeA7B1wN96x624n0l3q2Q",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.6478283405303955,
        "sentiment": 0.69,
        "count": 3,
        "label": "baby",
        "uri": "https://diffbot.com/entity/X4MZ_yLLbO0ueNL-zoshUhg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.532717764377594,
        "sentiment": 0.98,
        "count": 1,
        "label": "Jewish people",
        "uri": "https://diffbot.com/entity/XULyXZa1PMF-RNN4b1T_Vyg",
        "rdfTypes": ["http://dbpedia.org/ontology/EthnicGroup"]
      },
      {
        "score": 0.5161908864974976,
        "sentiment": 0.921,
        "count": 7,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 161056407937,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 8249934246,
    "gburl": "http://aurellem.org/society-of-mind/som-17.8.html-diffbotxyz4198697421",
    "lastCrawlTimeUTC": 1588762010,
    "timestamp": "Wed, 06 May 2020 10:46:50 GMT"
  },
  {
    "sentiment": 0.302,
    "humanLanguage": "en",
    "diffbotUri": "article|3|908818154",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-4.7.html",
    "html": "<p>We often become involved in projects that we can't complete. It is easy to solve small problems because we can treat them as though they were detached from all our other goals. But it is different for projects that span larger portions of our lives, like learning a trade, raising a child, or writing a book. We cannot simply <em>decide</em> or <em>choose</em> to accomplish an enterprise that makes a large demand for time, because it will inevitably conflict with other interests and ambitions. Then we'll be forced to ask questions like these:</p>\n<p>What must I give up for this? What will I learn from it? Will it bring power and influence? Will I remain interested in it? Will other people help me with it? Will they still like me?</p>\n<p>Perhaps the most difficult question of all is, <em>How will adopting this goal change me?</em> Just wanting to own a large, expensive house, for instance, can lead to elaborate thoughts like these:</p>\n<p><em>That means I'd have to save for years and not get other things I'd like. I doubt that I could bear it. True, I could reform myself, and try to be more thrifty and deliberate. But that's just not the sort of person I am.</em></p>\n<p>Until such doubts are set aside, all the plans we make will be subject to the danger that we may <em>change our mind.</em> So how can any long-range plan succeed? The easiest path to <em>self-control</em> is doing only what one is already disposed to do.</p>\n<p>Many of the schemes we use for self-control are the same as those we learn to use for influencing other people. We make ourselves behave by exploiting our own fears and desires, offering ourselves rewards, or threatening the loss of what we love. But when short-range tricks won't keep us to our projects for long enough, we may need some way to make changes that won't let us change ourselves back again. I suspect that, in order to commit ourselves to our largest, most ambitious plans, we learn to exploit agencies that operate on larger spans of time.</p>\n<p>Which are our slowest-changing agencies of all? Later we'll see that these must include the silent, hidden agencies that shape what we call character. These are the systems that are concerned not merely with the things we want, but with what we want ourselves to be &mdash; that is, the ideals we set for ourselves.</p>",
    "text": "We often become involved in projects that we can't complete. It is easy to solve small problems because we can treat them as though they were detached from all our other goals. But it is different for projects that span larger portions of our lives, like learning a trade, raising a child, or writing a book. We cannot simply decide or choose to accomplish an enterprise that makes a large demand for time, because it will inevitably conflict with other interests and ambitions. Then we'll be forced to ask questions like these:\nWhat must I give up for this? What will I learn from it? Will it bring power and influence? Will I remain interested in it? Will other people help me with it? Will they still like me?\nPerhaps the most difficult question of all is, How will adopting this goal change me? Just wanting to own a large, expensive house, for instance, can lead to elaborate thoughts like these:\nThat means I'd have to save for years and not get other things I'd like. I doubt that I could bear it. True, I could reform myself, and try to be more thrifty and deliberate. But that's just not the sort of person I am.\nUntil such doubts are set aside, all the plans we make will be subject to the danger that we may change our mind. So how can any long-range plan succeed? The easiest path to self-control is doing only what one is already disposed to do.\nMany of the schemes we use for self-control are the same as those we learn to use for influencing other people. We make ourselves behave by exploiting our own fears and desires, offering ourselves rewards, or threatening the loss of what we love. But when short-range tricks won't keep us to our projects for long enough, we may need some way to make changes that won't let us change ourselves back again. I suspect that, in order to commit ourselves to our largest, most ambitious plans, we learn to exploit agencies that operate on larger spans of time.\nWhich are our slowest-changing agencies of all? Later we'll see that these must include the silent, hidden agencies that shape what we call character. These are the systems that are concerned not merely with the things we want, but with what we want ourselves to be \u2014 that is, the ideals we set for ourselves.",
    "type": "article",
    "title": "4.7 long-range plans",
    "docId": 256808239535,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 22582264228,
    "gburl": "http://aurellem.org/society-of-mind/som-4.7.html-diffbotxyz3006790021",
    "lastCrawlTimeUTC": 1588762040,
    "timestamp": "Wed, 06 May 2020 10:47:20 GMT"
  },
  {
    "sentiment": 0.789,
    "humanLanguage": "en",
    "diffbotUri": "article|3|546129420",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-10.1.html",
    "html": "<p>The psychologist Jean Piaget was one of the first to realize that watching children might be a way to see how mind-societies grow. In one of his classic experiments, he showed a child two matching sets of eggs and cups &mdash; and asked, <em>Are there more eggs or more egg cups?</em></p>\n<p>Then he spread the eggs apart &mdash; before the child's eyes &mdash; and asked again if there were more eggs or more egg cups.</p>\n<p>One might try to explain this by supposing that older children are better at counting. However, this can't explain another famous experiment of Piaget's, which began by showing three jars, two filled with water. All the children agreed that the two short, wide jars contained equal amounts of liquid. Then, before their eyes, he poured all the liquid from one of the short jars into the tall, thin one and asked which jar had more liquid now.</p>\n<p>These experiments have been repeated in many ways and in many countries &mdash; and always with the same results: each normal child eventually acquires an adult view of quantity &mdash; apparently without adult help! The age at which this happens may vary, but the process itself seems so universal that one cannot help suspecting that it reflects some fundamental aspect of the child's development. In the next few sections we'll examine the idea of <em>more</em> and show that it conceals the workings of a large, complex Society-of-More &mdash; which takes many years to learn.</p>",
    "text": "The psychologist Jean Piaget was one of the first to realize that watching children might be a way to see how mind-societies grow. In one of his classic experiments, he showed a child two matching sets of eggs and cups \u2014 and asked, Are there more eggs or more egg cups?\nThen he spread the eggs apart \u2014 before the child's eyes \u2014 and asked again if there were more eggs or more egg cups.\nOne might try to explain this by supposing that older children are better at counting. However, this can't explain another famous experiment of Piaget's, which began by showing three jars, two filled with water. All the children agreed that the two short, wide jars contained equal amounts of liquid. Then, before their eyes, he poured all the liquid from one of the short jars into the tall, thin one and asked which jar had more liquid now.\nThese experiments have been repeated in many ways and in many countries \u2014 and always with the same results: each normal child eventually acquires an adult view of quantity \u2014 apparently without adult help! The age at which this happens may vary, but the process itself seems so universal that one cannot help suspecting that it reflects some fundamental aspect of the child's development. In the next few sections we'll examine the idea of more and show that it conceals the workings of a large, complex Society-of-More \u2014 which takes many years to learn.",
    "type": "article",
    "title": "10.1 piaget's experiments",
    "tags": [
      {
        "score": 0.8360515236854553,
        "sentiment": 0,
        "count": 2,
        "label": "Jean Piaget",
        "uri": "https://diffbot.com/entity/PDLtioHPGOfSddrmNWrXIYg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5984516143798828,
        "sentiment": 0,
        "count": 0,
        "label": "society",
        "uri": "https://diffbot.com/entity/X5-r2onDFMwqrJxIepxIeQw"
      },
      {
        "score": 0.540367603302002,
        "sentiment": 0,
        "count": 6,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.536941409111023,
        "sentiment": 0,
        "count": 1,
        "label": "psychologist",
        "uri": "https://diffbot.com/entity/XxLyD76oVMbO995xyZw1A8Q",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 9287434654,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 31073370525,
    "gburl": "http://aurellem.org/society-of-mind/som-10.1.html-diffbotxyz1210842001",
    "lastCrawlTimeUTC": 1588761919,
    "timestamp": "Wed, 06 May 2020 10:45:19 GMT"
  },
  {
    "sentiment": 0.933,
    "images": [
      {
        "naturalHeight": 202,
        "width": 425,
        "diffbotUri": "image|3|1838468189",
        "url": "http://aurellem.org/society-of-mind/illus/ch11/11-1.png",
        "naturalWidth": 425,
        "primary": true,
        "height": 202
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1087772227",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.1.html",
    "html": "<p>What possible kind of brain-event could correspond to anything like the meaning of an ordinary word? When you say <em>red,</em> your vocal cords obey commands from <em>pronouncing agents</em> in your brain, which make your chest and larynx muscles move to produce that special sound. These agents must in turn receive commands from somewhere else, where other agents respond to signals from yet other places. All those <em>places</em> must comprise the parts of some society of mental agencies.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch11/11-1.png\"/></figure>\n<p>It's easy to design a machine to tell when there is something red: start with sensors that respond to different hues of light, and connect the ones most sensitive to red to a central <em>red-agent,</em> making corrections for the color of the lighting of the scene. We could make this machine appear to <em>speak</em> by linking each color-agent to a device that pronounces the corresponding word. Then this machine could name the colors it <em>sees</em> &mdash; and even distinguish more hues than ordinary people can. But it would be a travesty to call this <em>sight,</em> since it's nothing but a catalog that lists a lot of colored dots. It would share no human notion of what colors come to mean to us, because without some sense of texture, form, and very much more, it would have few of the qualities of our human kinds of images and thoughts.</p>\n<p>Of course no little diagram can capture more than a fragment of any real person's thoughts about the world. But this should not be taken to mean that no machine could ever have the range of sensibilities that people have. It merely means that we aren't simple machines; indeed, we should understand that in learning to comprehend the qualities of vast machines, we are still in the dark ages. And in any case, a diagram can only illustrate a principle: there cannot be any compact way to represent all the details of full-grown mind-society. To talk about such complex things, we can only resort to language tricks that make our listeners' minds explore the worlds inside themselves.</p>",
    "text": "What possible kind of brain-event could correspond to anything like the meaning of an ordinary word? When you say red, your vocal cords obey commands from pronouncing agents in your brain, which make your chest and larynx muscles move to produce that special sound. These agents must in turn receive commands from somewhere else, where other agents respond to signals from yet other places. All those places must comprise the parts of some society of mental agencies.\nIt's easy to design a machine to tell when there is something red: start with sensors that respond to different hues of light, and connect the ones most sensitive to red to a central red-agent, making corrections for the color of the lighting of the scene. We could make this machine appear to speak by linking each color-agent to a device that pronounces the corresponding word. Then this machine could name the colors it sees \u2014 and even distinguish more hues than ordinary people can. But it would be a travesty to call this sight, since it's nothing but a catalog that lists a lot of colored dots. It would share no human notion of what colors come to mean to us, because without some sense of texture, form, and very much more, it would have few of the qualities of our human kinds of images and thoughts.\nOf course no little diagram can capture more than a fragment of any real person's thoughts about the world. But this should not be taken to mean that no machine could ever have the range of sensibilities that people have. It merely means that we aren't simple machines; indeed, we should understand that in learning to comprehend the qualities of vast machines, we are still in the dark ages. And in any case, a diagram can only illustrate a principle: there cannot be any compact way to represent all the details of full-grown mind-society. To talk about such complex things, we can only resort to language tricks that make our listeners' minds explore the worlds inside themselves.",
    "type": "article",
    "title": "11.1 seeing red",
    "tags": [
      {
        "score": 0.5408483147621155,
        "sentiment": 0.322,
        "count": 1,
        "label": "South Park: Bigger, Longer & Uncut",
        "uri": "https://diffbot.com/entity/XIjOAoBVAPmCfYcqt4MJvGg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      }
    ],
    "docId": 38709068205,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 205921337735,
    "gburl": "http://aurellem.org/society-of-mind/som-11.1.html-diffbotxyz877878133",
    "lastCrawlTimeUTC": 1588761852,
    "timestamp": "Wed, 06 May 2020 10:44:12 GMT"
  },
  {
    "sentiment": 0.506,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1199329850",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-2.2.html",
    "html": "<p>It's always best when mysteries can be explained in terms of things we know. But when we find this hard to do, we must decide whether to keep trying to make old theories work or to discard them and try new ones. I think this is partly a matter of personality. Let's call <em>Reductionists</em> those people who prefer to build on old ideas, and <em>Novelists</em> the ones who like to champion new hypotheses. Reduction- ists are usually right &mdash; at least at science's cautious core, where novelties rarely survive for long. Outside that realm, though, novelists reign, since older ideas have had more time to show their flaws.</p>\n<p>It really is amazing how certain sciences depend upon so few kinds of explanations. The science of physics can now explain virtually everything we see, at least in principle, in terms of how a very few kinds of particles and force-fields interact. Over the past few centuries reductionism has been remarkably successful. What makes it possible to describe so much of the world in terms of so few basic rules? No one knows.</p>\n<p>Many scientists look on chemistry and physics as ideal models of what psychology should be like. After all, the atoms in the brain are subject to the same all-inclusive physical laws that govern every other form of matter. Then can we also explain what our brains actually do entirely in terms of those same basic principles? The answer is no, simply because even if we understood how each of our billions of brain cells works separately, this would not tell us how the brain works as an agency. The <em>laws of thought</em> depend not only upon the properties of those brain cells, but also on how they are connected. And these connections are established not by the basic, general laws of physics, but by the particular arrangements of the millions of bits of information in our inherited genes. To be sure, general laws apply to everything. But, for that very reason, they can rarely explain anything in particular.</p>\n<p>Does this mean that psychology must reject the laws of physics and find its own? Of course not. It is not a matter of different laws, but of additional kinds of theories and principles that operate at higher levels of organization. Our ideas of how Builder works as an agency need not, and must not, conflict with our knowledge of how Builder's lower-level agents work. Each higher level of description must add to our knowledge about lower levels, rather than replace it. We'll return to the idea of <em>level</em> at many places in this book.</p>\n<p>Will psychology ever resemble any of the sciences that have successfully reduced their subjects to only a very few principles? That depends on what you mean by <em>few.</em> In physics, we're used to explanations in terms of perhaps a dozen basic principles. For psychology, our explanations will have to combine hundreds of smaller theories. To physicists, that number may seem too large. To humanists, it may seem too small.</p>",
    "text": "It's always best when mysteries can be explained in terms of things we know. But when we find this hard to do, we must decide whether to keep trying to make old theories work or to discard them and try new ones. I think this is partly a matter of personality. Let's call Reductionists those people who prefer to build on old ideas, and Novelists the ones who like to champion new hypotheses. Reduction- ists are usually right \u2014 at least at science's cautious core, where novelties rarely survive for long. Outside that realm, though, novelists reign, since older ideas have had more time to show their flaws.\nIt really is amazing how certain sciences depend upon so few kinds of explanations. The science of physics can now explain virtually everything we see, at least in principle, in terms of how a very few kinds of particles and force-fields interact. Over the past few centuries reductionism has been remarkably successful. What makes it possible to describe so much of the world in terms of so few basic rules? No one knows.\nMany scientists look on chemistry and physics as ideal models of what psychology should be like. After all, the atoms in the brain are subject to the same all-inclusive physical laws that govern every other form of matter. Then can we also explain what our brains actually do entirely in terms of those same basic principles? The answer is no, simply because even if we understood how each of our billions of brain cells works separately, this would not tell us how the brain works as an agency. The laws of thought depend not only upon the properties of those brain cells, but also on how they are connected. And these connections are established not by the basic, general laws of physics, but by the particular arrangements of the millions of bits of information in our inherited genes. To be sure, general laws apply to everything. But, for that very reason, they can rarely explain anything in particular.\nDoes this mean that psychology must reject the laws of physics and find its own? Of course not. It is not a matter of different laws, but of additional kinds of theories and principles that operate at higher levels of organization. Our ideas of how Builder works as an agency need not, and must not, conflict with our knowledge of how Builder's lower-level agents work. Each higher level of description must add to our knowledge about lower levels, rather than replace it. We'll return to the idea of level at many places in this book.\nWill psychology ever resemble any of the sciences that have successfully reduced their subjects to only a very few principles? That depends on what you mean by few. In physics, we're used to explanations in terms of perhaps a dozen basic principles. For psychology, our explanations will have to combine hundreds of smaller theories. To physicists, that number may seem too large. To humanists, it may seem too small.",
    "type": "article",
    "title": "2.2 Novelists and reductionists",
    "docId": 63758188957,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 171670913453,
    "gburl": "http://aurellem.org/society-of-mind/som-2.2.html-diffbotxyz2594306388",
    "lastCrawlTimeUTC": 1588761821,
    "timestamp": "Wed, 06 May 2020 10:43:41 GMT"
  },
  {
    "sentiment": 0.983,
    "humanLanguage": "en",
    "diffbotUri": "article|3|123208965",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-2.html",
    "html": "<blockquote> It is the nature of the mind that makes individuals kin, and the differences in the shape, form, or manner of the material atoms out of whose intricate relationships that mind is built are trivial. &mdash;Isaac Asimov </blockquote>",
    "text": "It is the nature of the mind that makes individuals kin, and the differences in the shape, form, or manner of the material atoms out of whose intricate relationships that mind is built are trivial. \u2014Isaac Asimov",
    "type": "article",
    "title": "2 Wholes and parts",
    "docId": 207360573834,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 211524616593,
    "gburl": "http://aurellem.org/society-of-mind/som-2.html-diffbotxyz1442379080",
    "lastCrawlTimeUTC": 1588761883,
    "timestamp": "Wed, 06 May 2020 10:44:43 GMT"
  },
  {
    "sentiment": 0.976,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1268494460",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.4.html",
    "html": "<p>We've talked about some ways to learn goals from other people. But how do we come to make goals for ourselves? It seems simple enough always to move from goal to subgoal &mdash; but how could one go the other way, moving outward to find new kinds of goals? Our answer may seem strange at first: there is a sense in which we never really need to invent new <em>high-level</em> goals at all. This is because, in principle, at least, it is enough to keep inventing lower-level subgoals for problems that we have to solve! Here is why this need not limit our ambitions:</p>\n<p>Functional Autonomy. In the course of pursuing any sufficiently complicated problem, the subgoals that engage our attentions can become both increasingly more ambitious and increasingly detached from the original problem.</p>\n<p>Suppose a baby's initial goal was to reach a certain cup. This could lead to the subgoal of learning how to move the arm and hand efficiently, which, in turn, could lead to sub-subgoals of learning to move around obstacles. And this could keep growing into increasingly general and abstract goals of learning how to understand and manage the physical world of space and time. Thus one can begin with a lowly goal, yet end up with some sub-subgoals that lead our minds into the most ambitious enterprises we can conceive.</p>\n<p>This can also happen in the social realm. The same baby can form, instead, the subgoal of engaging another person's help in bringing it that drinking cup. This can lead to trying to find more effective ways to influence that other person &mdash; and thus the child could become concerned with representing and predicting the motives and dispositions of other people. Again, a relatively modest drinking goal can lead to a larger competence &mdash; this time in the realm of comprehending social interactions. An initially simple concern with personal comfort becomes transformed into a more ambitious, less self-centered enterprise.</p>\n<p>Virtually any problem will be easier to solve the more one learns about the context world in which that problem occurs. No matter what one's problem is, provided that it's hard enough, one always gains from learning better ways to learn.</p>\n<p>Many of us like to believe that our intellectual enterprises lie on higher planes than our everyday activities. But now we can turn that academic value-scheme upon its head. When we get right down to it, our most abstract investigations can be seen as having origins in finding means to ordinary ends. These turn into what we regard as noble qualities when they gain enough functional autonomy to put their roots aside. In the end, our initial goals matter scarcely at all, because no matter what our original objectives, we can gain more by becoming better able to predict and control our world. It may not even matter whether an infant was initially inclined to emulate or to oppose a parent, or was first moved primarily by fear or by affection. The implements of accomplishment are much the same in either case. Knowledge is power. Whatever one's goals, they will be easier to achieve if one can become wise, wealthy, and powerful. And these in turn can best be gained by understanding how things work.</p>",
    "text": "We've talked about some ways to learn goals from other people. But how do we come to make goals for ourselves? It seems simple enough always to move from goal to subgoal \u2014 but how could one go the other way, moving outward to find new kinds of goals? Our answer may seem strange at first: there is a sense in which we never really need to invent new high-level goals at all. This is because, in principle, at least, it is enough to keep inventing lower-level subgoals for problems that we have to solve! Here is why this need not limit our ambitions:\nFunctional Autonomy. In the course of pursuing any sufficiently complicated problem, the subgoals that engage our attentions can become both increasingly more ambitious and increasingly detached from the original problem.\nSuppose a baby's initial goal was to reach a certain cup. This could lead to the subgoal of learning how to move the arm and hand efficiently, which, in turn, could lead to sub-subgoals of learning to move around obstacles. And this could keep growing into increasingly general and abstract goals of learning how to understand and manage the physical world of space and time. Thus one can begin with a lowly goal, yet end up with some sub-subgoals that lead our minds into the most ambitious enterprises we can conceive.\nThis can also happen in the social realm. The same baby can form, instead, the subgoal of engaging another person's help in bringing it that drinking cup. This can lead to trying to find more effective ways to influence that other person \u2014 and thus the child could become concerned with representing and predicting the motives and dispositions of other people. Again, a relatively modest drinking goal can lead to a larger competence \u2014 this time in the realm of comprehending social interactions. An initially simple concern with personal comfort becomes transformed into a more ambitious, less self-centered enterprise.\nVirtually any problem will be easier to solve the more one learns about the context world in which that problem occurs. No matter what one's problem is, provided that it's hard enough, one always gains from learning better ways to learn.\nMany of us like to believe that our intellectual enterprises lie on higher planes than our everyday activities. But now we can turn that academic value-scheme upon its head. When we get right down to it, our most abstract investigations can be seen as having origins in finding means to ordinary ends. These turn into what we regard as noble qualities when they gain enough functional autonomy to put their roots aside. In the end, our initial goals matter scarcely at all, because no matter what our original objectives, we can gain more by becoming better able to predict and control our world. It may not even matter whether an infant was initially inclined to emulate or to oppose a parent, or was first moved primarily by fear or by affection. The implements of accomplishment are much the same in either case. Knowledge is power. Whatever one's goals, they will be easier to achieve if one can become wise, wealthy, and powerful. And these in turn can best be gained by understanding how things work.",
    "type": "article",
    "title": "17.4 functional autonomy",
    "tags": [
      {
        "score": 0.5982980132102966,
        "sentiment": 0.838,
        "count": 2,
        "label": "autonomism",
        "uri": "https://diffbot.com/entity/XNIbF7KiLMfmCzYsQR9VBYA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Ideology"
        ]
      },
      {
        "score": 0.5476219654083252,
        "sentiment": 0,
        "count": 0,
        "label": "The Right to Be Greedy: Theses on the Practical Necessity Of Demanding Everything",
        "uri": "https://diffbot.com/entity/X4SmtEc2tPka5jIc49PS2nQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.5147449970245361,
        "sentiment": 0.493,
        "count": 1,
        "label": "Make Believe",
        "uri": "https://diffbot.com/entity/XUP3zc1dsNjmTWrqn_qDTkA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 33838137756,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 89481200016,
    "gburl": "http://aurellem.org/society-of-mind/som-17.4.html-diffbotxyz1381069797",
    "lastCrawlTimeUTC": 1588761728,
    "timestamp": "Wed, 06 May 2020 10:42:08 GMT"
  },
  {
    "sentiment": 0.642,
    "images": [
      {
        "naturalHeight": 148,
        "width": 470,
        "diffbotUri": "image|3|1784327165",
        "url": "http://aurellem.org/society-of-mind/illus/ch21/21-1.png",
        "naturalWidth": 470,
        "primary": true,
        "height": 148
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|2073328322",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.2.html",
    "html": "<p>Why are sentences so easy to understand? How do we compress our ideas into strings of words and, later, get them out again? Typically, an English sentence is built around a verb that represents some sort of act, event, or change:</p>\n<p>Jack drove from Boston to New York on the turnpike with Mary.</p>\n<p>As soon as you hear such a thing, parts of your mind become engaged with these sorts of concerns related to driving:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch21/21-1.png\"/></figure>\n<p>These concerns and <em>roles</em> seem so important that every language has developed special word-forms or grammatical constructions for them. How do we know who drove the car? We know that it's <em>Jack</em> &mdash; because the Actor comes before the verb. How do we know a car was involved? Because that is the default Vehicle for <em>drive.</em> When did all this happen? In the past &mdash; because the verb drive has the form dr-o-ve. Where did the journey start and end? We know that those places are Boston and New York, respectively, because in English the prepositions from and to precede the Origin and Destination. But we often use the same prepositions for different kinds of concerns. In the sentence about driving, <em>from</em> and <em>to</em> refer to places in space. But in the sentence below they refer to intervals of time:</p>\n<p>He changed the liquid from water to wine.</p>\n<p>The liquid has changed its composition from what it was <em>at</em> some previous time. In English we use prepositions like <em>from,</em> <em>to,</em> and <em>at</em> both for places in space and for moments in time. This is not an accident, since representing both space and time in similar ways lets us apply the selfsame reasoning skills to both of them. Thus, many of our language-grammar <em>rules</em> embody or reflect some systematic correspondences &mdash; and these are among our most powerful ways to think. Many other language-forms have evolved similarly to make it easy for us to formulate, and communicate, our most significant concerns. The next few sections discuss how the <em>pronomes</em> we mentioned earlier could be involved in processes we use to make both verbal and nonverbal <em>chains of reasoning.</em></p>",
    "text": "Why are sentences so easy to understand? How do we compress our ideas into strings of words and, later, get them out again? Typically, an English sentence is built around a verb that represents some sort of act, event, or change:\nJack drove from Boston to New York on the turnpike with Mary.\nAs soon as you hear such a thing, parts of your mind become engaged with these sorts of concerns related to driving:\nThese concerns and roles seem so important that every language has developed special word-forms or grammatical constructions for them. How do we know who drove the car? We know that it's Jack \u2014 because the Actor comes before the verb. How do we know a car was involved? Because that is the default Vehicle for drive. When did all this happen? In the past \u2014 because the verb drive has the form dr-o-ve. Where did the journey start and end? We know that those places are Boston and New York, respectively, because in English the prepositions from and to precede the Origin and Destination. But we often use the same prepositions for different kinds of concerns. In the sentence about driving, from and to refer to places in space. But in the sentence below they refer to intervals of time:\nHe changed the liquid from water to wine.\nThe liquid has changed its composition from what it was at some previous time. In English we use prepositions like from, to, and at both for places in space and for moments in time. This is not an accident, since representing both space and time in similar ways lets us apply the selfsame reasoning skills to both of them. Thus, many of our language-grammar rules embody or reflect some systematic correspondences \u2014 and these are among our most powerful ways to think. Many other language-forms have evolved similarly to make it easy for us to formulate, and communicate, our most significant concerns. The next few sections discuss how the pronomes we mentioned earlier could be involved in processes we use to make both verbal and nonverbal chains of reasoning.",
    "type": "article",
    "title": "21.2 pronomes",
    "tags": [
      {
        "score": 0.6742056608200073,
        "sentiment": 0,
        "count": 2,
        "label": "New York City",
        "uri": "https://diffbot.com/entity/AcMmgf99wMQ6XYnbChv5HiQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Settlement",
          "http://dbpedia.org/ontology/City",
          "http://dbpedia.org/ontology/Locality"
        ]
      },
      {
        "score": 0.6533600091934204,
        "sentiment": 0,
        "count": 3,
        "label": "English",
        "uri": "https://diffbot.com/entity/XfkXmX7NUMlCky37jTQU11w",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Language",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6236476898193359,
        "sentiment": 0,
        "count": 2,
        "label": "Boston",
        "uri": "https://diffbot.com/entity/A7vnJ0j-OP4qXP4s9wNDbEQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Locality",
          "http://dbpedia.org/ontology/Settlement",
          "http://dbpedia.org/ontology/City"
        ]
      },
      {
        "score": 0.5182715058326721,
        "sentiment": 0,
        "count": 1,
        "label": "actor",
        "uri": "https://diffbot.com/entity/X9qXzfDyuMNOCw-kCz5CNyQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.508655309677124,
        "sentiment": 0,
        "count": 1,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      }
    ],
    "docId": 33906917780,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 34130510231,
    "gburl": "http://aurellem.org/society-of-mind/som-21.2.html-diffbotxyz2100284707",
    "lastCrawlTimeUTC": 1588761707,
    "timestamp": "Wed, 06 May 2020 10:41:47 GMT"
  },
  {
    "sentiment": -0.702,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1012796789",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.8.html",
    "html": "<blockquote> That theory is worthless. It isn't even wrong! </blockquote>\n<p>Scientists and philosophers are always searching for simplicity. They're happiest when each new thing can be defined in terms of things that have already been defined. If we can keep doing this, then everything can be defined in successive layers and levels. This is how mathematicians usually define numbers. They begin by defining Zero &mdash; or, rather, they assume that Zero needs no definition. Then they define One as the <em>successor</em> of Zero, Two as the successor of One, and so on. But why prefer such slender chains? Why not prefer each thing to be connected to as many other things as possible? The answer is a sort of paradox. As scientists, we like to make our theories as delicate and fragile as possible. We like to arrange things so that if the slightest thing goes wrong, everything will collapse at once!</p>\n<p>Why do scientists use such shaky strategies? So that when anything goes wrong, they'll be the first to notice it. Scientists adore that flimsiness because it helps them find the precious proofs they love, with each next step in perfect mesh with every single previous one. Even when the process fails, it only means that we have made a new discovery! Especially in the world of mathematics, it is just as bad to be nearly right as it is to be totally wrong. In a sense, that's just what mathematics is &mdash; the quest for absolute consistency.</p>\n<p>But that isn't good psychology. In real life, our minds must always tolerate beliefs that later turn out to be wrong. It's also bad the way we let teachers shape our children's mathematics into slender, shaky tower chains instead of robust, cross-connected webs. A chain can break at any link, a tower can topple at the slightest shove. And that's what happens in a mathematics class to a child's mind whose attention turns just for a moment to watch a pretty cloud.</p>\n<p>Teachers try to convince their students that equations and formulas are more expressive than ordinary words. But it takes years to become proficient at using the language of mathematics, and until then, formulas and equations are in most respects even less trustworthy than commonsense reasoning. Accordingly, the investment principle works against the mathematics teacher, because even though the potential usefulness of formal mathematics is great, it is also so remote that most children will continue to use only their customary methods in ordinary life, outside of school. It is not enough to tell them, <em>Someday you will find this useful,</em> or even, <em>Learn this and I will love you.</em> Unless the new ideas become connected to the rest of the child's world, that knowledge can't be put to work.</p>\n<p>The ordinary goals of ordinary citizens are not the same as those of professional mathematicians and philosophers &mdash; who like to put things into forms with as few connections as possible. For children know from everyday experience that the more cross-connected their common-sense ideas are, the more useful they're likely to be. Why do so many schoolchildren learn to fear mathematics? Perhaps in part because we try to teach the children those formal definitions, which were designed to lead to meaning-networks as sparse and thin as possible. We shouldn't assume that making careful, narrow definitions will always help children <em>get things straight.</em> It can also make it easier for them to get things scrambled up. Instead, we ought to help them build more robust networks in their heads.</p>",
    "text": "That theory is worthless. It isn't even wrong!\nScientists and philosophers are always searching for simplicity. They're happiest when each new thing can be defined in terms of things that have already been defined. If we can keep doing this, then everything can be defined in successive layers and levels. This is how mathematicians usually define numbers. They begin by defining Zero \u2014 or, rather, they assume that Zero needs no definition. Then they define One as the successor of Zero, Two as the successor of One, and so on. But why prefer such slender chains? Why not prefer each thing to be connected to as many other things as possible? The answer is a sort of paradox. As scientists, we like to make our theories as delicate and fragile as possible. We like to arrange things so that if the slightest thing goes wrong, everything will collapse at once!\nWhy do scientists use such shaky strategies? So that when anything goes wrong, they'll be the first to notice it. Scientists adore that flimsiness because it helps them find the precious proofs they love, with each next step in perfect mesh with every single previous one. Even when the process fails, it only means that we have made a new discovery! Especially in the world of mathematics, it is just as bad to be nearly right as it is to be totally wrong. In a sense, that's just what mathematics is \u2014 the quest for absolute consistency.\nBut that isn't good psychology. In real life, our minds must always tolerate beliefs that later turn out to be wrong. It's also bad the way we let teachers shape our children's mathematics into slender, shaky tower chains instead of robust, cross-connected webs. A chain can break at any link, a tower can topple at the slightest shove. And that's what happens in a mathematics class to a child's mind whose attention turns just for a moment to watch a pretty cloud.\nTeachers try to convince their students that equations and formulas are more expressive than ordinary words. But it takes years to become proficient at using the language of mathematics, and until then, formulas and equations are in most respects even less trustworthy than commonsense reasoning. Accordingly, the investment principle works against the mathematics teacher, because even though the potential usefulness of formal mathematics is great, it is also so remote that most children will continue to use only their customary methods in ordinary life, outside of school. It is not enough to tell them, Someday you will find this useful, or even, Learn this and I will love you. Unless the new ideas become connected to the rest of the child's world, that knowledge can't be put to work.\nThe ordinary goals of ordinary citizens are not the same as those of professional mathematicians and philosophers \u2014 who like to put things into forms with as few connections as possible. For children know from everyday experience that the more cross-connected their common-sense ideas are, the more useful they're likely to be. Why do so many schoolchildren learn to fear mathematics? Perhaps in part because we try to teach the children those formal definitions, which were designed to lead to meaning-networks as sparse and thin as possible. We shouldn't assume that making careful, narrow definitions will always help children get things straight. It can also make it easier for them to get things scrambled up. Instead, we ought to help them build more robust networks in their heads.",
    "type": "article",
    "title": "18.8 mathematics made hard",
    "tags": [
      {
        "score": 0.811654806137085,
        "sentiment": -0.296,
        "count": 9,
        "label": "mathematics",
        "uri": "https://diffbot.com/entity/XsLHF-RZyOomEmARsR2YOgw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6938079595565796,
        "sentiment": 0.677,
        "count": 4,
        "label": "scientist",
        "uri": "https://diffbot.com/entity/XujaIGUOaNJ2JsSIPQozUng",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5226050615310669,
        "sentiment": -0.78,
        "count": 1,
        "label": "hard rock",
        "uri": "https://diffbot.com/entity/XDoRE3gmUNtiHdih9MUy7wg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Genre",
          "http://dbpedia.org/ontology/MusicGenre",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 79937880468,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 105430696332,
    "gburl": "http://aurellem.org/society-of-mind/som-18.8.html-diffbotxyz4102257356",
    "lastCrawlTimeUTC": 1588761756,
    "timestamp": "Wed, 06 May 2020 10:42:36 GMT"
  },
  {
    "sentiment": -0.403,
    "humanLanguage": "en",
    "diffbotUri": "article|3|344670108",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.8.html",
    "html": "<p>Just as we walk without thinking, we think without thinking! We don't know how our muscles make us walk &mdash; nor do we know much more about the agencies that do our mental work. When you have a hard problem to solve, you think about it for a time. Then, perhaps, the answer seems to come all at once, and you say, <em>Aha, I've got it. I'll do such and such.</em> But if someone were to ask how you found the solution, you could rarely say more than things like the following:</p>\n<ul> <li>I suddenly realized . . .</li> <li>I just got this idea . . .</li> <li>It occurred to me that . . .</li></ul>\n<p>If we could really sense the workings of our minds, we wouldn't act so often in accord with motives we don't suspect. We wouldn't have such varied and conflicting theories for psychology. And when we're asked how people get their good ideas, we wouldn't be reduced to metaphors about <em>ruminating,</em> and <em>digesting,</em> <em>conceiving</em> and <em>giving birth</em> to concepts &mdash; as though our thoughts were anywhere but in the head. If we could see inside our minds, we'd surely have more useful things to say.</p>\n<p>Many people seem absolutely certain that no computer could ever be sentient, conscious, self-willed, or in any other way <em>aware</em> of itself. But what makes everyone so sure that they themselves possess those admirable qualities? It's true that if we're sure of anything at all, it is that <em>I'm aware &mdash; hence I'm aware.</em> Yet what do such convictions really mean? If self-awareness means to know what's happening inside one's mind, no realist could maintain for long that people have much insight, in the literal sense of seeing-in. Indeed, the evidence that we are self-aware &mdash; that is, that we have any special aptitude for finding out what's happening inside ourselves &mdash; is very weak indeed. It is true that certain people have a special excellence at assessing the attitudes and motivations of other persons (and, more rarely, of themselves). But this does not justify the belief that how we learn things about people, including ourselves, is fundamentally different from how we learn about other things. Most of the understandings we call <em>insights</em> are merely variants of our other ways to <em>figure out</em> what's happening.</p>",
    "text": "Just as we walk without thinking, we think without thinking! We don't know how our muscles make us walk \u2014 nor do we know much more about the agencies that do our mental work. When you have a hard problem to solve, you think about it for a time. Then, perhaps, the answer seems to come all at once, and you say, Aha, I've got it. I'll do such and such. But if someone were to ask how you found the solution, you could rarely say more than things like the following:\nI suddenly realized . . .\nI just got this idea . . .\nIt occurred to me that . . .\nIf we could really sense the workings of our minds, we wouldn't act so often in accord with motives we don't suspect. We wouldn't have such varied and conflicting theories for psychology. And when we're asked how people get their good ideas, we wouldn't be reduced to metaphors about ruminating, and digesting, conceiving and giving birth to concepts \u2014 as though our thoughts were anywhere but in the head. If we could see inside our minds, we'd surely have more useful things to say.\nMany people seem absolutely certain that no computer could ever be sentient, conscious, self-willed, or in any other way aware of itself. But what makes everyone so sure that they themselves possess those admirable qualities? It's true that if we're sure of anything at all, it is that I'm aware \u2014 hence I'm aware. Yet what do such convictions really mean? If self-awareness means to know what's happening inside one's mind, no realist could maintain for long that people have much insight, in the literal sense of seeing-in. Indeed, the evidence that we are self-aware \u2014 that is, that we have any special aptitude for finding out what's happening inside ourselves \u2014 is very weak indeed. It is true that certain people have a special excellence at assessing the attitudes and motivations of other persons (and, more rarely, of themselves). But this does not justify the belief that how we learn things about people, including ourselves, is fundamentally different from how we learn about other things. Most of the understandings we call insights are merely variants of our other ways to figure out what's happening.",
    "type": "article",
    "title": "6.8 thinking without thinking",
    "tags": [
      {
        "score": 0.5820854902267456,
        "sentiment": 0.58,
        "count": 3,
        "label": "What's Going On",
        "uri": "https://diffbot.com/entity/XsuJv6OJPNxumjrik0fQ8Fg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 94235050419,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 252813279647,
    "gburl": "http://aurellem.org/society-of-mind/som-6.8.html-diffbotxyz3561479403",
    "lastCrawlTimeUTC": 1588761790,
    "timestamp": "Wed, 06 May 2020 10:43:10 GMT"
  },
  {
    "sentiment": -0.216,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-646603995",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.10.html",
    "html": "<p>Isn't it interesting how often we find ourselves using the idea of level? We talk about a person's levels of aspiration or accomplishment. We talk about levels of abstraction, levels of management, levels of detail. Is there anything in common to all the level-things people talk about? Yes: they each appear to reflect some way to organize ideas &mdash; and each seems vaguely hierarchical. Usually, we tend to think that each of those hierarchies illustrates some kind of order that exists in the world. But frequently those orderings come from the mind and merely appear to belong to the world. Indeed, if our theory of K-line trees is correct, it would seem <em>natural</em> for us to classify things into levels and hierarchies &mdash; even when this does not work out perfectly. The diagram below portrays two ways to classify physical objects.</p>\n<p>These two hierarchies split things up in different ways. The birds and airplanes are close together on one side, but far apart on the other side. Which classification is correct? Silly question! It depends on what you want to use it for. The one on the left is more useful for biologists, and the one on the right is more useful for hunters.</p>\n<p>How would you classify a porcelain duck, a pretty decorative toy? Is it a kind of bird? Is it an animal? Or is it just a lifeless piece of clay? It makes no sense to argue about it: <em>That's not a bird!</em> <em>Oh, yes, it is, and it is also pottery.</em> Instead, we frequently use two or more classifications at the same time. For example, a thoughtful child can play with a porcelain duck as though it were a make-believe animal, yet at the same time treat it carefully, in its role as a delicate object.</p>\n<p>Whenever we develop a new skill or extend an old one, we have to emphasize the relative importance of some aspects and features over others. We can place these into neat levels only when we discover systematic ways to do so. Then our classifications can resemble level-schemes and hierarchies. But the hierarchies always end up getting tangled and disorderly because there are also exceptions and interactions to each classification scheme. When attempting a new task, we never like to start anew: we try to use what has worked previously. So we search around inside our minds for old ideas to use. Then, when part of any hierarchy seems to work, we drag the rest along with it.</p>",
    "text": "Isn't it interesting how often we find ourselves using the idea of level? We talk about a person's levels of aspiration or accomplishment. We talk about levels of abstraction, levels of management, levels of detail. Is there anything in common to all the level-things people talk about? Yes: they each appear to reflect some way to organize ideas \u2014 and each seems vaguely hierarchical. Usually, we tend to think that each of those hierarchies illustrates some kind of order that exists in the world. But frequently those orderings come from the mind and merely appear to belong to the world. Indeed, if our theory of K-line trees is correct, it would seem natural for us to classify things into levels and hierarchies \u2014 even when this does not work out perfectly. The diagram below portrays two ways to classify physical objects.\nThese two hierarchies split things up in different ways. The birds and airplanes are close together on one side, but far apart on the other side. Which classification is correct? Silly question! It depends on what you want to use it for. The one on the left is more useful for biologists, and the one on the right is more useful for hunters.\nHow would you classify a porcelain duck, a pretty decorative toy? Is it a kind of bird? Is it an animal? Or is it just a lifeless piece of clay? It makes no sense to argue about it: That's not a bird! Oh, yes, it is, and it is also pottery. Instead, we frequently use two or more classifications at the same time. For example, a thoughtful child can play with a porcelain duck as though it were a make-believe animal, yet at the same time treat it carefully, in its role as a delicate object.\nWhenever we develop a new skill or extend an old one, we have to emphasize the relative importance of some aspects and features over others. We can place these into neat levels only when we discover systematic ways to do so. Then our classifications can resemble level-schemes and hierarchies. But the hierarchies always end up getting tangled and disorderly because there are also exceptions and interactions to each classification scheme. When attempting a new task, we never like to start anew: we try to use what has worked previously. So we search around inside our minds for old ideas to use. Then, when part of any hierarchy seems to work, we drag the rest along with it.",
    "type": "article",
    "title": "8.10 levels and classifications",
    "tags": [
      {
        "score": 0.6127170324325562,
        "sentiment": 0.163,
        "count": 1,
        "label": "taxonomy",
        "uri": "https://diffbot.com/entity/XphM87uCDPSSUbJC6czKXCw"
      },
      {
        "score": 0.5510160326957703,
        "sentiment": 0,
        "count": 1,
        "label": "statistical classification",
        "uri": "https://diffbot.com/entity/Xax6hbbIWMraWnvam9xqHag"
      },
      {
        "score": 0.5139992237091064,
        "sentiment": 0,
        "count": 1,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      }
    ],
    "docId": 206218199462,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 192805634445,
    "gburl": "http://aurellem.org/society-of-mind/som-8.10.html-diffbotxyz4026613622",
    "lastCrawlTimeUTC": 1588761694,
    "timestamp": "Wed, 06 May 2020 10:41:34 GMT"
  },
  {
    "sentiment": 0.997,
    "images": [
      {
        "naturalHeight": 198,
        "width": 404,
        "diffbotUri": "image|3|-1909917566",
        "url": "http://aurellem.org/society-of-mind/illus/ch18/18-6.png",
        "naturalWidth": 404,
        "primary": true,
        "height": 198
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-260207972",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.3.html",
    "html": "<p>Why is chaining so important? Because, as we've just seen, it seems to work in many different realms. More than that, it can also work in several ways at once, inside the same world. Consider how, with no apparent mental strain at all, we can first imagine the same kind of arch to be a bridge, a tunnel, or a table &mdash; and then we can imagine chains of these, according to quite different views:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch18/18-6.png\"/></figure>\n<p>Chaining seems to permeate not only how we reason, but how we think of structures in space and time. We find ourselves involved with chains whenever we imagine or explain. Why does the ability to build mental chains help us solve so many different kinds of problems? Perhaps because all sorts of chains share common properties like these:</p>\n<p>When chains are stressed, the weakest links break first. To fix a broken chain, one needs to repair only its broken links. No part of a chain can be removed if both ends remain fixed. If pulling A makes B move, there must be a chain connecting A and B.</p>\n<p>Each separate rule seems common sense, at least when we apply it to a solid thing like a bridge, a fence, or a physical chain. But why do chains apply so well to insubstantial <em>lines of thought</em>? It is because there's such a good analogy between how chains can break and how reasoning can fail.</p>",
    "text": "Why is chaining so important? Because, as we've just seen, it seems to work in many different realms. More than that, it can also work in several ways at once, inside the same world. Consider how, with no apparent mental strain at all, we can first imagine the same kind of arch to be a bridge, a tunnel, or a table \u2014 and then we can imagine chains of these, according to quite different views:\nChaining seems to permeate not only how we reason, but how we think of structures in space and time. We find ourselves involved with chains whenever we imagine or explain. Why does the ability to build mental chains help us solve so many different kinds of problems? Perhaps because all sorts of chains share common properties like these:\nWhen chains are stressed, the weakest links break first. To fix a broken chain, one needs to repair only its broken links. No part of a chain can be removed if both ends remain fixed. If pulling A makes B move, there must be a chain connecting A and B.\nEach separate rule seems common sense, at least when we apply it to a solid thing like a bridge, a fence, or a physical chain. But why do chains apply so well to insubstantial lines of thought? It is because there's such a good analogy between how chains can break and how reasoning can fail.",
    "type": "article",
    "title": "18.3 chaining",
    "docId": 425558432,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 15157182877,
    "gburl": "http://aurellem.org/society-of-mind/som-18.3.html-diffbotxyz2343238280",
    "lastCrawlTimeUTC": 1588761551,
    "timestamp": "Wed, 06 May 2020 10:39:11 GMT"
  },
  {
    "sentiment": 0.995,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-525227755",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-13.html",
    "text": "",
    "type": "article",
    "title": "13 seeing and believing",
    "docId": 80871178648,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 107311251848,
    "gburl": "http://aurellem.org/society-of-mind/som-13.html-diffbotxyz2686705644",
    "lastCrawlTimeUTC": 1588761652,
    "timestamp": "Wed, 06 May 2020 10:40:52 GMT"
  },
  {
    "sentiment": 0.714,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1672756870",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.2.html",
    "html": "<p>We'll now see how frames can help to explain how we understand that children's tale. How do we know that the kite is a present for Jack &mdash; when neither sentence mentioned this?</p>\n<p>Mary was invited to Jack's party. She wondered if he would like a kite.</p>\n<p>After the first sentence activates a party-invitation frame, the reader's mind remains engaged with that frame's concerns &mdash; including the question of what type of birthday gift to bring. If this concern is represented by some subframe, what are the concerns of that subframe? That present must be something that will please the party host. <em>Toy</em> would be a good default for it, since that's the most usual kind of gift for a child.</p>\n<p>Since <em>Jack</em> is a <em>he</em> and a <em>kite</em> is a <em>toy,</em> these two frames will merge perfectly &mdash; provided that the reader's frame for boy assumes that Jack is likely to enjoy kites. Then our two sentences combine perfectly to fill the present frame's terminals, and our problem is solved!</p>\n<p>What makes a story comprehensible? What gives it coherency? The secret lies in how each phrase and sentence stirs frames into activity or helps already active ones to fill their terminals. When the first sentence of our story mentions a party, various frames are excited &mdash; and these are still active in the reader's mind when the next sentence is read. The ground is prepared for understanding the second sentence because so many agents are already ready to recognize possible references to presents, clothes, and other matters that might be related to birthday parties.</p>",
    "text": "We'll now see how frames can help to explain how we understand that children's tale. How do we know that the kite is a present for Jack \u2014 when neither sentence mentioned this?\nMary was invited to Jack's party. She wondered if he would like a kite.\nAfter the first sentence activates a party-invitation frame, the reader's mind remains engaged with that frame's concerns \u2014 including the question of what type of birthday gift to bring. If this concern is represented by some subframe, what are the concerns of that subframe? That present must be something that will please the party host. Toy would be a good default for it, since that's the most usual kind of gift for a child.\nSince Jack is a he and a kite is a toy, these two frames will merge perfectly \u2014 provided that the reader's frame for boy assumes that Jack is likely to enjoy kites. Then our two sentences combine perfectly to fill the present frame's terminals, and our problem is solved!\nWhat makes a story comprehensible? What gives it coherency? The secret lies in how each phrase and sentence stirs frames into activity or helps already active ones to fill their terminals. When the first sentence of our story mentions a party, various frames are excited \u2014 and these are still active in the reader's mind when the next sentence is read. The ground is prepared for understanding the second sentence because so many agents are already ready to recognize possible references to presents, clothes, and other matters that might be related to birthday parties.",
    "type": "article",
    "title": "26.2 understanding stories",
    "tags": [
      {
        "score": 0.8059074282646179,
        "sentiment": 0.942,
        "count": 4,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5655861496925354,
        "sentiment": 0.567,
        "count": 1,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5096361637115479,
        "sentiment": 0,
        "count": 3,
        "label": "narrative",
        "uri": "https://diffbot.com/entity/X-yV-ag8aOfmhY9T-yRt-wg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5016264319419861,
        "sentiment": 0.539,
        "count": 2,
        "label": "birthday",
        "uri": "https://diffbot.com/entity/X7APQG3P0Nv-J_tj34bW5Lg"
      }
    ],
    "docId": 137500590527,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 271523037627,
    "gburl": "http://aurellem.org/society-of-mind/som-26.2.html-diffbotxyz3873489184",
    "lastCrawlTimeUTC": 1588761587,
    "timestamp": "Wed, 06 May 2020 10:39:47 GMT"
  },
  {
    "sentiment": -0.25,
    "images": [
      {
        "naturalHeight": 112,
        "width": 411,
        "diffbotUri": "image|3|-789407733",
        "url": "http://aurellem.org/society-of-mind/illus/ch26/26-12.png",
        "naturalWidth": 411,
        "primary": true,
        "height": 112
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1117710716",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.7.html",
    "html": "<p>At various points in their development, most children seem suddenly to comprehend new kinds of sentences. Thus, once they learn to deal with single adjectives, some children quickly learn to deal with longer strings like these:</p>\n<p>Dogs bark. Big dogs bark. Big shaggy dogs bark. Big black shaggy dogs bark.</p>\n<p>If this were done by using word-string sentence-frames, it would require a separate frame for each different number of adjectives. Another scheme would not use any frames at all but have the language-agency convert each adjective, as it arrives, into some corresponding neme. And yet another scheme to handle this (still popular among some grammar theorists) would have each successive adjective arouse a new subframe inside the previous one. However, when we look more closely at how people use adjectives, we find that these strings are not simple at all. Compare the two phrases below:</p>\n<p>The wooden three heavy brown big first boxes . . . The first three big brown heavy wooden boxes . . .</p>\n<p>Our language-agents scarcely know what to do with that first string of words because it doesn't fit the patterns we normally use for describing things. This suggests that we use framelike structures for describing nouns as well as verbs &mdash; that is, for describing things as well as actions. To fill the terminals of those frames, we expect their ingredients to arrive in a more or less definite order. We find it hard to understand a group of English adjectives unless they are arranged roughly as shown below.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch26/26-12.png\"/></figure>\n<p>Whenever a language community can agree on forms like these, expression becomes easier. Then every individual can learn, once and for all, where to put &mdash; and where to find &mdash; the answers to questions most frequently asked. In English one learns to say <em>green box,</em> while in French one says <em>box green.</em> It doesn't matter which order is used &mdash; as long as everyone agrees to do it the same way. But what are the <em>questions most frequently asked</em> &mdash; the ones we build into our language-forms? The answer to this is likely to be somewhat circular, since the language culture in which we're raised will probably affect the kinds of questions that will seem most natural to ask. Still, there could be useful clues in features that are common to many different languages.</p>\n<p>Many scientists have asked, indeed, why so many human languages use similar structures such as nouns, adjectives, verbs, clauses, and sentences. It is likely that some of these reflect what is genetically built into our language-agencies. But it seems to me even more likely that most of these nearly universal language-forms scarcely depend on language at all &mdash; but reflect how descriptions are formed in other agencies. The most common forms of phrases could arise not so much from the architecture of the language-agencies as from the machinery used by other agencies for representing objects, actions, differences, and purposes &mdash; as suggested in section 22.7 &mdash; and from how those other agencies manipulate their memories. In short, the ways we think must have a strong and universal influence on how we speak &mdash; if only through its influence on the sorts of things we'll want to say.</p>",
    "text": "At various points in their development, most children seem suddenly to comprehend new kinds of sentences. Thus, once they learn to deal with single adjectives, some children quickly learn to deal with longer strings like these:\nDogs bark. Big dogs bark. Big shaggy dogs bark. Big black shaggy dogs bark.\nIf this were done by using word-string sentence-frames, it would require a separate frame for each different number of adjectives. Another scheme would not use any frames at all but have the language-agency convert each adjective, as it arrives, into some corresponding neme. And yet another scheme to handle this (still popular among some grammar theorists) would have each successive adjective arouse a new subframe inside the previous one. However, when we look more closely at how people use adjectives, we find that these strings are not simple at all. Compare the two phrases below:\nThe wooden three heavy brown big first boxes . . . The first three big brown heavy wooden boxes . . .\nOur language-agents scarcely know what to do with that first string of words because it doesn't fit the patterns we normally use for describing things. This suggests that we use framelike structures for describing nouns as well as verbs \u2014 that is, for describing things as well as actions. To fill the terminals of those frames, we expect their ingredients to arrive in a more or less definite order. We find it hard to understand a group of English adjectives unless they are arranged roughly as shown below.\nWhenever a language community can agree on forms like these, expression becomes easier. Then every individual can learn, once and for all, where to put \u2014 and where to find \u2014 the answers to questions most frequently asked. In English one learns to say green box, while in French one says box green. It doesn't matter which order is used \u2014 as long as everyone agrees to do it the same way. But what are the questions most frequently asked \u2014 the ones we build into our language-forms? The answer to this is likely to be somewhat circular, since the language culture in which we're raised will probably affect the kinds of questions that will seem most natural to ask. Still, there could be useful clues in features that are common to many different languages.\nMany scientists have asked, indeed, why so many human languages use similar structures such as nouns, adjectives, verbs, clauses, and sentences. It is likely that some of these reflect what is genetically built into our language-agencies. But it seems to me even more likely that most of these nearly universal language-forms scarcely depend on language at all \u2014 but reflect how descriptions are formed in other agencies. The most common forms of phrases could arise not so much from the architecture of the language-agencies as from the machinery used by other agencies for representing objects, actions, differences, and purposes \u2014 as suggested in section 22.7 \u2014 and from how those other agencies manipulate their memories. In short, the ways we think must have a strong and universal influence on how we speak \u2014 if only through its influence on the sorts of things we'll want to say.",
    "type": "article",
    "title": "26.7 frames for nouns",
    "tags": [
      {
        "score": 0.5920180678367615,
        "sentiment": 0,
        "count": 2,
        "label": "English",
        "uri": "https://diffbot.com/entity/XfkXmX7NUMlCky37jTQU11w",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Language",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5768176317214966,
        "sentiment": -0.931,
        "count": 1,
        "label": "Dogs",
        "uri": "https://diffbot.com/entity/Xe5BIM9LDP460d3Dq42lMAA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.55235356092453,
        "sentiment": -0.84,
        "count": 2,
        "label": "string orchestra",
        "uri": "https://diffbot.com/entity/XefA8BhC6PPGBvS920gdgJA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Genre",
          "http://dbpedia.org/ontology/MusicGenre"
        ]
      },
      {
        "score": 0.5293211936950684,
        "sentiment": 0,
        "count": 1,
        "label": "Some Great Reward",
        "uri": "https://diffbot.com/entity/X9lhUyut0NUGe4Fi67PFKHg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5170460343360901,
        "sentiment": 0,
        "count": 1,
        "label": "France",
        "uri": "https://diffbot.com/entity/AF-T5rsSIMVOAK-HK7eoXBw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Country",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5128858089447021,
        "sentiment": 0,
        "count": 2,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 235664097727,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 213472526775,
    "gburl": "http://aurellem.org/society-of-mind/som-26.7.html-diffbotxyz4059839769",
    "lastCrawlTimeUTC": 1588761631,
    "timestamp": "Wed, 06 May 2020 10:40:31 GMT"
  },
  {
    "sentiment": -0.27,
    "humanLanguage": "en",
    "diffbotUri": "article|3|827638650",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-9.1.html",
    "html": "<p>One thing I hate is being asked questions like these:</p>\n<p>Do you prefer physics to biology? Did you like that play? Do you like Wagner? Did you enjoy your year abroad?</p>\n<p>What makes us want to compress so much into such inexpressive summaries as <em>like,</em> <em>prefer,</em> and <em>enjoy</em>? Why try to reduce such complex things to simple values or amounts of pleasurable quality? The answer is that our measures of pleasure have many uses. They help us make comparisons, compromises, and choices. They are involved with the communication signs that we use to signify various degrees of attachment, satisfaction, and agreement. They show themselves not only in words, but also as gestures, intonations, smiles and frowns, and many other expressive signs. But we have to be careful not to accept those signs at their face value. Neither the state of the world nor that of the mind is ever so simple that it can be expressed in a single, one-dimensional judgment. No situation is ever completely satisfactory or entirely disagreeable, and our reactions of pleasure or disgust are only superficial summaries of pyramids of underlying processes. To <em>enjoy</em> an experience, some of our agents must summarize success &mdash; but other agents must be censuring their subordinates for failing to achieve their goals. So we ought to be suspicious when we find ourselves liking something very much, because that might mean some of our agencies are forcefully suppressing other possibilities.</p>\n<p>The surer you are that you like what you are doing, the more completely your other ambitions are being suppressed.</p>\n<p>To choose between alternatives, the highest levels of the mind demand the simplest summaries. If your top-level feelings were too often <em>mixed,</em> you would rarely be able to make a choice to decide which foods to eat, which paths to walk, or which thoughts to think. At the level of action, you're forced to simplify right down to expressions like <em>Yes</em> and <em>No.</em> But these are not informative enough to serve the lower levels of the mind, where many processes go on at once, and every agent has to judge how well it is serving some local goals. At lower levels of the mind, there must be hosts of smaller, coexisting satisfactions and annoyances.</p>\n<p>We often talk as though we ought to be controlled by what we want. Indeed we scarcely distinguish between wanting something and potentially obtaining pleasure from it; the relation between these two ideas seems so intimate that it actually feels odd to mention it. It seems so natural to want what we like and to avoid what we don't like that we sometimes feel a sense of unnatural horror when another person appears to violate that rule; then we think, They surely wouldn't do such things unless, deep down, they really wanted to. It is as though we feel that people ought to want only to do the things they like to do.</p>\n<p>But the relation between wanting and liking is not simple at all, because our preferences are the end products of so many negotiations among our agencies. To accomplish any substantial goal, we must renounce the other possibilities and engage machinery to keep ourselves from succumbing to nostalgia or remorse. Then we use words like <em>liking</em> to express the operation of the mechanisms that hold us to our choice. Liking's job is shutting off alternatives; we ought to understand its role since, unconstrained, it narrows down our universe. This leads to liking's artificial clarity: it does not reflect what liking is but only shows what liking does.</p>",
    "text": "One thing I hate is being asked questions like these:\nDo you prefer physics to biology? Did you like that play? Do you like Wagner? Did you enjoy your year abroad?\nWhat makes us want to compress so much into such inexpressive summaries as like, prefer, and enjoy? Why try to reduce such complex things to simple values or amounts of pleasurable quality? The answer is that our measures of pleasure have many uses. They help us make comparisons, compromises, and choices. They are involved with the communication signs that we use to signify various degrees of attachment, satisfaction, and agreement. They show themselves not only in words, but also as gestures, intonations, smiles and frowns, and many other expressive signs. But we have to be careful not to accept those signs at their face value. Neither the state of the world nor that of the mind is ever so simple that it can be expressed in a single, one-dimensional judgment. No situation is ever completely satisfactory or entirely disagreeable, and our reactions of pleasure or disgust are only superficial summaries of pyramids of underlying processes. To enjoy an experience, some of our agents must summarize success \u2014 but other agents must be censuring their subordinates for failing to achieve their goals. So we ought to be suspicious when we find ourselves liking something very much, because that might mean some of our agencies are forcefully suppressing other possibilities.\nThe surer you are that you like what you are doing, the more completely your other ambitions are being suppressed.\nTo choose between alternatives, the highest levels of the mind demand the simplest summaries. If your top-level feelings were too often mixed, you would rarely be able to make a choice to decide which foods to eat, which paths to walk, or which thoughts to think. At the level of action, you're forced to simplify right down to expressions like Yes and No. But these are not informative enough to serve the lower levels of the mind, where many processes go on at once, and every agent has to judge how well it is serving some local goals. At lower levels of the mind, there must be hosts of smaller, coexisting satisfactions and annoyances.\nWe often talk as though we ought to be controlled by what we want. Indeed we scarcely distinguish between wanting something and potentially obtaining pleasure from it; the relation between these two ideas seems so intimate that it actually feels odd to mention it. It seems so natural to want what we like and to avoid what we don't like that we sometimes feel a sense of unnatural horror when another person appears to violate that rule; then we think, They surely wouldn't do such things unless, deep down, they really wanted to. It is as though we feel that people ought to want only to do the things they like to do.\nBut the relation between wanting and liking is not simple at all, because our preferences are the end products of so many negotiations among our agencies. To accomplish any substantial goal, we must renounce the other possibilities and engage machinery to keep ourselves from succumbing to nostalgia or remorse. Then we use words like liking to express the operation of the mechanisms that hold us to our choice. Liking's job is shutting off alternatives; we ought to understand its role since, unconstrained, it narrows down our universe. This leads to liking's artificial clarity: it does not reflect what liking is but only shows what liking does.",
    "type": "article",
    "title": "9.1 wanting and liking",
    "tags": [
      {
        "score": 0.6805891990661621,
        "sentiment": 0,
        "count": 5,
        "label": "anhedonia",
        "uri": "https://diffbot.com/entity/Xy7B1j_bRN2yWFIeQDX64cg",
        "rdfTypes": ["http://dbpedia.org/ontology/Disease"]
      },
      {
        "score": 0.5619295835494995,
        "sentiment": 0,
        "count": 1,
        "label": "Richard Wagner",
        "uri": "https://diffbot.com/entity/Ptz52PqNnP6WippVOBGKPIA",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5157499313354492,
        "sentiment": 0,
        "count": 1,
        "label": "physics",
        "uri": "https://diffbot.com/entity/XoKWD2p8mMwOqRXiV5SlHFg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 132875207058,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 72879751595,
    "gburl": "http://aurellem.org/society-of-mind/som-9.1.html-diffbotxyz3778076272",
    "lastCrawlTimeUTC": 1588761428,
    "timestamp": "Wed, 06 May 2020 10:37:08 GMT"
  },
  {
    "date": "Thu, 16 Jan 2020 00:00:00 GMT",
    "sentiment": 0.936,
    "humanLanguage": "en",
    "estimatedDate": "Thu, 16 Jan 2020 00:00:00 GMT",
    "diffbotUri": "article|3|-60006822",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-16.1.html",
    "html": "<p>Why do so many people think emotion is harder to explain than intellect? They're always saying things like this:</p>\n<p><em>I understand, in principle, how a computer might solve problems by reasoning. But I can't imagine how a computer could have emotions, or comprehend them. That doesn't seem at all the sort of thing machines could ever do.</em></p>\n<p>We often think of anger as nonrational. But in our Challenger scenario, the way that Work employs Anger to subdue Sleep seems no less rational than using a stick to reach for something beyond one's grasp. Anger is merely an implement that Work can use to solve one of its problems. The only complication is that Work cannot arouse Anger directly; however, it discovers a way to do this indirectly, by turning on the fantasy of Professor Challenger. No matter that this leads to states of mind that people call emotional. To Work it's merely one more way to do what it's assigned to do. We're always using images and fantasies in ordinary thought. We use <em>imagination</em> to solve a geometry problem, plan a walk to some familiar place, or choose what to eat for dinner: in each, we must envision things that aren't actually there. The use of fantasies, emotional or not, is indispensable for every complicated problem-solving process. We always have to deal with nonexistent scenes, because only when a mind can change the ways things appear to be can it really start to think of how to change the ways things are.</p>\n<p>In any case, our culture wrongly teaches us that thoughts and feelings lie in almost separate worlds. In fact, they're always intertwined. In the next few sections we'll propose to regard emotions not as separate from thoughts in general, but as varieties or types of thoughts, each based on a different brain-machine that specializes in some particular domain of thought. In infancy, these <em>protospecialists</em> have little to do with one another, but later they grow together as they learn to exploit one another, albeit without understanding one another, the way Work exploits Anger to stop Sleep.</p>\n<p>Another reason we consider emotion to be more mysterious and powerful than reason is that we wrongly credit it with many things that reason does. We're all so insensitive to the complexity of ordinary thinking that we take the marvels of our common sense for granted. Then, whenever anyone does something outstanding, instead of trying to understand the process of thought that did the real work, we attribute that virtue to whichever superficial emotional signs we can easily discern, like motivation, passion, inspiration, or sensibility.</p>\n<p>In any case, no matter how neutral and rational a goal may seem, it will eventually conflict with other goals if it persists for long enough. No long-term project can be carried out without some defense against competing interests, and this is likely to produce what we call emotional reactions to the conflicts that come about among our most insistent goals. The question is not whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions. I suspect that once we give machines the ability to alter their own abilities we'll have to provide them with all sorts of complex checks and balances. It is probably no accident that the term</p>\n<p><em>machinelike</em> has come to have two opposite connotations. One means completely unconcerned, unfeeling, and emotionless, devoid of any interest. The other means being implacably committed to some single cause. Thus each suggests not only inhumanity, but also some stupidity. Too much commitment leads to doing only one single thing; too little concern produces aimless wandering.</p>",
    "text": "Why do so many people think emotion is harder to explain than intellect? They're always saying things like this:\nI understand, in principle, how a computer might solve problems by reasoning. But I can't imagine how a computer could have emotions, or comprehend them. That doesn't seem at all the sort of thing machines could ever do.\nWe often think of anger as nonrational. But in our Challenger scenario, the way that Work employs Anger to subdue Sleep seems no less rational than using a stick to reach for something beyond one's grasp. Anger is merely an implement that Work can use to solve one of its problems. The only complication is that Work cannot arouse Anger directly; however, it discovers a way to do this indirectly, by turning on the fantasy of Professor Challenger. No matter that this leads to states of mind that people call emotional. To Work it's merely one more way to do what it's assigned to do. We're always using images and fantasies in ordinary thought. We use imagination to solve a geometry problem, plan a walk to some familiar place, or choose what to eat for dinner: in each, we must envision things that aren't actually there. The use of fantasies, emotional or not, is indispensable for every complicated problem-solving process. We always have to deal with nonexistent scenes, because only when a mind can change the ways things appear to be can it really start to think of how to change the ways things are.\nIn any case, our culture wrongly teaches us that thoughts and feelings lie in almost separate worlds. In fact, they're always intertwined. In the next few sections we'll propose to regard emotions not as separate from thoughts in general, but as varieties or types of thoughts, each based on a different brain-machine that specializes in some particular domain of thought. In infancy, these protospecialists have little to do with one another, but later they grow together as they learn to exploit one another, albeit without understanding one another, the way Work exploits Anger to stop Sleep.\nAnother reason we consider emotion to be more mysterious and powerful than reason is that we wrongly credit it with many things that reason does. We're all so insensitive to the complexity of ordinary thinking that we take the marvels of our common sense for granted. Then, whenever anyone does something outstanding, instead of trying to understand the process of thought that did the real work, we attribute that virtue to whichever superficial emotional signs we can easily discern, like motivation, passion, inspiration, or sensibility.\nIn any case, no matter how neutral and rational a goal may seem, it will eventually conflict with other goals if it persists for long enough. No long-term project can be carried out without some defense against competing interests, and this is likely to produce what we call emotional reactions to the conflicts that come about among our most insistent goals. The question is not whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions. I suspect that once we give machines the ability to alter their own abilities we'll have to provide them with all sorts of complex checks and balances. It is probably no accident that the term\nmachinelike has come to have two opposite connotations. One means completely unconcerned, unfeeling, and emotionless, devoid of any interest. The other means being implacably committed to some single cause. Thus each suggests not only inhumanity, but also some stupidity. Too much commitment leads to doing only one single thing; too little concern produces aimless wandering.",
    "type": "article",
    "title": "16.1 emotion",
    "tags": [
      {
        "score": 0.658996045589447,
        "sentiment": 0.286,
        "count": 1,
        "label": "Professor Challenger",
        "uri": "https://diffbot.com/entity/XobmEtwweOk688vIQ90PUWw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.593441903591156,
        "sentiment": 0.3,
        "count": 1,
        "label": "Challenger",
        "uri": "https://diffbot.com/entity/XkzzHeCalMbWxeZ0vxplZLg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/MeanOfTransportation",
          "http://dbpedia.org/ontology/Rocket"
        ]
      },
      {
        "score": 0.5634320378303528,
        "sentiment": 0.734,
        "count": 2,
        "label": "fantasy",
        "uri": "https://diffbot.com/entity/Xkva-h3StP56KXipLY9BeRw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5449221730232239,
        "sentiment": 0.226,
        "count": 1,
        "label": "Sleep",
        "uri": "https://diffbot.com/entity/PbfOZswWSNxeqloyEyK9_cQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 169417851325,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 99782541739,
    "gburl": "http://aurellem.org/society-of-mind/som-16.1.html-diffbotxyz3221629939",
    "lastCrawlTimeUTC": 1588761486,
    "timestamp": "Wed, 06 May 2020 10:38:06 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1615480609",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-24.html",
    "html": "<ul><li><a href=\"http://aurellem.org/society-of-mind/som-24.3.html\"><em>24.3</em> How trans-frames work</a></li> <br><br> </ul>",
    "text": "24.3 How trans-frames work",
    "type": "article",
    "title": "24 frames",
    "docId": 171356635577,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 233432252805,
    "gburl": "http://aurellem.org/society-of-mind/som-24.html-diffbotxyz854018631",
    "lastCrawlTimeUTC": 1588761515,
    "timestamp": "Wed, 06 May 2020 10:38:35 GMT"
  },
  {
    "sentiment": 0.58,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1614899613",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-2.3.html",
    "html": "<p>We're often told that certain wholes are <em>more than the sum of their parts.</em> We hear this expressed with reverent words like <em>holistic</em> and <em>gestalt,</em> whose academic tones suggest that they refer to clear and definite ideas. But I suspect the actual function of such terms is to anesthetize a sense of ignorance. We say <em>gestalt</em> when things combine to act in ways we can't explain, <em>holistic</em> when we're caught off guard by unexpected happenings and realize we understand less than we thought we did. For example, consider the two sets of questions below, the first <em>subjective</em> and the second <em>objective</em>:</p>\n<p>What makes a drawing more than just its separate lines? How is a personality more than a set of traits? In what way is a culture more than a mere collection of customs? What makes a tower more than separate blocks? Why is a chain more than its various links? How is a wall more than a set of many bricks?</p>\n<p>Why do the <em>objective</em> questions seem less mysterious? Because we have good ways to answer them &mdash; in terms of how things interact. To explain how walls and towers work, we just point out how every block is held in place by its neighbors and by gravity. To explain why chain-links cannot come apart, we can demonstrate how each would get in its neighbors' way. These explanations seem almost self-evident to adults. However, they did not seem so simple when we were children, and it took each of us several years to learn how real-world objects interact &mdash; for example, to prevent any two objects from ever being in the same place. We regard such knowledge as <em>obvious</em> only because we cannot remember how hard it was to learn.</p>\n<p>Why does it seem so much harder to explain our reactions to drawings, personalities, and cultural traditions? Many people assume that those <em>subjective</em> kinds of questions are impossible to answer because they involve our minds. But that doesn't mean they can't be answered. It only means that we must first know more about our minds.</p>\n<p><em>Subjective</em> reactions are also based on how things interact. The difference is that here we are not concerned with objects in the world outside, but with processes inside our brains.</p>\n<p>In other words, those questions about arts, traits, and styles of life are actually quite technical. They ask us to explain what happens among the agents in our minds. But this is a subject about which we have never learned very much &mdash; and neither have our sciences. Such questions will be answered in time. But it will just prolong the wait if we keep using pseudo-explanation words like <em>holistic</em> and <em>gestalt.</em> True, sometimes giving names to things can help by leading us to focus on some mystery. It's harmful, though, when naming leads the mind to think that names alone bring meaning close.</p>",
    "text": "We're often told that certain wholes are more than the sum of their parts. We hear this expressed with reverent words like holistic and gestalt, whose academic tones suggest that they refer to clear and definite ideas. But I suspect the actual function of such terms is to anesthetize a sense of ignorance. We say gestalt when things combine to act in ways we can't explain, holistic when we're caught off guard by unexpected happenings and realize we understand less than we thought we did. For example, consider the two sets of questions below, the first subjective and the second objective:\nWhat makes a drawing more than just its separate lines? How is a personality more than a set of traits? In what way is a culture more than a mere collection of customs? What makes a tower more than separate blocks? Why is a chain more than its various links? How is a wall more than a set of many bricks?\nWhy do the objective questions seem less mysterious? Because we have good ways to answer them \u2014 in terms of how things interact. To explain how walls and towers work, we just point out how every block is held in place by its neighbors and by gravity. To explain why chain-links cannot come apart, we can demonstrate how each would get in its neighbors' way. These explanations seem almost self-evident to adults. However, they did not seem so simple when we were children, and it took each of us several years to learn how real-world objects interact \u2014 for example, to prevent any two objects from ever being in the same place. We regard such knowledge as obvious only because we cannot remember how hard it was to learn.\nWhy does it seem so much harder to explain our reactions to drawings, personalities, and cultural traditions? Many people assume that those subjective kinds of questions are impossible to answer because they involve our minds. But that doesn't mean they can't be answered. It only means that we must first know more about our minds.\nSubjective reactions are also based on how things interact. The difference is that here we are not concerned with objects in the world outside, but with processes inside our brains.\nIn other words, those questions about arts, traits, and styles of life are actually quite technical. They ask us to explain what happens among the agents in our minds. But this is a subject about which we have never learned very much \u2014 and neither have our sciences. Such questions will be answered in time. But it will just prolong the wait if we keep using pseudo-explanation words like holistic and gestalt. True, sometimes giving names to things can help by leading us to focus on some mystery. It's harmful, though, when naming leads the mind to think that names alone bring meaning close.",
    "type": "article",
    "title": "2.3 Parts and wholes",
    "tags": [
      {
        "score": 0.56550532579422,
        "sentiment": 0,
        "count": 1,
        "label": "The Second Objective",
        "uri": "https://diffbot.com/entity/XW34JhmCoMyGjsOX652LWRQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.541102945804596,
        "sentiment": 0.328,
        "count": 1,
        "label": "arts",
        "uri": "https://diffbot.com/entity/XQFHSSpFAODGp_Nx3oG2jCQ"
      }
    ],
    "docId": 193737769359,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 108421513624,
    "gburl": "http://aurellem.org/society-of-mind/som-2.3.html-diffbotxyz3711841192",
    "lastCrawlTimeUTC": 1588761456,
    "timestamp": "Wed, 06 May 2020 10:37:36 GMT"
  },
  {
    "date": "Thu, 12 Dec 2019 00:00:00 GMT",
    "sentiment": 0.525,
    "humanLanguage": "en",
    "estimatedDate": "Thu, 12 Dec 2019 00:00:00 GMT",
    "diffbotUri": "article|3|-537679683",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.12.html",
    "html": "<p>What is a meaning? Sometimes we're told a definition of a word, and suddenly, we know a way to use that word. But definitions do not often work so well. Suppose you had to explain what <em>game</em> means. You could start like this:</p>\n<p>GAME: An activity in which two teams compete to make a ball do something that results in a winning score.</p>\n<p>This fits a certain range of games &mdash; but what of games that just use words, or keep no scores, or lack the element of competition? We can capture the nature of more kinds of games by using other definitions, but nothing seems to catch them all. We simply cannot find much in common to everything we call a game. Yet one still feels there is a certain unity that underlies the idea of a game. For example, we feel that we could recognize new games, and that <em>game</em> is more than an arbitrary accumulation.</p>\n<p>But now let's turn our attention away from the physical aspects of games and focus on the psychological purposes that games can serve. Then it is much easier to find some qualities that are common to most adult games:</p>\n<p>GAME: An activity that is engaging and diverting, deliberately detached from real life.</p>\n<p>This second kind of definition treats a game, not as a kind of object-thing, but as a process in the mind. At first this might seem somewhat strange, but really it is nothing new &mdash; even our first definition already contained psychological elements, concealed in the words <em>competing</em> and <em>winning.</em> When seen this way, different kinds of games seem much more similar. This is because they all serve common purposes &mdash; despite the great diversity of their physical appearances. After all, there is virtually no limit to the variety of physical objects or structures that could be used to accomplish the same psychological purpose &mdash; in this case, to make an activity diverting (whatever that might mean). Naturally, then, it would be hard to specify the range of all the possible physical forms of games.</p>\n<p>Of course, it is no great surprise to find that <em>game</em> has a more psychological character than does <em>brick,</em> which we can define in physical terms without referring to our goals. But most ideas lie in between. We saw this in the case of <em>chair,</em> which we cannot describe without referring both to a physical structure and to a psychological function.</p>",
    "text": "What is a meaning? Sometimes we're told a definition of a word, and suddenly, we know a way to use that word. But definitions do not often work so well. Suppose you had to explain what game means. You could start like this:\nGAME: An activity in which two teams compete to make a ball do something that results in a winning score.\nThis fits a certain range of games \u2014 but what of games that just use words, or keep no scores, or lack the element of competition? We can capture the nature of more kinds of games by using other definitions, but nothing seems to catch them all. We simply cannot find much in common to everything we call a game. Yet one still feels there is a certain unity that underlies the idea of a game. For example, we feel that we could recognize new games, and that game is more than an arbitrary accumulation.\nBut now let's turn our attention away from the physical aspects of games and focus on the psychological purposes that games can serve. Then it is much easier to find some qualities that are common to most adult games:\nGAME: An activity that is engaging and diverting, deliberately detached from real life.\nThis second kind of definition treats a game, not as a kind of object-thing, but as a process in the mind. At first this might seem somewhat strange, but really it is nothing new \u2014 even our first definition already contained psychological elements, concealed in the words competing and winning. When seen this way, different kinds of games seem much more similar. This is because they all serve common purposes \u2014 despite the great diversity of their physical appearances. After all, there is virtually no limit to the variety of physical objects or structures that could be used to accomplish the same psychological purpose \u2014 in this case, to make an activity diverting (whatever that might mean). Naturally, then, it would be hard to specify the range of all the possible physical forms of games.\nOf course, it is no great surprise to find that game has a more psychological character than does brick, which we can define in physical terms without referring to our goals. But most ideas lie in between. We saw this in the case of chair, which we cannot describe without referring both to a physical structure and to a psychological function.",
    "type": "article",
    "title": "12.12 meaning and definition",
    "tags": [
      {
        "score": 0.6781343221664429,
        "sentiment": 0.489,
        "count": 8,
        "label": "game",
        "uri": "https://diffbot.com/entity/XhJcUtxlYN-G2fYaqw_NG2Q"
      },
      {
        "score": 0.5391238927841187,
        "sentiment": 0.805,
        "count": 3,
        "label": "psychological horror",
        "uri": "https://diffbot.com/entity/XOi5-EFxgMaqhjRwvZnYUlg",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      }
    ],
    "docId": 40848064938,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 183522558348,
    "gburl": "http://aurellem.org/society-of-mind/som-12.12.html-diffbotxyz2521765244",
    "lastCrawlTimeUTC": 1588761337,
    "timestamp": "Wed, 06 May 2020 10:35:37 GMT"
  },
  {
    "sentiment": 0.291,
    "images": [
      {
        "naturalHeight": 143,
        "width": 418,
        "diffbotUri": "image|3|-1614206766",
        "url": "http://aurellem.org/society-of-mind/illus/ch8/8-4.png",
        "naturalWidth": 418,
        "primary": true,
        "height": 143
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1196067931",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.4.html",
    "html": "<p>We make our new ideas by merging parts of older ones &mdash; and that means keeping more than one idea in mind at once. Let's oversimplify matters for the moment and imagine that the mind is composed of many <em>divisions,</em> each involved with a different activity, like vision, locomotion, language, and so forth. This pattern repeats on smaller scales, so that even the thought of the simplest ordinary object is made up of smaller thoughts in smaller agencies. Thinking about a small white rubber ball could activate some divisions like these:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch8/8-4.png\"/></figure>\n<p>We'll need some way to talk about the states of many agencies at once. So, in this book, I'll use the expression <em>mental state</em> or <em>total mental state</em> when talking about the states of all of one's agents. The new phrase <em>partial mental state</em> is for talking about the states of smaller groups of agents. Now in order to be clear, we'll have to simplify our picture of the situation, the way scientists do. We shall assume that each agent in our society, at each moment, is either in a <em>quiet state</em> or an <em>active state.</em> Why can't an agent be partially aroused, instead of only <em>on</em> or <em>off</em>? They could indeed, but there are technical reasons why this would not make any fundamental difference to the issues we are discussing here. In any case, this assumption allows us to be precise:</p>\n<p>A <em>total state</em> of mind is a list that specifies which agents are active and which are quiet at a certain moment. A <em>partial state</em> of mind merely specifies that certain agents are active but does not say which other agents are quiet.</p>\n<p>Notice that according to this definition, a mind can have exactly one total state at any moment, but it can be in many partial states at the same time &mdash; because partial states are incomplete descriptions. The picture above shows a mind-society made up of several separate divisions, so we can think of each division's state as one partial state, and this lets us imagine that the entire system can <em>think several thoughts at once,</em> just as a crowd of separate people can. When your speech division is being occupied with what your friend is saying while your vision division looks for a door to exit through &mdash; then your mind is in two partial states at once.</p>\n<p>The situation is more interesting when two K-lines activate agents in the same division at the same time: imposing two different partial mental states on the same agency can lead to conflicts. It is easy to think of a small white ball because this activates K-lines that connect to unrelated sets of agents. But when you try to imagine a round square, your agents for round and square are forced to compete to control the same set of shape-describing agents. If the conflict is not settled soon, noncompromise may eliminate both &mdash; and leave you with the sense of an undefined shape.</p>",
    "text": "We make our new ideas by merging parts of older ones \u2014 and that means keeping more than one idea in mind at once. Let's oversimplify matters for the moment and imagine that the mind is composed of many divisions, each involved with a different activity, like vision, locomotion, language, and so forth. This pattern repeats on smaller scales, so that even the thought of the simplest ordinary object is made up of smaller thoughts in smaller agencies. Thinking about a small white rubber ball could activate some divisions like these:\nWe'll need some way to talk about the states of many agencies at once. So, in this book, I'll use the expression mental state or total mental state when talking about the states of all of one's agents. The new phrase partial mental state is for talking about the states of smaller groups of agents. Now in order to be clear, we'll have to simplify our picture of the situation, the way scientists do. We shall assume that each agent in our society, at each moment, is either in a quiet state or an active state. Why can't an agent be partially aroused, instead of only on or off? They could indeed, but there are technical reasons why this would not make any fundamental difference to the issues we are discussing here. In any case, this assumption allows us to be precise:\nA total state of mind is a list that specifies which agents are active and which are quiet at a certain moment. A partial state of mind merely specifies that certain agents are active but does not say which other agents are quiet.\nNotice that according to this definition, a mind can have exactly one total state at any moment, but it can be in many partial states at the same time \u2014 because partial states are incomplete descriptions. The picture above shows a mind-society made up of several separate divisions, so we can think of each division's state as one partial state, and this lets us imagine that the entire system can think several thoughts at once, just as a crowd of separate people can. When your speech division is being occupied with what your friend is saying while your vision division looks for a door to exit through \u2014 then your mind is in two partial states at once.\nThe situation is more interesting when two K-lines activate agents in the same division at the same time: imposing two different partial mental states on the same agency can lead to conflicts. It is easy to think of a small white ball because this activates K-lines that connect to unrelated sets of agents. But when you try to imagine a round square, your agents for round and square are forced to compete to control the same set of shape-describing agents. If the conflict is not settled soon, noncompromise may eliminate both \u2014 and leave you with the sense of an undefined shape.",
    "type": "article",
    "title": "8.4 partial mental states",
    "tags": [
      {
        "score": 0.5655183792114258,
        "sentiment": -0.256,
        "count": 2,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5319467782974243,
        "sentiment": 0,
        "count": 2,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5036424994468689,
        "sentiment": 0,
        "count": 7,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 69707940273,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 176087941513,
    "gburl": "http://aurellem.org/society-of-mind/som-8.4.html-diffbotxyz1145652059",
    "lastCrawlTimeUTC": 1588761370,
    "timestamp": "Wed, 06 May 2020 10:36:10 GMT"
  },
  {
    "sentiment": -0.953,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-340521202",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.7.html",
    "html": "<p>From every moment to the next, a person's state of mind is involved with various objects, topics, goals, and scripts. When you hear the words of the sentence, <em>Put the apple in the pail,</em> somehow this causes the subjects of <em>apple,</em> <em>pail</em> and <em>putting in</em> to <em>occupy your mind.</em> Later we'll speculate about how these become assigned to appropriate roles. Here, to make the story short, let's just assume that at a certain point the language-agency interprets the verb <em>put</em> to activate a certain Trans-frame and to assign the apple-neme to the Object pronome of that Trans-frame. The automatic <em>finding machine</em> described earlier then assigns the apple's location to the Origin pronome. Similarly, the pail's location is assigned to the Destination pronome. (As for the Instrument pronome, this is assigned to the listener's hand by default.) Now each entity <em>on the listener's mind</em> is represented by one or another pronome assignment. We're almost done, except that in order to actually perform the imagined action, we need some kind of control process to activate the proper agencies in the proper sequence!</p>\n<p>Activate apple-neme, Look-for, and Move. Then activate Grasp. Activate pail-neme, Look-for, and Move. Then activate Ungrasp.</p>\n<p>This suggests a way to learn a skill. The first few times you try to do something new, you must experiment to find which agents to activate, and at what times, and for how long. Later, you can prepare a script that will do the job more quickly and easily by accumulating memories of which agent-activations were successful, together with memories of which polynemes were assigned to various pronomes at those moments. For example, if you were to <em>play back</em> the <em>Trans-script</em> shown above, your arm would find and put a second apple in that pail &mdash; without invoking any higher-level agencies at all! However, this script has a dreadful limitation: it will work only to put apples into pails. What if you later wished to put a block into a box or a spoon into a bowl? We could do that by dividing the process into two scripts: a <em>pronome-assignment script</em> and an <em>action script.</em></p>\n<p>Assignment Script: Assign the apple-neme to the Origin pronome. Assign the pail-neme to the Destination pronome.</p>\n<p>Action Script: Activate Origin. Then turn on Look-for, Move, and Grasp. Activate Destination. Then turn on Look-for, Move, and Ungrasp.</p>\n<p>Now notice that the action script never actually mentions the apple or pail at all but refers only to the pronomes that represent them. Thus the same action script will serve as well for putting a block into a box as for putting an apple into a pail!</p>",
    "text": "From every moment to the next, a person's state of mind is involved with various objects, topics, goals, and scripts. When you hear the words of the sentence, Put the apple in the pail, somehow this causes the subjects of apple, pail and putting in to occupy your mind. Later we'll speculate about how these become assigned to appropriate roles. Here, to make the story short, let's just assume that at a certain point the language-agency interprets the verb put to activate a certain Trans-frame and to assign the apple-neme to the Object pronome of that Trans-frame. The automatic finding machine described earlier then assigns the apple's location to the Origin pronome. Similarly, the pail's location is assigned to the Destination pronome. (As for the Instrument pronome, this is assigned to the listener's hand by default.) Now each entity on the listener's mind is represented by one or another pronome assignment. We're almost done, except that in order to actually perform the imagined action, we need some kind of control process to activate the proper agencies in the proper sequence!\nActivate apple-neme, Look-for, and Move. Then activate Grasp. Activate pail-neme, Look-for, and Move. Then activate Ungrasp.\nThis suggests a way to learn a skill. The first few times you try to do something new, you must experiment to find which agents to activate, and at what times, and for how long. Later, you can prepare a script that will do the job more quickly and easily by accumulating memories of which agent-activations were successful, together with memories of which polynemes were assigned to various pronomes at those moments. For example, if you were to play back the Trans-script shown above, your arm would find and put a second apple in that pail \u2014 without invoking any higher-level agencies at all! However, this script has a dreadful limitation: it will work only to put apples into pails. What if you later wished to put a block into a box or a spoon into a bowl? We could do that by dividing the process into two scripts: a pronome-assignment script and an action script.\nAssignment Script: Assign the apple-neme to the Origin pronome. Assign the pail-neme to the Destination pronome.\nAction Script: Activate Origin. Then turn on Look-for, Move, and Grasp. Activate Destination. Then turn on Look-for, Move, and Ungrasp.\nNow notice that the action script never actually mentions the apple or pail at all but refers only to the pronomes that represent them. Thus the same action script will serve as well for putting a block into a box as for putting an apple into a pail!",
    "type": "article",
    "title": "21.7 generalizing with pronomes",
    "docId": 171042062773,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 40695939483,
    "gburl": "http://aurellem.org/society-of-mind/som-21.7.html-diffbotxyz640679000",
    "lastCrawlTimeUTC": 1588761399,
    "timestamp": "Wed, 06 May 2020 10:36:39 GMT"
  },
  {
    "sentiment": 0.691,
    "author": "\u2014John Bowlby",
    "humanLanguage": "en",
    "diffbotUri": "article|3|435266109",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.3.html",
    "html": "<blockquote> No form of behavior is accompanied by stronger feeling than is attachment behavior. The figures towards whom it is directed are loved and their advent is treated with joy. So long as a child is in the unchallenged presence of a principal attachment-figure, or within easy reach, he feels secure. A threat of loss creates anxiety, and actuall loss, sorrow; both, moreover, are likely to arouse anger. </blockquote>\n<p>Most higher animals have evolved instinctive <em>bonding</em> mechanisms that keep the youngsters close to the parents. Human infants, too, are born with tendencies to form special attachments; all parents know their powerful effects. Early in life, most children become attached to one or a few family members or caretakers, sometimes so firmly that for several years such children may never stray more than a few meters from the attachment-figure. During those years, a prolonged separation of the child from those particular persons may be followed by an enduring depression or disturbance, during which the child's personality does not develop normally.</p>\n<p>What is the function of childhood attachment? The simplest explanation is that it evolved to keep children within a safe sphere of nurture and protection. But according to our theory, our human bond machinery has the additional function of forcing children to acquire values, goals, and ideals from particular older individuals. Why is this so important? Because even though there are many ways a child could learn about ordinary causes and effects, there is no way for a child to construct a coherent system of values &mdash; except by basing it upon some already existing model. The task of constructing a <em>civilized personality</em> must be far beyond the inventive power of any single individual. Furthermore, if too wide a variety of adult models were available, it would be too hard to build a coherent personality of one's own, because one would have to pick and choose fragments from all those different personalities &mdash; and this might lead to so many conflicts and inconsistencies that many of them would cancel each other out. It would simplify the child's task if the attachment mechanism restricted attention to only a few role models.</p>\n<p>How did our attachment-bonds evolve? In many species of animals, attachment occurs so swiftly and firmly that scientists who study animal behavior call it <em>imprinting.</em> Presumably, the machinery that makes us learn our parents' goals is descended from the mechanisms of our animal ancestors. Presumably our infantile attachment-bonds form as soon as various inborn systems learn to distinguish the parents' individual peculiarities &mdash; first by senses of touch, taste, and smell; then by sound of voice and, finally, by sight of face.</p>\n<p>Once those attachment-bonds are formed, a child won't react in the same way to the faces and voices of strangers and parents, for these have different effects on how we learn. The effect of an attachment-person's affection or rejection is not like that of ordinary <em>success-failure</em> goal-rewards &mdash; which merely teach us what to do in order to achieve our goals. Attachment-related signals seem to work directly on those goals themselves &mdash; and thus can modify our personalities. Attachments teach us ends, not means &mdash; and thus impose on us our parents' dreams.</p>",
    "text": "No form of behavior is accompanied by stronger feeling than is attachment behavior. The figures towards whom it is directed are loved and their advent is treated with joy. So long as a child is in the unchallenged presence of a principal attachment-figure, or within easy reach, he feels secure. A threat of loss creates anxiety, and actuall loss, sorrow; both, moreover, are likely to arouse anger.\nMost higher animals have evolved instinctive bonding mechanisms that keep the youngsters close to the parents. Human infants, too, are born with tendencies to form special attachments; all parents know their powerful effects. Early in life, most children become attached to one or a few family members or caretakers, sometimes so firmly that for several years such children may never stray more than a few meters from the attachment-figure. During those years, a prolonged separation of the child from those particular persons may be followed by an enduring depression or disturbance, during which the child's personality does not develop normally.\nWhat is the function of childhood attachment? The simplest explanation is that it evolved to keep children within a safe sphere of nurture and protection. But according to our theory, our human bond machinery has the additional function of forcing children to acquire values, goals, and ideals from particular older individuals. Why is this so important? Because even though there are many ways a child could learn about ordinary causes and effects, there is no way for a child to construct a coherent system of values \u2014 except by basing it upon some already existing model. The task of constructing a civilized personality must be far beyond the inventive power of any single individual. Furthermore, if too wide a variety of adult models were available, it would be too hard to build a coherent personality of one's own, because one would have to pick and choose fragments from all those different personalities \u2014 and this might lead to so many conflicts and inconsistencies that many of them would cancel each other out. It would simplify the child's task if the attachment mechanism restricted attention to only a few role models.\nHow did our attachment-bonds evolve? In many species of animals, attachment occurs so swiftly and firmly that scientists who study animal behavior call it imprinting. Presumably, the machinery that makes us learn our parents' goals is descended from the mechanisms of our animal ancestors. Presumably our infantile attachment-bonds form as soon as various inborn systems learn to distinguish the parents' individual peculiarities \u2014 first by senses of touch, taste, and smell; then by sound of voice and, finally, by sight of face.\nOnce those attachment-bonds are formed, a child won't react in the same way to the faces and voices of strangers and parents, for these have different effects on how we learn. The effect of an attachment-person's affection or rejection is not like that of ordinary success-failure goal-rewards \u2014 which merely teach us what to do in order to achieve our goals. Attachment-related signals seem to work directly on those goals themselves \u2014 and thus can modify our personalities. Attachments teach us ends, not means \u2014 and thus impose on us our parents' dreams.",
    "type": "article",
    "title": "17.3 attachment simplifies",
    "docId": 223488770479,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 115442221495,
    "gburl": "http://aurellem.org/society-of-mind/som-17.3.html-diffbotxyz1085241144",
    "lastCrawlTimeUTC": 1588761298,
    "timestamp": "Wed, 06 May 2020 10:34:58 GMT"
  },
  {
    "sentiment": 0.218,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1365231858",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.7.html",
    "html": "<p>Let's return to moving mental furniture. What would we need to imagine moving things around a room? First we'd need some way to represent how objects are arranged in space. In our Block-Arch scenario, the scene was represented in terms of the shapes of the objects and the relations between them. In the case of a room scene, you might also relate each object to the walls and corners of the room; you might notice that the couch is about midway between a table and a chair, and that all three are lined up near a certain wall.</p>\n<p>Once we have a method for representing rooms, we also need techniques for manipulating these representations. How could we envision the result of exchanging that couch and chair? Let's oversimplify, and suppose that this can he done simply by exchanging the states of two agencies &mdash; an agency A, which represents the couch, and another agency B, which represents the chair. To exchange these states, let's assume that both agencies have access to two <em>short-term memory-units,</em> called M-1 and M-2, which can record the states of agencies. Then we can exchange the states of A and B, first by storing away the states A and B, and then by restoring them in reverse order. In other words, we could use the following simple four-step <em>script</em>:</p>\n<p>1. Store the state of A in M-1. 2. Store the state of B in M-2. 3. Use M-2 to determine the state of A. 4. Use M-1 to determine the state of B.</p>\n<p>A <em>memory-control script</em> like this can work only if we have memory-units that are small enough to pick out couch-sized portions of the larger scene. M-1 and M-2 would not do the job if they could store only descriptions of entire rooms. In other words, we have to be able to connect our short-term memories only to appropriate aspects of our current problems. Learning such abilities is not simple, and perhaps it is a skill some people never really master. What if we wanted to rearrange three or more objects? As a matter of fact, it is possible to produce any rearrangement whatsoever, using only operations that exchange two objects at a time! When you approach an unfamiliar kind of problem, it's best to start by making only one or two changes at a time. Then, in the course of becoming an expert, you discover schemes that make several useful changes in memory at once.</p>\n<p>Our pair-exchanging script needs more machinery. Because each memory-unit must wait until the previous step is finished, the timing of each script step may have to depend on various <em>condition sensors.</em> Shortly we'll see that even this is not enough to solve hard problems: our memory-control processes also need ways to interrupt themselves while they call on other agencies or memories for help. Indeed, the problems we must solve when managing our memories are surprisingly like those we face when dealing with things in the outside world.</p>",
    "text": "Let's return to moving mental furniture. What would we need to imagine moving things around a room? First we'd need some way to represent how objects are arranged in space. In our Block-Arch scenario, the scene was represented in terms of the shapes of the objects and the relations between them. In the case of a room scene, you might also relate each object to the walls and corners of the room; you might notice that the couch is about midway between a table and a chair, and that all three are lined up near a certain wall.\nOnce we have a method for representing rooms, we also need techniques for manipulating these representations. How could we envision the result of exchanging that couch and chair? Let's oversimplify, and suppose that this can he done simply by exchanging the states of two agencies \u2014 an agency A, which represents the couch, and another agency B, which represents the chair. To exchange these states, let's assume that both agencies have access to two short-term memory-units, called M-1 and M-2, which can record the states of agencies. Then we can exchange the states of A and B, first by storing away the states A and B, and then by restoring them in reverse order. In other words, we could use the following simple four-step script:\n1. Store the state of A in M-1. 2. Store the state of B in M-2. 3. Use M-2 to determine the state of A. 4. Use M-1 to determine the state of B.\nA memory-control script like this can work only if we have memory-units that are small enough to pick out couch-sized portions of the larger scene. M-1 and M-2 would not do the job if they could store only descriptions of entire rooms. In other words, we have to be able to connect our short-term memories only to appropriate aspects of our current problems. Learning such abilities is not simple, and perhaps it is a skill some people never really master. What if we wanted to rearrange three or more objects? As a matter of fact, it is possible to produce any rearrangement whatsoever, using only operations that exchange two objects at a time! When you approach an unfamiliar kind of problem, it's best to start by making only one or two changes at a time. Then, in the course of becoming an expert, you discover schemes that make several useful changes in memory at once.\nOur pair-exchanging script needs more machinery. Because each memory-unit must wait until the previous step is finished, the timing of each script step may have to depend on various condition sensors. Shortly we'll see that even this is not enough to solve hard problems: our memory-control processes also need ways to interrupt themselves while they call on other agencies or memories for help. Indeed, the problems we must solve when managing our memories are surprisingly like those we face when dealing with things in the outside world.",
    "type": "article",
    "title": "15.7 memory rearrangements",
    "tags": [
      {
        "score": 0.7226964831352234,
        "sentiment": -0.623,
        "count": 2,
        "label": "computer memory",
        "uri": "https://diffbot.com/entity/XWsavN-0wOLquQDMlQ-MDYw"
      },
      {
        "score": 0.642825722694397,
        "sentiment": 0,
        "count": 4,
        "label": "M-1",
        "uri": "https://diffbot.com/entity/LpI5ZR-zqN2e6r9sXzf4UCw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/ArchitecturalStructure",
          "http://dbpedia.org/ontology/Infrastructure",
          "http://dbpedia.org/ontology/RouteOfTransportation",
          "http://dbpedia.org/ontology/Road"
        ]
      },
      {
        "score": 0.6184872388839722,
        "sentiment": 0,
        "count": 2,
        "label": "Let's",
        "uri": "https://diffbot.com/entity/CRLcl-YyyOXa6DYMRB1qTdA",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      },
      {
        "score": 0.6061389446258545,
        "sentiment": 0,
        "count": 2,
        "label": "Sydney Trains A & B sets",
        "uri": "https://diffbot.com/entity/LVFGdn9jYP7uPmae4XPWnRw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/MeanOfTransportation",
          "http://dbpedia.org/ontology/Train"
        ]
      },
      {
        "score": 0.6032947897911072,
        "sentiment": 0,
        "count": 2,
        "label": "Fly Me to the Moon",
        "uri": "https://diffbot.com/entity/XRZrQ-OahMnaptNVys6VVhw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 94511956378,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 206002028928,
    "gburl": "http://aurellem.org/society-of-mind/som-15.7.html-diffbotxyz2565150609",
    "lastCrawlTimeUTC": 1588761253,
    "timestamp": "Wed, 06 May 2020 10:34:13 GMT"
  },
  {
    "sentiment": -0.794,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-528981635",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.7.html",
    "html": "<p>To speak and understand a language well, an ordinary person must learn thousands of words. To learn the proper use of a single word must involve great numbers of connections between the agents for that word and other agents. What could cause such connections to be made, and what might be their physical embodiments?</p>\n<p>Any comprehensive theory of the mind must include some ideas about the nature of the connections among agents. Consider that a person can learn to <em>associate</em> virtually any combination of ideas or words. Does this require us to assume that it is possible for a given K-line agent to become connected, directly, to any of thousands or millions of other agents? That seems out of the question, in view of what we know about connections in the human brain. Many brain cells have fibers that branch out enough to approach many thousands of other cells &mdash; but few of them branch out enough to reach millions of other cells, and as far as we know, a mature brain cell can only make new connections to other cells that already lie close to the fibers that branch into or out from it. Furthermore, we do not seem to grow many new brain cells after birth; on the contrary, their number actually decreases. To be sure, brain cells continue to <em>mature</em> for several years, and probably their fibers grow extensively. But no one yet knows whether this comes about as a result of learning new connections or whether it must happen first, to make it feasible for those cells to learn new connections.</p>\n<p>Even the arrangements of long-distance connections between our brain cells do not permit direct connection between arbitrary pairs of agents, for those long connections are generally arranged in relatively orderly bundles, less regular but otherwise resembling the parallel pathways from skin to brain. Fortunately, direct connections are not really necessary, for the same reasons that every telephone in the world can easily be connected to any other telephone without the need for connecting a billion separate wires to each house. Instead, telephone systems make their connections indirectly, by using agencies called <em>exchanges</em> that require only moderate numbers of wires. I don't mean to suggest that brains use the sorts of switching-schemes found in telephone systems but only to say that it is not necessary for every K-line agent to directly contact every agent to which it might eventually become linked.</p>\n<p>There are several factors that reduce the magnitude of the interconnection problem. First, in order to reproduce the major features of a remembered partial state of mind, it should suffice to activate only a representative sample of its agents. Second, according to our theory of knowledge-trees, most K-lines' connections are indirect to begin with, since they connect only to other, nearby K-line trees. A polyneme, too, need be connected only to a single memorizer agent near each agency. And no K-line needs potential connections to all the agents in any agency, since it is enough to make connections only in a certain level-band.</p>",
    "text": "To speak and understand a language well, an ordinary person must learn thousands of words. To learn the proper use of a single word must involve great numbers of connections between the agents for that word and other agents. What could cause such connections to be made, and what might be their physical embodiments?\nAny comprehensive theory of the mind must include some ideas about the nature of the connections among agents. Consider that a person can learn to associate virtually any combination of ideas or words. Does this require us to assume that it is possible for a given K-line agent to become connected, directly, to any of thousands or millions of other agents? That seems out of the question, in view of what we know about connections in the human brain. Many brain cells have fibers that branch out enough to approach many thousands of other cells \u2014 but few of them branch out enough to reach millions of other cells, and as far as we know, a mature brain cell can only make new connections to other cells that already lie close to the fibers that branch into or out from it. Furthermore, we do not seem to grow many new brain cells after birth; on the contrary, their number actually decreases. To be sure, brain cells continue to mature for several years, and probably their fibers grow extensively. But no one yet knows whether this comes about as a result of learning new connections or whether it must happen first, to make it feasible for those cells to learn new connections.\nEven the arrangements of long-distance connections between our brain cells do not permit direct connection between arbitrary pairs of agents, for those long connections are generally arranged in relatively orderly bundles, less regular but otherwise resembling the parallel pathways from skin to brain. Fortunately, direct connections are not really necessary, for the same reasons that every telephone in the world can easily be connected to any other telephone without the need for connecting a billion separate wires to each house. Instead, telephone systems make their connections indirectly, by using agencies called exchanges that require only moderate numbers of wires. I don't mean to suggest that brains use the sorts of switching-schemes found in telephone systems but only to say that it is not necessary for every K-line agent to directly contact every agent to which it might eventually become linked.\nThere are several factors that reduce the magnitude of the interconnection problem. First, in order to reproduce the major features of a remembered partial state of mind, it should suffice to activate only a representative sample of its agents. Second, according to our theory of knowledge-trees, most K-lines' connections are indirect to begin with, since they connect only to other, nearby K-line trees. A polyneme, too, need be connected only to a single memorizer agent near each agency. And no K-line needs potential connections to all the agents in any agency, since it is enough to make connections only in a certain level-band.",
    "type": "article",
    "title": "20.7 connections",
    "docId": 156631286164,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 144760799617,
    "gburl": "http://aurellem.org/society-of-mind/som-20.7.html-diffbotxyz3935985645",
    "lastCrawlTimeUTC": 1588761154,
    "timestamp": "Wed, 06 May 2020 10:32:34 GMT"
  },
  {
    "sentiment": -0.342,
    "images": [
      {
        "naturalHeight": 143,
        "width": 336,
        "diffbotUri": "image|3|-1223544995",
        "url": "http://aurellem.org/society-of-mind/illus/ch19/19-1.png",
        "naturalWidth": 336,
        "primary": true,
        "height": 143
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1683455642",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.2.html",
    "html": "<p>We're normally quite unaware of how our brain-machines enable us to see, or walk, or remember what we want. And we're equally unaware of how we speak or of how we comprehend the words we hear. As far as consciousness can tell, no sooner do we hear a phrase than all its meanings spring to mind &mdash; yet we have no conscious sense of how those words produce their effects. Consider that all children learn to speak and understand &mdash; yet few adults will ever recognize the regularities of their grammars. For example, all English speakers learn that saying <em>big brown dog</em> is right, while <em>brown big dog</em> is somehow wrong. How do we learn which phrases are admissible? No language scientist even knows whether brains must learn this once or twice &mdash; first, for knowing what to say, and second, for knowing what to hear. Do we reuse the same machinery for both? Our conscious minds just cannot tell, since consciousness does not reveal how language works.</p>\n<p>However, on the other side, language seems to play a role in much of what our consciousness does. I suspect that this is because our language-agency plays special roles in how we think, through having a great deal of control over the memory-systems in other agencies and therefore over the huge accumulations of knowledge they contain. But language is only a part of thought. We sometimes seem to think in words &mdash; and sometimes not. What do we <em>think in</em> when we aren't using words? And how do the agents that work with words communicate with those that don't? Since no one knows, we'll have to make a theory. We'll start by imagining that the language-system is divided into three regions.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch19/19-1.png\"/></figure>\n<p>The upper region contains agents that are concerned specifically with words. The lower region includes all the agencies that are affected by words. And in the center lie the agencies involved with how words engage our recollections, expectations, and other kinds of mental processes. There is also one peculiarity: the language-agency seems to have an unusual capacity to control its own memories. Our diagram suggests that this could be because the language-agency can exploit itself as though it were just another agency.</p>",
    "text": "We're normally quite unaware of how our brain-machines enable us to see, or walk, or remember what we want. And we're equally unaware of how we speak or of how we comprehend the words we hear. As far as consciousness can tell, no sooner do we hear a phrase than all its meanings spring to mind \u2014 yet we have no conscious sense of how those words produce their effects. Consider that all children learn to speak and understand \u2014 yet few adults will ever recognize the regularities of their grammars. For example, all English speakers learn that saying big brown dog is right, while brown big dog is somehow wrong. How do we learn which phrases are admissible? No language scientist even knows whether brains must learn this once or twice \u2014 first, for knowing what to say, and second, for knowing what to hear. Do we reuse the same machinery for both? Our conscious minds just cannot tell, since consciousness does not reveal how language works.\nHowever, on the other side, language seems to play a role in much of what our consciousness does. I suspect that this is because our language-agency plays special roles in how we think, through having a great deal of control over the memory-systems in other agencies and therefore over the huge accumulations of knowledge they contain. But language is only a part of thought. We sometimes seem to think in words \u2014 and sometimes not. What do we think in when we aren't using words? And how do the agents that work with words communicate with those that don't? Since no one knows, we'll have to make a theory. We'll start by imagining that the language-system is divided into three regions.\nThe upper region contains agents that are concerned specifically with words. The lower region includes all the agencies that are affected by words. And in the center lie the agencies involved with how words engage our recollections, expectations, and other kinds of mental processes. There is also one peculiarity: the language-agency seems to have an unusual capacity to control its own memories. Our diagram suggests that this could be because the language-agency can exploit itself as though it were just another agency.",
    "type": "article",
    "title": "19.2 the language-agency",
    "tags": [
      {
        "score": 0.7005513906478882,
        "sentiment": 0.341,
        "count": 4,
        "label": "language",
        "uri": "https://diffbot.com/entity/XZLUv8RZWNw62dTgKTTa7JQ"
      },
      {
        "score": 0.52593994140625,
        "sentiment": 0,
        "count": 1,
        "label": "English",
        "uri": "https://diffbot.com/entity/XfkXmX7NUMlCky37jTQU11w",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Language",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 180143767957,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 242083529119,
    "gburl": "http://aurellem.org/society-of-mind/som-19.2.html-diffbotxyz34604234",
    "lastCrawlTimeUTC": 1588761226,
    "timestamp": "Wed, 06 May 2020 10:33:46 GMT"
  },
  {
    "sentiment": 0.8,
    "images": [
      {
        "naturalHeight": 195,
        "width": 430,
        "diffbotUri": "image|3|-1276762498",
        "url": "http://aurellem.org/society-of-mind/illus/ch29/29-2.png",
        "naturalWidth": 430,
        "primary": true,
        "height": 195
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-995544317",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-29.1.html",
    "html": "<p>Our view of the body and the mind as separate entities is only one example of our many ways to view the world as divided into different realms. Imagine that a committee were commissioned to write down everything about the universe in a perfectly organized book.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch29/29-2.png\"/></figure>\n<p>Why do the gaps between the lines seem smaller than those that separate the pages? It is because we better understand what happens in between. We understand how walls relate to bricks because they represent closely related <em>levels of organization.</em> Similarly, we understand the relation between houses and walls. But it would be hard to cross the gap between houses and bricks without having enough intermediate concepts such as that of a wall. It simply isn't practical to think of the place where someone lives as a network of relationships among a million boards and bricks.</p>\n<p>It's much the same in other realms; we need to be able to describe things at many levels of detail. We all belong to families or companies, and sometimes we can think of each group as <em>nothing but</em> a network of agreements and relationships. But when we need a larger view, as when thinking about the politics of an entire country, we cannot think effectively without regarding entire families or companies as though they were single objects in a different realm. The same applies to how we think about our minds. Even if you knew all the details of each little agent in your brain, your higher-level processes would still need coarser summaries.</p>\n<p>Why is it easier to understand how walls relate to bricks, or families to individuals, than to understand how thoughts relate to things? It's not because there's any single mystery. It is because the level gap between walls and bricks is really much smaller than that between minds and brain cells. Suppose we actually had that wonderful encyclopedia of <em>all possible knowledge,</em> arranged according to the nearness of topics. There we'd find the essays on walls quite close to those about bricks. But the sections that cover the nature of thoughts would lie volumes away from the ones on the nature of things.</p>",
    "text": "Our view of the body and the mind as separate entities is only one example of our many ways to view the world as divided into different realms. Imagine that a committee were commissioned to write down everything about the universe in a perfectly organized book.\nWhy do the gaps between the lines seem smaller than those that separate the pages? It is because we better understand what happens in between. We understand how walls relate to bricks because they represent closely related levels of organization. Similarly, we understand the relation between houses and walls. But it would be hard to cross the gap between houses and bricks without having enough intermediate concepts such as that of a wall. It simply isn't practical to think of the place where someone lives as a network of relationships among a million boards and bricks.\nIt's much the same in other realms; we need to be able to describe things at many levels of detail. We all belong to families or companies, and sometimes we can think of each group as nothing but a network of agreements and relationships. But when we need a larger view, as when thinking about the politics of an entire country, we cannot think effectively without regarding entire families or companies as though they were single objects in a different realm. The same applies to how we think about our minds. Even if you knew all the details of each little agent in your brain, your higher-level processes would still need coarser summaries.\nWhy is it easier to understand how walls relate to bricks, or families to individuals, than to understand how thoughts relate to things? It's not because there's any single mystery. It is because the level gap between walls and bricks is really much smaller than that between minds and brain cells. Suppose we actually had that wonderful encyclopedia of all possible knowledge, arranged according to the nearness of topics. There we'd find the essays on walls quite close to those about bricks. But the sections that cover the nature of thoughts would lie volumes away from the ones on the nature of things.",
    "type": "article",
    "title": "29.1 the realms of thought",
    "docId": 213884649908,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 70309478799,
    "gburl": "http://aurellem.org/society-of-mind/som-29.1.html-diffbotxyz4225686453",
    "lastCrawlTimeUTC": 1588761208,
    "timestamp": "Wed, 06 May 2020 10:33:28 GMT"
  },
  {
    "sentiment": 0.598,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-581987217",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.7.html",
    "html": "<p>For virtually every change we see, we tend to seek some cause. And when we find no cause on the scene, we'll postulate that one exists, even though we might be wrong. We do this so consistently that I wouldn't be surprised to find that brains have built-in tendencies to try to represent all situations in certain special ways:</p>\n<p>THINGS. Whatever we may see or touch, we represent the scene in terms of separate object-things. We do the same for representing processes and mental states. In languages, these object-symbols tend to correspond to nouns. DIFFERENCES. Whenever we discern a change or just compare two different things, we represent this as a difference thing. In languages, these often correspond to verbs.</p>\n<p>CAUSES. Whenever we conceive of an action, change, or difference, we try to assign a cause to it &mdash; that is, some other person, process, or thing that we can hold to be responsible for it. In languages, causes often take the forms of things. CLAUSES. Whatever structures we conceive are dealt with like single things. In languages, this corresponds to treating an entire phrase as though it were a single word.</p>\n<p>In English, almost every sentence form demands some sort of Actor noun &mdash; and I think this reflects the need to find a motive or a cause. Consider how we place that it in <em>Soon it will begin to rain.</em> We're always chopping complex situations into artificially clear-cut chunks which we perceive as separate things. Then we notice various differences and relationships among those parts and assign them to various parts of speech. We string our words into clauses and our clauses into chains, often interrupting one by inserting fragments of others inside it, yet proceeding as though there were no interruptions at all. It has been alleged that the construction of such structures is unique to the grammar-machinery of language, but I suspect that languages evolved those forms because of mechanisms deeper in the grain of how we think. For example, when we talked about visual ambiguity, we saw that our vision-systems are highly proficient at representing structures that interrupt one another. This suggests that both our visual and linguistic abilities to deal with <em>interruptions</em> could be based on similar methods with which we <em>manage</em> what is represented in our short-term memories.</p>\n<p>In any case, our brains appear to make us seek to represent dependencies. Whatever happens, where or when, we're prone to wonder who or what's responsible. This leads us to discover explanations that we might not otherwise imagine, and that helps us predict and control not only what happens in the world, but also what happens in our minds. But what if those same tendencies should lead us to imagine things and causes that do not exist? Then we'll invent false gods and superstitions and see their hand in every chance coincidence. Indeed, perhaps that strange word <em>I</em> &mdash; as used in <em>I just had a good idea</em> &mdash; reflects the selfsame tendency. If you're compelled to find some cause that causes everything you do &mdash; why, then, that something needs a name. You call it <em>me.</em> I call it <em>you.</em></p>",
    "text": "For virtually every change we see, we tend to seek some cause. And when we find no cause on the scene, we'll postulate that one exists, even though we might be wrong. We do this so consistently that I wouldn't be surprised to find that brains have built-in tendencies to try to represent all situations in certain special ways:\nTHINGS. Whatever we may see or touch, we represent the scene in terms of separate object-things. We do the same for representing processes and mental states. In languages, these object-symbols tend to correspond to nouns. DIFFERENCES. Whenever we discern a change or just compare two different things, we represent this as a difference thing. In languages, these often correspond to verbs.\nCAUSES. Whenever we conceive of an action, change, or difference, we try to assign a cause to it \u2014 that is, some other person, process, or thing that we can hold to be responsible for it. In languages, causes often take the forms of things. CLAUSES. Whatever structures we conceive are dealt with like single things. In languages, this corresponds to treating an entire phrase as though it were a single word.\nIn English, almost every sentence form demands some sort of Actor noun \u2014 and I think this reflects the need to find a motive or a cause. Consider how we place that it in Soon it will begin to rain. We're always chopping complex situations into artificially clear-cut chunks which we perceive as separate things. Then we notice various differences and relationships among those parts and assign them to various parts of speech. We string our words into clauses and our clauses into chains, often interrupting one by inserting fragments of others inside it, yet proceeding as though there were no interruptions at all. It has been alleged that the construction of such structures is unique to the grammar-machinery of language, but I suspect that languages evolved those forms because of mechanisms deeper in the grain of how we think. For example, when we talked about visual ambiguity, we saw that our vision-systems are highly proficient at representing structures that interrupt one another. This suggests that both our visual and linguistic abilities to deal with interruptions could be based on similar methods with which we manage what is represented in our short-term memories.\nIn any case, our brains appear to make us seek to represent dependencies. Whatever happens, where or when, we're prone to wonder who or what's responsible. This leads us to discover explanations that we might not otherwise imagine, and that helps us predict and control not only what happens in the world, but also what happens in our minds. But what if those same tendencies should lead us to imagine things and causes that do not exist? Then we'll invent false gods and superstitions and see their hand in every chance coincidence. Indeed, perhaps that strange word I \u2014 as used in I just had a good idea \u2014 reflects the selfsame tendency. If you're compelled to find some cause that causes everything you do \u2014 why, then, that something needs a name. You call it me. I call it you.",
    "type": "article",
    "title": "22.7 causes and clauses",
    "docId": 214985490875,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 242400608663,
    "gburl": "http://aurellem.org/society-of-mind/som-22.7.html-diffbotxyz3588632683",
    "lastCrawlTimeUTC": 1588761185,
    "timestamp": "Wed, 06 May 2020 10:33:05 GMT"
  },
  {
    "sentiment": 0.947,
    "images": [
      {
        "naturalHeight": 123,
        "width": 313,
        "diffbotUri": "image|3|1003992899",
        "url": "http://aurellem.org/society-of-mind/illus/ch16/16-7.png",
        "naturalWidth": 313,
        "primary": true,
        "height": 123
      },
      {
        "naturalHeight": 146,
        "width": 323,
        "diffbotUri": "image|3|1004916420",
        "url": "http://aurellem.org/society-of-mind/illus/ch16/16-8.png",
        "naturalWidth": 323,
        "height": 146
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1472844585",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-16.6.html",
    "html": "<p>Imagine that a thirsty child has learned to reach for a nearby cup. What keeps that child, afterward, from reaching for a cup in every other circumstance &mdash; say, when it is lonely or when it is cold? How do we keep separate what we learn for satisfying different goals? One way is to maintain a separate memory bank for every distinct goal.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch16/16-7.png\"/></figure>\n<p>To make this work, we must restrict each specialist to learn only when its own goal is active. We can accomplish that by building them into a cross-exclusion system so that, for example, Hunger's memories can be formed only when Hunger is active. Such a system will never get confused about which memories to use. When it is hungry it will do only what it learned to do at previous times when it was hungry; it won't eat when it is thirsty or drink when it is hungry. But it would be too extravagant to have to keep completely different memories for every goal &mdash; since, as we said, most real-world goals engage the same kinds of knowledge about the world. Wouldn't it be better if all those specialists could share a common, general-purpose memory?</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch16/16-8.png\"/></figure>\n<p>This would lead to problems, too. Whenever any specialist tried to rearrange some memories to its own advantage, it might damage structures upon which the others have come to depend. There would be too many unpredictable interactions. How could specialists cooperate and share what they have learned? If they were like people, they could communicate, negotiate, and organize. But because each separate specialist is much too small and specialized to understand how the others work, the best each can do is learn to exploit what the others can do, without understanding how they do it.</p>",
    "text": "Imagine that a thirsty child has learned to reach for a nearby cup. What keeps that child, afterward, from reaching for a cup in every other circumstance \u2014 say, when it is lonely or when it is cold? How do we keep separate what we learn for satisfying different goals? One way is to maintain a separate memory bank for every distinct goal.\nTo make this work, we must restrict each specialist to learn only when its own goal is active. We can accomplish that by building them into a cross-exclusion system so that, for example, Hunger's memories can be formed only when Hunger is active. Such a system will never get confused about which memories to use. When it is hungry it will do only what it learned to do at previous times when it was hungry; it won't eat when it is thirsty or drink when it is hungry. But it would be too extravagant to have to keep completely different memories for every goal \u2014 since, as we said, most real-world goals engage the same kinds of knowledge about the world. Wouldn't it be better if all those specialists could share a common, general-purpose memory?\nThis would lead to problems, too. Whenever any specialist tried to rearrange some memories to its own advantage, it might damage structures upon which the others have come to depend. There would be too many unpredictable interactions. How could specialists cooperate and share what they have learned? If they were like people, they could communicate, negotiate, and organize. But because each separate specialist is much too small and specialized to understand how the others work, the best each can do is learn to exploit what the others can do, without understanding how they do it.",
    "type": "article",
    "title": "16.6 motivation",
    "tags": [
      {
        "score": 0.6863533854484558,
        "sentiment": 0.821,
        "count": 2,
        "label": "hunger",
        "uri": "https://diffbot.com/entity/Xo7QCmaohNGCXAGXrhtotMA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill",
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject"
        ]
      },
      {
        "score": 0.5864775776863098,
        "sentiment": 0,
        "count": 1,
        "label": "Imagine That",
        "uri": "https://diffbot.com/entity/XAZo_OzOPMfGcX8iOUA655A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Single"
        ]
      },
      {
        "score": 0.505000650882721,
        "sentiment": 0.794,
        "count": 2,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 34053620124,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 100592845213,
    "gburl": "http://aurellem.org/society-of-mind/som-16.6.html-diffbotxyz2739981375",
    "lastCrawlTimeUTC": 1588761055,
    "timestamp": "Wed, 06 May 2020 10:30:55 GMT"
  },
  {
    "sentiment": 0.832,
    "images": [
      {
        "naturalHeight": 129,
        "width": 449,
        "diffbotUri": "image|3|-1222621474",
        "url": "http://aurellem.org/society-of-mind/illus/ch19/19-2.png",
        "naturalWidth": 449,
        "primary": true,
        "height": 129
      },
      {
        "naturalHeight": 138,
        "width": 427,
        "diffbotUri": "image|3|-1221697953",
        "url": "http://aurellem.org/society-of-mind/illus/ch19/19-3.png",
        "naturalWidth": 427,
        "height": 138
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|410654772",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.5.html",
    "html": "<p>What happens when a single agent sends messages to several different agencies? In many cases, such a message will have a different effect on each of those other agencies. As I mentioned earlier, I'll call such an agent a <em>polyneme.</em> For example, your word-agent for the word <em>apple</em> must be a polyneme because it sets your agencies for color, shape, and size into unrelated states that represent the independent properties of being red, round, and <em>apple-sized.</em></p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch19/19-2.png\"/></figure>\n<p>But how could the same message come to have such diverse effects on so many agencies, with each effect so specifically appropriate to the idea of <em>apple</em>? There is only one explanation: Each of those other agencies must already have learned its own response to that same signal. Because polynemes, like politicians, mean different things to different listeners, each listener must learn its own, different way to react to that message. (The prefix poly- is to suggest diversity, and the suffix -neme is to indicate how this depends on memory.)</p>\n<p>To understand a polyneme, each agency must learn its own specific and appropriate response. Each agency must have its private dictionary or memory bank to tell it how to respond to every polyneme.</p>\n<p>How could all those agencies learn how to respond to each polyneme? If each polyneme were connected to a K-line in each agency, each of those K-lines would need only to learn what partial state to arouse inside its agency. The drawing below suggests that those K-lines could form little <em>memorizers</em> next to the agencies that they affect. Thus, memories are formed and stored close to the places where they are used.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch19/19-3.png\"/></figure>\n<p>Can any simple scheme like this give rise to all the richness of the meaning of a real language-word? The answer is that all ideas about meaning will seem inadequate by themselves, since nothing can mean anything except within some larger context of ideas.</p>",
    "text": "What happens when a single agent sends messages to several different agencies? In many cases, such a message will have a different effect on each of those other agencies. As I mentioned earlier, I'll call such an agent a polyneme. For example, your word-agent for the word apple must be a polyneme because it sets your agencies for color, shape, and size into unrelated states that represent the independent properties of being red, round, and apple-sized.\nBut how could the same message come to have such diverse effects on so many agencies, with each effect so specifically appropriate to the idea of apple? There is only one explanation: Each of those other agencies must already have learned its own response to that same signal. Because polynemes, like politicians, mean different things to different listeners, each listener must learn its own, different way to react to that message. (The prefix poly- is to suggest diversity, and the suffix -neme is to indicate how this depends on memory.)\nTo understand a polyneme, each agency must learn its own specific and appropriate response. Each agency must have its private dictionary or memory bank to tell it how to respond to every polyneme.\nHow could all those agencies learn how to respond to each polyneme? If each polyneme were connected to a K-line in each agency, each of those K-lines would need only to learn what partial state to arouse inside its agency. The drawing below suggests that those K-lines could form little memorizers next to the agencies that they affect. Thus, memories are formed and stored close to the places where they are used.\nCan any simple scheme like this give rise to all the richness of the meaning of a real language-word? The answer is that all ideas about meaning will seem inadequate by themselves, since nothing can mean anything except within some larger context of ideas.",
    "type": "article",
    "title": "19.5 polynemes",
    "tags": [
      {
        "score": 0.5461015701293945,
        "sentiment": 0,
        "count": 2,
        "label": "apple",
        "uri": "https://diffbot.com/entity/XgrXdplpfMRWDa_VBAOd2oA"
      },
      {
        "score": 0.5429016351699829,
        "sentiment": -0.161,
        "count": 2,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      }
    ],
    "docId": 88239964607,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 127780913568,
    "gburl": "http://aurellem.org/society-of-mind/som-19.5.html-diffbotxyz1712608277",
    "lastCrawlTimeUTC": 1588761083,
    "timestamp": "Wed, 06 May 2020 10:31:23 GMT"
  },
  {
    "date": "Fri, 13 Dec 2019 00:00:00 GMT",
    "sentiment": 0.925,
    "humanLanguage": "en",
    "estimatedDate": "Fri, 13 Dec 2019 00:00:00 GMT",
    "diffbotUri": "article|3|1111783907",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.13.html",
    "html": "<p>At last we're coming close to capturing the meanings of things like chairs and games. We found that structural descriptions are useful, but they always seem too specific. Most chairs have legs, and most games have scores &mdash; but there are always exceptions. We also found purposeful descriptions to be useful, but they never seemed specific enough. <em>Thing you can sit upon</em> is too general to specify a chair, since you can sit on almost anything. <em>Diverting activity</em> is too broad for game &mdash; since there are many other ways to turn our minds from serious things. In general, a single definition rarely works.</p>\n<p>Purposeful definitions are usually too loose. They include many things we do not intend. Structural definitions are usually too tight. They reject many things we want to include.</p>\n<p>But we can often capture an idea by squeezing in from several sides at once, to get exactly what we need by using two or more different kinds of descriptions at the same time.</p>\n<p>Our best ideas are often those that bridge between two different worlds!</p>\n<p>I don't insist that every definition combine just these particular ingredients of structure and purpose. But that specific mixture does have a peculiar virtue: it helps us bridge between the <em>ends</em> we seek and the <em>means</em> we have. That is, it helps us connect things we can recognize (or make, find, do, or think) to problems we want to solve. It would be of little use to know that X's <em>exist</em> without some way to find and use them.</p>\n<p>When we discussed accumulation, we saw that the concepts of <em>furniture</em> and <em>money</em> have reasonably compact functional definitions but accumulate many structural descriptions. Conversely, the concepts of <em>square</em> or <em>circle</em> have compact structural definitions but accumulate endless collections of possible uses.</p>\n<p>To learn to use a new or unfamiliar word, you start by taking it to be a sign that there exists, inside some other person's mind, a structure you could use. But no matter how carefully it is explained, you must still rebuild that thought yourself, from materials already in your own mind. It helps to be given a good definition, but still you must mold and shape each new idea to suit your own existing skills &mdash; hoping to make it work for you the way it seems to work for those from whom you learn.</p>\n<p>What people call <em>meanings</em> do not usually correspond to particular and definite structures, but to connections among and across fragments of the great interlocking networks of connections and constraints among our agencies. Because these networks are constantly growing and changing, meanings are rarely sharp, and we cannot always expect to be able to <em>define</em> them in terms of compact sequences of words. Verbal explanations serve only as partial hints; we also have to learn from watching, working, playing &mdash; and thinking.</p>",
    "text": "At last we're coming close to capturing the meanings of things like chairs and games. We found that structural descriptions are useful, but they always seem too specific. Most chairs have legs, and most games have scores \u2014 but there are always exceptions. We also found purposeful descriptions to be useful, but they never seemed specific enough. Thing you can sit upon is too general to specify a chair, since you can sit on almost anything. Diverting activity is too broad for game \u2014 since there are many other ways to turn our minds from serious things. In general, a single definition rarely works.\nPurposeful definitions are usually too loose. They include many things we do not intend. Structural definitions are usually too tight. They reject many things we want to include.\nBut we can often capture an idea by squeezing in from several sides at once, to get exactly what we need by using two or more different kinds of descriptions at the same time.\nOur best ideas are often those that bridge between two different worlds!\nI don't insist that every definition combine just these particular ingredients of structure and purpose. But that specific mixture does have a peculiar virtue: it helps us bridge between the ends we seek and the means we have. That is, it helps us connect things we can recognize (or make, find, do, or think) to problems we want to solve. It would be of little use to know that X's exist without some way to find and use them.\nWhen we discussed accumulation, we saw that the concepts of furniture and money have reasonably compact functional definitions but accumulate many structural descriptions. Conversely, the concepts of square or circle have compact structural definitions but accumulate endless collections of possible uses.\nTo learn to use a new or unfamiliar word, you start by taking it to be a sign that there exists, inside some other person's mind, a structure you could use. But no matter how carefully it is explained, you must still rebuild that thought yourself, from materials already in your own mind. It helps to be given a good definition, but still you must mold and shape each new idea to suit your own existing skills \u2014 hoping to make it work for you the way it seems to work for those from whom you learn.\nWhat people call meanings do not usually correspond to particular and definite structures, but to connections among and across fragments of the great interlocking networks of connections and constraints among our agencies. Because these networks are constantly growing and changing, meanings are rarely sharp, and we cannot always expect to be able to define them in terms of compact sequences of words. Verbal explanations serve only as partial hints; we also have to learn from watching, working, playing \u2014 and thinking.",
    "type": "article",
    "title": "12.13 bridge-definitions",
    "tags": [
      {
        "score": 0.7099590301513672,
        "sentiment": 0,
        "count": 0,
        "label": "bridge",
        "uri": "https://diffbot.com/entity/XUFThsE1HPQiotZpA1idzfA"
      }
    ],
    "docId": 153579241879,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 34470756788,
    "gburl": "http://aurellem.org/society-of-mind/som-12.13.html-diffbotxyz3822158619",
    "lastCrawlTimeUTC": 1588761027,
    "timestamp": "Wed, 06 May 2020 10:30:27 GMT"
  },
  {
    "sentiment": 0.223,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-613882547",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.9.html",
    "html": "<p>We'll take the view that nothing can have meaning by itself, but only in relation to whatever other meanings we already know. One might complain that this has the quality of the old question, <em>Which came first, the chicken or the egg?</em> If each thing one knows depends on other things one knows, isn't that like castles built on air? What keeps them from all falling down, if none are tied to solid ground?</p>\n<p>Well, first, there's nothing basically wrong with the idea of a society in which each part lends meaning to the other parts. Some sets of thoughts are much like twisted ropes or woven cloths in which each strand holds others both together and apart. Consider all the music tunes you know. Among them you can surely find two tunes of which you like each one the more because of how it's similar to or different from the other one. Besides, no human mind remains entirely afloat. Later we'll see how our conceptions of space and time can be based entirely on networks of relationships, yet can still reflect the structure of reality.</p>\n<p>If every mind builds somewhat different things inside itself, how can any mind communicate with a different mind? In the end, surely, communication is a matter of degree but it is not always lamentable when different minds don't understand each other perfectly. For then, provided some communication remains, we can share the richness of each other's thoughts. What good would other people be if we were all identical? In any case, the situation is the same inside your mind &mdash; since even you yourself can never know precisely what you mean! How useless any thought would be if, afterward, your mind returned to the selfsame state. But that never happens, because every time we think about a certain thing, our thoughts go off in different ways.</p>\n<p>The secret of what anything means to us depends on how we've connected it to all the other things we know. That's why it's almost always wrong to seek the <em>real meaning</em> of anything. A thing with just one meaning has scarcely any meaning at all.</p>\n<p>An idea with a single sense can lead you along only one track. Then, if anything goes wrong, it just gets stuck &mdash; a thought that sits there in your mind with nowhere to go. That's why, when someone learns something <em>by rote</em> &mdash; that is, with no sensible connections &mdash; we say that they <em>don't really understand.</em> Rich meaning-networks, however, give you many different ways to go: if you can't solve a problem one way, you can try another. True, too many indiscriminate connections will turn a mind to mush. But well-connected meaning-structures let you turn ideas around in your mind, to consider alternatives and envision things from many perspectives until you find one that works. And that's what we mean by thinking!</p>",
    "text": "We'll take the view that nothing can have meaning by itself, but only in relation to whatever other meanings we already know. One might complain that this has the quality of the old question, Which came first, the chicken or the egg? If each thing one knows depends on other things one knows, isn't that like castles built on air? What keeps them from all falling down, if none are tied to solid ground?\nWell, first, there's nothing basically wrong with the idea of a society in which each part lends meaning to the other parts. Some sets of thoughts are much like twisted ropes or woven cloths in which each strand holds others both together and apart. Consider all the music tunes you know. Among them you can surely find two tunes of which you like each one the more because of how it's similar to or different from the other one. Besides, no human mind remains entirely afloat. Later we'll see how our conceptions of space and time can be based entirely on networks of relationships, yet can still reflect the structure of reality.\nIf every mind builds somewhat different things inside itself, how can any mind communicate with a different mind? In the end, surely, communication is a matter of degree but it is not always lamentable when different minds don't understand each other perfectly. For then, provided some communication remains, we can share the richness of each other's thoughts. What good would other people be if we were all identical? In any case, the situation is the same inside your mind \u2014 since even you yourself can never know precisely what you mean! How useless any thought would be if, afterward, your mind returned to the selfsame state. But that never happens, because every time we think about a certain thing, our thoughts go off in different ways.\nThe secret of what anything means to us depends on how we've connected it to all the other things we know. That's why it's almost always wrong to seek the real meaning of anything. A thing with just one meaning has scarcely any meaning at all.\nAn idea with a single sense can lead you along only one track. Then, if anything goes wrong, it just gets stuck \u2014 a thought that sits there in your mind with nowhere to go. That's why, when someone learns something by rote \u2014 that is, with no sensible connections \u2014 we say that they don't really understand. Rich meaning-networks, however, give you many different ways to go: if you can't solve a problem one way, you can try another. True, too many indiscriminate connections will turn a mind to mush. But well-connected meaning-structures let you turn ideas around in your mind, to consider alternatives and envision things from many perspectives until you find one that works. And that's what we mean by thinking!",
    "type": "article",
    "title": "6.9 heads in the clouds",
    "tags": [
      {
        "score": 0.5525190234184265,
        "sentiment": 0.17,
        "count": 2,
        "label": "That's Why",
        "uri": "https://diffbot.com/entity/XQrsBwfsfM8m8c8b5Ot87sg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 212206584210,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 111019753911,
    "gburl": "http://aurellem.org/society-of-mind/som-6.9.html-diffbotxyz974403733",
    "lastCrawlTimeUTC": 1588761125,
    "timestamp": "Wed, 06 May 2020 10:32:05 GMT"
  },
  {
    "sentiment": 0.71,
    "images": [
      {
        "naturalHeight": 137,
        "width": 413,
        "diffbotUri": "image|3|-392239933",
        "url": "http://aurellem.org/society-of-mind/illus/ch2/2-1.png",
        "naturalWidth": 413,
        "primary": true,
        "height": 137
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-46525618",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-2.1.html",
    "html": "<p>We saw that Builder's skill could be reduced to the simpler skills of Get and Put. Then we saw how these, in turn, could be made of even simpler ones. Get merely needs to Move the hand to Grasp the block that Find just found. Put only has to Move the hand so that it puts that block upon the tower top. So it might appear that all of Builder's functions have been <em>reduced</em> to things that simpler parts can do.</p>\n<p>But something important has been left out. Builder is not merely a collection of parts like Find, Get, Put, and all the rest. For Builder would not work at all unless those agents were linked to one another by a suitable network of interconnections.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch2/2-1.png\"/></figure>\n<p>Could you predict what Builder does from knowing just that left-hand list? Of course not: you must also know which agents work for which. Similarly, you couldn't predict what would happen in a human community from knowing only what each separate individual can do; you must also know how they are organized &mdash; that is, who talks to whom. And it's the same for understanding any large and complex thing. First, we must know how each separate part works. Second, we must know how each part interacts with those to which it is connected. And third we have to understand how all these local interactions combine to accomplish what that system does &mdash; as seen from the outside.</p>\n<p>In the case of the human brain, it will take a long time to solve these three kinds of problems. First we will have to understand how brain cells work. This will be difficult because there are hundreds of different types of brain cells. Then we'll have to understand how the cells of each type interact with the other types of cells to which they connect. There could be thousands of these different kinds of interactions. Then, finally, comes the hardest part: we'll also have to understand how our billions of brain cells are organized into societies. To do this, we'll need to develop many new theories and organizational concepts. The more we can find out about how our brains evolved from those of simpler animals, the easier that task will be.</p>",
    "text": "We saw that Builder's skill could be reduced to the simpler skills of Get and Put. Then we saw how these, in turn, could be made of even simpler ones. Get merely needs to Move the hand to Grasp the block that Find just found. Put only has to Move the hand so that it puts that block upon the tower top. So it might appear that all of Builder's functions have been reduced to things that simpler parts can do.\nBut something important has been left out. Builder is not merely a collection of parts like Find, Get, Put, and all the rest. For Builder would not work at all unless those agents were linked to one another by a suitable network of interconnections.\nCould you predict what Builder does from knowing just that left-hand list? Of course not: you must also know which agents work for which. Similarly, you couldn't predict what would happen in a human community from knowing only what each separate individual can do; you must also know how they are organized \u2014 that is, who talks to whom. And it's the same for understanding any large and complex thing. First, we must know how each separate part works. Second, we must know how each part interacts with those to which it is connected. And third we have to understand how all these local interactions combine to accomplish what that system does \u2014 as seen from the outside.\nIn the case of the human brain, it will take a long time to solve these three kinds of problems. First we will have to understand how brain cells work. This will be difficult because there are hundreds of different types of brain cells. Then we'll have to understand how the cells of each type interact with the other types of cells to which they connect. There could be thousands of these different kinds of interactions. Then, finally, comes the hardest part: we'll also have to understand how our billions of brain cells are organized into societies. To do this, we'll need to develop many new theories and organizational concepts. The more we can find out about how our brains evolved from those of simpler animals, the easier that task will be.",
    "type": "article",
    "title": "2.1 Components and connections",
    "tags": [
      {
        "score": 0.8264476656913757,
        "sentiment": 0.193,
        "count": 4,
        "label": "construction worker",
        "uri": "https://diffbot.com/entity/XelVgOc5QN4uPgw8FXzWHiA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5075506567955017,
        "sentiment": 0.342,
        "count": 2,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 5434065293,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 257919418791,
    "gburl": "http://aurellem.org/society-of-mind/som-2.1.html-diffbotxyz2019344698",
    "lastCrawlTimeUTC": 1588760938,
    "timestamp": "Wed, 06 May 2020 10:28:58 GMT"
  },
  {
    "sentiment": 0.206,
    "humanLanguage": "en",
    "diffbotUri": "article|3|773972819",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.9.html",
    "html": "<blockquote>Everyone can master a grief but he who has it. &mdash;William Shakespeare </blockquote>\n<p>Consider the plight of a mother with a new infant. Her baby will demand her time for many years. Sometimes she must wonder, <em>How does this baby justify such sacrifice?</em> Various answers come to mind: <em>Because I love it.</em> <em>Because someday it will care for me.</em> <em>Because it's here to carry on our line.</em> But reasoning rarely brings answers to such questions. Usually, those questions simply fade away as parents continue to nurture their children as though they were parts of their own bodies. Sometimes, though, strains may overwhelm the mechanisms that protect each child from harm, and this results in tragedies.</p>\n<p>These complex parent-to-child and child-to-parent bonds must be based on certain types of memory. Some memories are less changeable than others, and I suspect that attachment-bonds involve memory-records of a type that can be rapidly formed but then become peculiarly slow to change. On the child's side, perhaps these bonds are descended from the forms of learning called <em>imprinting,</em> with which many kinds of infant animals quickly learn to recognize their parents. On the parents' side, the adult animals of many species will reject infants not involved in bonding shortly after birth; then foster-parenting becomes impossible. Why should bonding memories be so hard to change? In animals, there usually are evolutionary disadvantages to raising the offspring of unrelated individuals. Human infants must develop under the additional constraint of requiring constant adult models as a basis for their personalities. Similar goal-affecting bonds could explain the often irresistible force of <em>peer pressure</em> in later life. Perhaps all such attachment-bonds exploit the same machinery.</p>\n<p>Many animals form other kinds of social bonds as well, like those in which an individual selects a mate and then remains attached to it for life. Many people do this, too, and a number of the ones who don't have been observed to select, instead, from among alternatives of seemingly similar appearance or character &mdash; as though those persons were attached, if not to individuals, to certain constant prototypes. Other people frequently find themselves enslaved by infatuations that some parts of their minds find unwelcome but cannot prevent or overcome; once formed, those memory-bonds will only slowly fade away. The time spans of our different sorts of memories evolved to suit, not our own needs, but those of our ancestors.</p>\n<p>We all know the seemingly inexorable time span of mourning, in which it often takes so long to accept the loss of those we love. Perhaps this, too, reflects the slowness of attachment-change, though it is only one factor. This could also be partially responsible for the prolonged psychological disability that can follow the experience of a physical, emotional, or sexual assault upon a person. One might ask, since there are so many other devastating aspects of such an experience, why it should involve any connection with attachment memory. I suspect that any form of intimacy, however unwelcome, has effects upon machinery shared by both attachment and sexuality,</p>\n<p>and is liable to disturb or disrupt the machinery with which we make relationships in ordinary life. No matter how brief that violent episode, it may lead to long derangements in our usual relationships, in part because those agencies are slow to change. It doesn't help very much for the victim to try to view the situation neutrally, because the rest of the mind cannot control those agencies; only time can reconstruct their normal functioning. It is an injury more terrible than loss of sight or limb, to lose the normal use of the agencies with which one builds one's own identity.</p>",
    "text": "Everyone can master a grief but he who has it. \u2014William Shakespeare\nConsider the plight of a mother with a new infant. Her baby will demand her time for many years. Sometimes she must wonder, How does this baby justify such sacrifice? Various answers come to mind: Because I love it. Because someday it will care for me. Because it's here to carry on our line. But reasoning rarely brings answers to such questions. Usually, those questions simply fade away as parents continue to nurture their children as though they were parts of their own bodies. Sometimes, though, strains may overwhelm the mechanisms that protect each child from harm, and this results in tragedies.\nThese complex parent-to-child and child-to-parent bonds must be based on certain types of memory. Some memories are less changeable than others, and I suspect that attachment-bonds involve memory-records of a type that can be rapidly formed but then become peculiarly slow to change. On the child's side, perhaps these bonds are descended from the forms of learning called imprinting, with which many kinds of infant animals quickly learn to recognize their parents. On the parents' side, the adult animals of many species will reject infants not involved in bonding shortly after birth; then foster-parenting becomes impossible. Why should bonding memories be so hard to change? In animals, there usually are evolutionary disadvantages to raising the offspring of unrelated individuals. Human infants must develop under the additional constraint of requiring constant adult models as a basis for their personalities. Similar goal-affecting bonds could explain the often irresistible force of peer pressure in later life. Perhaps all such attachment-bonds exploit the same machinery.\nMany animals form other kinds of social bonds as well, like those in which an individual selects a mate and then remains attached to it for life. Many people do this, too, and a number of the ones who don't have been observed to select, instead, from among alternatives of seemingly similar appearance or character \u2014 as though those persons were attached, if not to individuals, to certain constant prototypes. Other people frequently find themselves enslaved by infatuations that some parts of their minds find unwelcome but cannot prevent or overcome; once formed, those memory-bonds will only slowly fade away. The time spans of our different sorts of memories evolved to suit, not our own needs, but those of our ancestors.\nWe all know the seemingly inexorable time span of mourning, in which it often takes so long to accept the loss of those we love. Perhaps this, too, reflects the slowness of attachment-change, though it is only one factor. This could also be partially responsible for the prolonged psychological disability that can follow the experience of a physical, emotional, or sexual assault upon a person. One might ask, since there are so many other devastating aspects of such an experience, why it should involve any connection with attachment memory. I suspect that any form of intimacy, however unwelcome, has effects upon machinery shared by both attachment and sexuality,\nand is liable to disturb or disrupt the machinery with which we make relationships in ordinary life. No matter how brief that violent episode, it may lead to long derangements in our usual relationships, in part because those agencies are slow to change. It doesn't help very much for the victim to try to view the situation neutrally, because the rest of the mind cannot control those agencies; only time can reconstruct their normal functioning. It is an injury more terrible than loss of sight or limb, to lose the normal use of the agencies with which one builds one's own identity.",
    "type": "article",
    "title": "17.9 different spans of memories",
    "tags": [
      {
        "score": 0.6643093824386597,
        "sentiment": 0.866,
        "count": 1,
        "label": "Because I Love It",
        "uri": "https://diffbot.com/entity/XY4pceyqrMSenntOdAGlYhQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.6233497262001038,
        "sentiment": 0.94,
        "count": 1,
        "label": "William Shakespeare",
        "uri": "https://diffbot.com/entity/PjjLkNTA0O1KDotckPhZqlg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.6053658127784729,
        "sentiment": 0.887,
        "count": 2,
        "label": "baby",
        "uri": "https://diffbot.com/entity/X4MZ_yLLbO0ueNL-zoshUhg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5122372508049011,
        "sentiment": 0.55,
        "count": 4,
        "label": "bond",
        "uri": "https://diffbot.com/entity/XWjd5y-0BPiOxPq0SOWb0uA"
      }
    ],
    "docId": 45612794287,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 81534566817,
    "gburl": "http://aurellem.org/society-of-mind/som-17.9.html-diffbotxyz1250628081",
    "lastCrawlTimeUTC": 1588760960,
    "timestamp": "Wed, 06 May 2020 10:29:20 GMT"
  },
  {
    "sentiment": 0.149,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1665667577",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-14.4.html",
    "html": "<p>Once you set that nine-dot problem into a larger frame, you could solve it in a routine way, with only a little thought. What lets you reformulate such complex scenes so easily &mdash; once you've thought of doing it? It must be that your mind is constantly preparing ways to do such things by building up connections between different kinds of descriptions. Then, when you finally change your view to find another way to look at things, you can apply a lifetime of experience as easily as turning on a switch.</p>\n<p>This takes us back to the question of when to try to be a Reductionist or a Novelist. How do you decide when to quit after investing a great deal of effort in doing something a certain way? It would be bad to discard all that work just before you might find the answer &mdash; but there's no way to be sure when that will happen. Should people always try to break their well established, self-made mental bonds and try to think in less constricted ways? Of course not. It will usually do more harm than good.</p>\n<p>However, when you're really stuck, you may as well try wilder ways to find some new ideas. You might even consider using one of the systematic, therapylike disciplines that go under names like brainstorming, lateral thinking, meditation, and so forth. These can help, when people get badly stuck, by encouraging the search for new formulations. However, when you switch to unfamiliar views of things you may get new ideas, but you also put yourself in the situation of a novice; you become less able to judge which new ideas are likely to be compatible with any of your older skills.</p>\n<p>In any case, you must not be too quick to think, How stupid it was not to see that right away! Remember the principle of exceptions: it may be rash to change yourself too much just to accommodate a single strange experience. To take every exception seriously is to risk the loss of general rules that previous experience has shown to work most frequently. You must also be particularly wary of methods you can always use:</p>\n<p>Quit what you're doing. Find an easier problem. Take a rest. You'll feel better afterward. Simply wait. Eventually the situation will change. Start over again. Things may work out better the next time.</p>\n<p>Such methods are too general; they're things that one can always do, but they do not apply especially well to any particular problem. Sometimes they can help us get unstuck, but they must be barred from usual thought &mdash; or at least be given low priority. It isn't any accident that the things that we can <em>always</em> do are just the ones we should rarely do.</p>",
    "text": "Once you set that nine-dot problem into a larger frame, you could solve it in a routine way, with only a little thought. What lets you reformulate such complex scenes so easily \u2014 once you've thought of doing it? It must be that your mind is constantly preparing ways to do such things by building up connections between different kinds of descriptions. Then, when you finally change your view to find another way to look at things, you can apply a lifetime of experience as easily as turning on a switch.\nThis takes us back to the question of when to try to be a Reductionist or a Novelist. How do you decide when to quit after investing a great deal of effort in doing something a certain way? It would be bad to discard all that work just before you might find the answer \u2014 but there's no way to be sure when that will happen. Should people always try to break their well established, self-made mental bonds and try to think in less constricted ways? Of course not. It will usually do more harm than good.\nHowever, when you're really stuck, you may as well try wilder ways to find some new ideas. You might even consider using one of the systematic, therapylike disciplines that go under names like brainstorming, lateral thinking, meditation, and so forth. These can help, when people get badly stuck, by encouraging the search for new formulations. However, when you switch to unfamiliar views of things you may get new ideas, but you also put yourself in the situation of a novice; you become less able to judge which new ideas are likely to be compatible with any of your older skills.\nIn any case, you must not be too quick to think, How stupid it was not to see that right away! Remember the principle of exceptions: it may be rash to change yourself too much just to accommodate a single strange experience. To take every exception seriously is to risk the loss of general rules that previous experience has shown to work most frequently. You must also be particularly wary of methods you can always use:\nQuit what you're doing. Find an easier problem. Take a rest. You'll feel better afterward. Simply wait. Eventually the situation will change. Start over again. Things may work out better the next time.\nSuch methods are too general; they're things that one can always do, but they do not apply especially well to any particular problem. Sometimes they can help us get unstuck, but they must be barred from usual thought \u2014 or at least be given low priority. It isn't any accident that the things that we can always do are just the ones we should rarely do.",
    "type": "article",
    "title": "14.4 brainstorming",
    "docId": 162669805967,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 210384044461,
    "gburl": "http://aurellem.org/society-of-mind/som-14.4.html-diffbotxyz2091555089",
    "lastCrawlTimeUTC": 1588761004,
    "timestamp": "Wed, 06 May 2020 10:30:04 GMT"
  },
  {
    "sentiment": 0,
    "images": [
      {
        "naturalHeight": 240,
        "width": 391,
        "diffbotUri": "image|3|-1345758238",
        "url": "http://aurellem.org/society-of-mind/illus/appendix/Appendix-1.png",
        "naturalWidth": 391,
        "primary": true,
        "height": 240
      },
      {
        "naturalHeight": 271,
        "width": 479,
        "diffbotUri": "image|3|-1344834717",
        "url": "http://aurellem.org/society-of-mind/illus/appendix/Appendix-2.png",
        "naturalWidth": 479,
        "height": 271
      },
      {
        "naturalHeight": 266,
        "width": 443,
        "diffbotUri": "image|3|-1343911196",
        "url": "http://aurellem.org/society-of-mind/illus/appendix/Appendix-3.png",
        "naturalWidth": 443,
        "height": 266
      },
      {
        "naturalHeight": 118,
        "width": 479,
        "diffbotUri": "image|3|-1342064154",
        "url": "http://aurellem.org/society-of-mind/illus/appendix/Appendix-5.png",
        "naturalWidth": 479,
        "height": 118
      },
      {
        "naturalHeight": 188,
        "width": 357,
        "diffbotUri": "image|3|-1341140633",
        "url": "http://aurellem.org/society-of-mind/illus/appendix/Appendix-6.png",
        "naturalWidth": 357,
        "height": 188
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1952362434",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-appendix.html",
    "html": "<p>Sometimes we ask why people are so similar. At other times, we wonder why they differ so much from one to the next. Often we try to classify our differences into those with which we're born and those we later learn &mdash; and then we find ourselves arguing about which virtues are inherited, and which we acquire from experience. Most arguments about <em>nature vs. nurture</em> are based on two mistakes. One is to talk about intelligence as though a person's quality of mind were like a quantity one could pour into a cup. The other mistake is to assume that there is a clear distinction between what we learn and how we learn &mdash; as though experience had no effect on how we learn.</p>\n<p>Chance plays a major role in human differences, because each of us starts out by drawing lots from among our parents' genes. A gene is a unit of heredity &mdash; a specific chemical whose structure affects some aspects of the construction of the body and the brain. We inherit each of our genes from one parent or the other, more or less by chance, so as to receive about half of each parent's genes. Within the population as a whole, each particular kind of gene has variants that work in somewhat different ways, and there are so many possible combinations of these alternative genes that every child is born unique &mdash; except in the case of identical twins, who carry identical copies of genes. One thing that makes people both so different and so similar is this: we are similar because those alternative genes are usually quite similar &mdash; and we are different because those genes are not identical.</p>\n<p>Every cell of the body contains identical copies of all of that person's genes. But not all genes are active at the same time &mdash; and this is why the cells in our different organs do different things. When a particular gene is <em>turned on</em> inside a cell, that cell manufactures copies of a particular chemical (called a protein), whose structure is determined by the structure of that gene. Proteins are used in many ways. Some of them are assembled into permanent structures, some are involved in manufacturing other chemicals, and certain proteins move around in the cell, to serve as messages that alter other processes. Since certain combinations of these messages can turn other genes on and off, the gene-constructed chemicals in cells can act like small societies of agencies.</p>\n<p>Every cell has windows in its walls and special genes that control which chemicals can enter or leave through those windows. Then, certain of those chemicals can act as messages that change the states of specific genes in other cells. Thus groups of cells can also be societies. The effects of most of those messages between cells are temporary and reversible, but some of them can permanently change the character of other cells by altering the types of messages they can transmit and receive. In effect, this converts them into other <em>types</em> of cells. When new types of cells are produced this way, certain of them remain in place, but other types proceed to move and reproduce &mdash; to form new layers, strands, or clumps. Inside the brain, certain types of cells emit specific chemicals that drift out like scents; this causes certain other types of mobile cells that are sensitive to those particular chemicals to sniff out those scents and track them back to their sources &mdash; leaving tubelike trails behind. These traces of the travels of those migratory cells then form the nerve-bundles that interconnect various pairs of far-apart brain-agencies. With all this activity, the embryonic brain resembles a complex animal ecology &mdash; which even includes predators programmed to find and kill the many cells that happen to reach <em>wrong</em> destinations.</p>\n<p>All human brains are similar in size and shape but differ in many smaller respects because of different alternative genes. Why does the human population support so many variant genes? One reason is that genes are sometimes altered by accidents. When this happens to a gene that lies within a reproductive cell &mdash; that is, inside an ovum or a sperm &mdash; the change will be inherited. We call this a <em>mutation.</em> Most often, a mutant gene will simply fail to manufacture some vital chemical, and this will so badly impair the offspring that natural selection will quickly remove the mutated gene from the population. But occasionally a mutant gene will endow those offspring with some substantial advantage. Then natural selection will spread copies of that gene so widely among the population that its predecessor gene becomes extinct. Finally, a variant gene may provide an advantage only in certain circumstances; this type of mutation may spread only to a certain proportion of the population, and both the new and the old variants will continue to coexist indefinitely. The richness of this reservoir of alternative genes can determine how quickly a population adapts to changes in ecological conditions &mdash; and thus determines whether the entire species can escape extinction over longer periods of time.</p>\n<p>Now let's return to what genes do. Not all genes turn on at once; some start early and some start late. In general, the sooner a gene starts working, the greater its effect on what comes afterward. Accordingly, it is the early-starting genes that most affect the basic, large-scale architecture of our bodies and our brains. A mutation in an early-working gene is likely to cause such a drastic alteration of an animal's basic architecture that the embryo will not survive to be born, grow up, and reproduce successfully. Accordingly, most mutations of early-working genes are swiftly weeded out of the population by natural selection. Mutations in later-starting genes tend to cause less drastic differences, hence are not so swiftly weeded out and can accumulate in the population &mdash; for example, as variations in the genes that affect the sizes of connections between various brain-agencies. Every different combination of such variant genes produces a person with a somewhat different brain.</p>\n<p>The early-starting genes thus frame the large-scale outlines of the brain &mdash; and their uniformity explains why people are so similar on the broadest scale. These must be the genes responsible for what we call <em>human nature</em> &mdash; that is, the predispositions every normal person shares. Generally, the uniformity of early-starting genes is what makes all the members of each animal species seem so similar; indeed, it is partly why the earth is populated with distinct, recognizable species like lions, turtles, and people, rather than with an indistinct continuum of all conceivable animals. No human mother ever bears a cat, since that would require too many different early-starting genes.</p>\n<h2>the genesis of mental realms</h2>\n<p>All normal children come to recognize the same sorts of physical objects. Is that because the concept of an object is innate in the human mind? Each of us becomes attached to certain other individuals. Does this mean that the concept of person, and the notion of love, are part of our inheritance? Every human child forms <em>realms of thought</em> that represent the physical, possessional, and psychological. But how could genes build concepts into minds when genes themselves are merely linked-together chemicals?</p>\n<p>The problem is that thoughts proceed on levels so far removed from those of chemicals. This makes it hard for genes, which are merely chemicals, to represent such things as objects, persons, or ideas &mdash; at least in anything like the way that strings of words express our thoughts. Then how do genes encode ideas? The answer lies in the concept of <em>predestined learning</em> discussed in section 11.7. Although groups of genes cannot directly encode specific ideas, they can determine the architecture of agencies that are destined to learn particular kinds of processes. To illustrate this principle, we'll outline the architecture of an agency destined to learn to recognize a human individual.</p>\n<p>When we first introduced the concept of a recognizer, we suggested a simple way to represent a physical object in terms of properties like color, texture, size, and shape &mdash; by combining evidence from several agencies, each containing sensors especially designed to react to certain particular properties. Now we'll take another step, by dividing each of those agencies into two sections that are similar in architecture, and which both receive sensory inputs from the eyes, ears, skin, and nose. The first system is destined, as before, to learn to represent physical objects in terms of simple properties. However, because the second system's inputs come from different types of agents, it is destined to learn to represent <em>social objects</em> &mdash; that is, people.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/appendix/Appendix-1.png\"/></figure>\n<p>Our second <em>social object</em> agency takes all of its inputs from sensors that detect stimuli which usually signify the presence of a person &mdash; namely, human odors, voices, and faces. Because of this &mdash; and even though the genes that assemble this system know nothing of people &mdash; this system has no alternative but to learn to represent relations among the features of human individuals. Accordingly, this agency is destined to learn to recognize people!</p>\n<p>The large-scale outline of this agency poses no engineering mystery &mdash; but we have to ask how genes could produce the sensory detectors that the system needs to do its job. There is ample evidence that the recognition of both voices and faces does indeed take place in special sections of the brain &mdash; since certain injuries of the brain leave their victims unable to distinguish voice sounds yet still able to recognize many other kinds of sounds, while other brain injuries destroy the ability to recognize faces but leave other visual functions intact. No one yet knows just how these recognition-systems work, but let's consider each in turn.</p>\n<p>ODOR RECOGNITION: It is easy to build recognizers for particular odors because an odor is merely the presence of a certain combination of chemicals in the air, and a specific gene can make a cell sensitive to a particular chemical. So, to build agents for recognizing the odors of particular objects or people requires little more than connecting a variety of evidence-weighing</p>\n<p>agents to a variety of specific chemical detectors. VOICE RECOGNITION: To distinguish the sounds of a human voice requires more machinery because vocal expressions are complicated sequences of events. Machines have been built that can make such distinctions. FACE RECOGNITION: No one has yet been able to build vision machines that approach our human ability to distinguish faces from other objects &mdash; or even to distinguish dogs from cats. This remains a problem for research.</p>\n<p>In their first few days, human infants learn to distinguish people by their odors; then, over the next few weeks, they learn to recognize individuals by sound of voice; only after several months do they start to reliably distinguish the sights of faces. Most likely we learn to make each of these distinctions by several different methods, and it is probably no accident that these abilities develop in a sequence that corresponds to their increasing complexity.</p>\n<h2>gestures and trajectories</h2>\n<p>To recognize a voice or face seems hard enough; how does a child learn to recognize another person's mental state &mdash; of being angry or affectionate, for example. One way is by distinguishing trajectories. Just as we learn to interpret certain types of changes as representing the motions of objects in the physical realm, we learn to classify other types of changes as signifying mental events; these are what we call <em>gestures</em> and <em>expressions.</em> For example, to identify a certain sound to be a particular language-word, some agencies inside your brain must recognize that a certain sequence of phonetic features has occurred. At the same time, other agencies interpret sequences of vocal sounds as having significance in other realms. In particular, certain types of vocal sounds are recognized as signifying specific emotional qualities. For example, people almost universally agree on which expressions seem most angry or imperative. In general, abruptly changing sounds evoke alarm &mdash; perhaps by inducing the sort of narrowing of interest that comes with pain; in any case, sudden changes in volume and pitch demand that we attend to them. In contrast, we react to <em>gentle</em> sounds in ways that people label <em>positive,</em> as with affection, love, or reverence; the smoother time-trajectories do somehow serve to <em>calm us down,</em> thus frequently inducing us to set aside our other interests. It's quite the same for sight and touch; hostile persons tend to jab and shout, while friendly people speak and wave with gestures and trajectories that we perceive as signifying gentleness and tenderness. Indeed, as shown in Manfred Clynes's book, Sentics, Doubleday, New York, 1978, people show similar emotional responses to certain types of time-trajectories regardless of the sensory domain. We consistently identify certain sudden, jerky types of action patterns as indicating anger &mdash; regardless of whether these are presented as visual motions, as envelopes of voice sounds, or as pushing, shoving tactile stimuli. In the same way, we consistently identify certain other, smoother action patterns to indicate affection. Clynes concludes that at least half a dozen distinct types of trajectories are universally associated with particular emotional states. What sort of brain-machinery could cause us to react in such similar ways to such different kinds of stimuli? I propose a three-part hypothesis. First, each of our sensory-agencies is equipped with special agents that detect certain types of time-trajectories. Second, the outputs of all the agents that detect similar <em>trajectory types</em> in different agencies are connected, through special connection bundles, to converge upon agents in some central <em>gesture-recognizing</em> agency. Finally, genetically established nerve-bundles run from each gesture-recognizing agent to a particular infantile <em>proto-specialist</em> of the sort described in section 16. 3.</p>\n<p>According to this hypothesis, each sensory-agency contains agents that are specialized to react to various types of temporal trajectories. For example, one kind might react only to stimuli that increase slowly and then decrease quickly; another kind might react only to signals that increase quickly and decay slowly. Inside the brain, although the agencies for hearing, sight, and touch lie far apart, the signals from agents that detect similar trajectories converge on a common agency composed of evidence-weighing agents.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/appendix/Appendix-2.png\"/></figure>\n<p>Notice that the architecture of this system is so similar to that of our <em>person-recognizing</em> agency that the two systems could form parallel layers; however, the destiny of each central <em>trajectory-type</em> agent is to learn to recognize, not a particular person, but a particular type of gesture or expression. For example, one such agent might learn to react in similar ways to a snarl, grimace, or shaken fist &mdash; and thus become an <em>anger-recognizing</em> agent whose function is <em>abstract</em> in the sense of being detached from any particular class of sensations.</p>\n<p>To be sure, recognizing anger is not the same as comprehending or sympathizing with anger &mdash; nor does merely learning to make such a recognition, by itself, teach us to identify an <em>anger-type</em> trajectory of another person with our own personal experience of being angry. But if our genes equip us with connections from particular central trajectory-type agents to specific <em>proto-specialist</em> agencies, then each particular trajectory-type recognition would tend to activate a specific kind of emotional reaction.</p>\n<p>Some of these connections could endow us with certain <em>empathies</em> &mdash; for example, to feel elated upon recognizing another person's joyous gestures. Other connections could make us become defensive at signs of aggression &mdash; or even aggressive at signs of weakness and withdrawal. There are innumerable examples, in animal behavior, of particular types of gestures evoking <em>instinctive</em> types of reactions, as when a sudden motion toward a bird provokes a fear-reaction flight. Surely our human genes provide us with a great deal of instinctive wiring. However, far more than any other kind of animal, we also have machinery that can bridge new agencies across the older ones, so that we can learn to bury ancient instincts under modern social disciplines.</p>\n<p>We've seen how a gene-built agency could predispose us to use trajectory types to represent emotional and other sorts of states of mind. Once this is done, higher-level agencies could use the signals from trajectory-type agents to learn to recognize and represent more complex successions of mental states. In time, those representations could be assembled into models we could use for predicting and controlling our own mental processes. This illustrates how architectures framed by genes could serve our minds as stepping- stones toward learning how to think about ourselves.</p>\n<p>As soon as you enter a certain room, you may experience the feeling that you can directly sense its history. Many people attribute such perceptions to imaginary influences with names like <em>intuitions,</em> <em>spirits,</em> <em>atmospheres,</em> and <em>vibrations.</em> Yet very likely all such perceptions come from inside the mind of those observers, as various mental agencies accomplish clever syntheses from clues derived from features and trajectories. In my view, believing in vibrations and ghosts diminishes our capabilities for mental growth by diverting attention from the mind and attributing those abilities to imaginary entities outside ourselves.</p>\n<h2>brain connections</h2>\n<p>What possible sort of brain-machine could support a billion-agent society of mind? The human brain contains so many agencies and connections that it resembles a great nation of cities and towns, linked by vast networks of roads and highways. We are born with brain centers that are specialized for every sense and muscle group: for moving every eye and limb; for distinguishing the sounds of words, the features of faces, and all sorts of touches, tastes and smells. We're born with protospecialists involved with hunger, laughter, fear and anger, sleep, and sexual activity &mdash; and surely many other functions no one has discovered yet &mdash; each based upon a somewhat different architecture and mode of operation. Thousands of different genes must be involved in laying out these agencies and the nerve-bundles between them &mdash; and those brain-growth genes must generate at least three kinds of processes. Those genetic systems first must form the clumps and layers of brain cells that eventually become groups of agents; they must dictate the inner workings of those agencies; and, finally, they must determine the sizes and destinations of the nerve-bundles that interconnect those agencies &mdash; in order to constrain <em>who talks to whom</em> within each mind-society.</p>\n<p>Now every population will include some variants among the genes that shape those highways in the brain, and this must influence their bearers' potential styles of thought. A person born with unusually sparse connections between the agencies for sight and speech might develop powerful machinery in both those realms but find it hard to make direct connections between them. On the surface, that might seem to constitute a disability. However, it might also lead to an advantage &mdash; if it served to force one's higher-level agencies to seek out indirect connections that lead to more articulate ways to represent reality. Similarly, one might suppose there would be advantages in having an uncommonly large capacity for short-term memory. Yet for all we know, our evolution has disfavored that because it tends to lead to less effective use of hard-learned long-term memories. Other differences in how we think could stem from variations in connection paths. An individual whose K-lines had more branches than usual might become inclined to assemble larger-than-usual accumulations in cases where a person whose memory-agents had fewer branches might be more disposed toward building uniframes. But the same genetic disposition can lead to different styles of thought: one person who is genetically disposed toward making uniframes might succumb to the chronic use of superficial stereotypes, while another person similarly endowed might compensate by building more deeply layered agencies that lead to more profound ideas. Although each particular variation will dispose each individual toward certain traits of personality, the final effect of any gene depends upon how it interacts with the structures built by other genes &mdash; and upon countless other accidents. This makes it almost meaningless to ask which particular genes lead to <em>good</em> forms of thought. It is better to think of a developing brain as a forest within which many different creatures grow, in conflict and in harmony.</p>\n<p>Let's return to the architecture of machines that could hold societies of mind. How complicated this must be depends in part upon how many agents must be active at each moment. We can clarify the problem by considering two extremes. If only a few agents need to work at any time, then even an ordinary, serial, one-step-at-a-time computer could support billions of such agents &mdash; because each agent could be represented by a separate computer program. Then the computer itself could be quite simple, provided it has access to enough memory to hold all those little programs. On the other hand, no such arrangement would suffice to simulate societies of mind in which each of billions of agents constantly interact with all the others, all at once, because that would need more wires than any animal could carry in its head. I suspect that the human brain works somewhere in between; we do indeed have billions of nerve cells working at the same time, but relatively few of them have any need to communicate with more than a small proportion of the rest; this is simply because most agents are too specialized to deal with many types of messages. Accordingly, we'll propose an architecture that lies between those serial and parallel extremes &mdash; namely, a compromise in which a typical agent has comparatively few direct connections to other agents but can still influence a great many other agents through several indirect steps. For example, we can imagine a society in which each of a billion agents is connected to thirty other agents, selected at random. Then most pairs of agents should be able to communicate through merely half a dozen intermediates! This is because a typical agent can reach thirty others in one step, a thousand others in two steps, and a million others in only four steps. Thus a typical agent could reach any of the other billion agents in only six or seven steps!</p>\n<p>However, randomly selected connections would not be very useful, because very few randomly selected pairs of agents would have any messages that might be useful to one another. When we actually examine the human brain, we find that connections between cells are not made either uniformly or randomly. Instead, within any typical small region, we see a great many direct connections between nearby cells but only a relatively small number of bundles of connections to other regions of cells that lie farther away. Here is an idealized representation of this arrangement:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/appendix/Appendix-3.png\"/></figure>\n<p>An embryonic brain might assemble such a structure by repeating a sequence of cell divisions and migrations perhaps half a dozen times. If only that were done, the resulting structure would be uselessly repetitive. However, in a real brain's growth, this underlying building plan is modified at every step by many other processes, and this produces many agencies that are similar in general form but different in specific details. Some of these gene-controlled interventions modify the properties of specific layers and clumps of cells, and this determines the internal workings of particular agencies. Other interventions affect the sizes and destinations of the nerve-bundles that interconnect particular pairs of agencies. Such highway-routing processes could be used, for example, to lead the nerves that emerge from the trajectory-type sensors in different agencies to the same central destination. This would be quite easy to arrange because the trajectory agents of similar types would tend to have similar genetic origins &mdash; and that would predispose them to be able to <em>smell</em> the same varieties of embryonic message chemicals and thus grow toward the same destination.</p>\n<p>The same genetic argument can be applied to other aspects of a child's development &mdash; for example, to why all children seem to grow such similar Societies-of-More. When we discussed Jean Piaget's experiments, we left it as a mystery how children form the agencies called History and Appearance. What leads all those different minds to similar conceptions of comparisons? In section 10.7 we hinted that this might happen because similar agents like Tall and Thin originate in related sections of the brain. Consider that despite the fact that we do not know the brain-machinery for agents like Tall and Thin, we can be virtually sure that they are similar internally, because they both respond to the same sorts of spatial differences. Therefore, they almost surely have a common evolutionary origin and are constructed by the same or similar genes. Consequently, the embryonic brain cells that form these agents will tend to have similar <em>senses of smell</em> and are therefore likely to send out nerves that converge upon the same (or similar) agencies. From this point of view, the formation of a Spatial agency on which such properties converge need not be an unlikely chance event, but could be virtually predestined by inheritance.</p>\n<p>Papert's principle requires many agencies to grow by inserting new layers of agents into older, already working systems. But this poses a problem because, once brain cells reach maturity, they no longer have much mobility. Consequently, adding new layers to old agencies must involve using brain cells in other locations. As far as we know, the only way this could be done is by using connections already available in the neighborhood of the original agency. Here's one way embryonic cells could provide frameworks for future multilayered mind-societies:</p>\n<p>Any agency that is potentially capable of expanding to assimilate a lifetime of experience would need more space than any clump or layer of cells could provide in any compact neighborhood. This must be why the cerebral cortex &mdash; the newest and largest portion of the brain &mdash; evolved its convoluted form.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/appendix/Appendix-5.png\"/></figure>\n<p>If the connections in the cortex of the brain develop this way, through sequences of cell migrations, it could provide each local neighborhood with potential access to several other areas, through fanlike bundles and arrays of nerves. I have the impression the human cortex becomes thus folded upon itself perhaps five or six times, so that agents in each neighborhood have potential access to several other levels of convolution. This makes it possible for a typical agent to become connected to millions of other agents through only a few indirect connections. Presumably, only a small minority of cells ever actually acquire many such connections for their own exclusive use; however, such an arrangement makes any particular group of cells potentially capable of acquiring more significance &mdash; for example, by gaining control of a substantial bundle of connections that comes to represent some useful microneme. In its evolutionary course of making available so many potential connections, the human brain has actually gone so far that the major portion of its substance is no longer in its agencies but constitutes the enormous bundles of nerve fibers that potentially connect those agencies. The brain of Homo sapiens is mainly composed of cabling.</p>\n<h2>survival instinct</h2>\n<p>Many people seem to think that living things are born endowed with built-in instincts to survive. And certainly all animals do many things to stay alive. They build defenses against threats; they reproduce at any cost; they shy away from extremes of cold or heat, and from unfamiliarity. Now it usually is sensible, when one sees similarities, to seek some common cause for them. But I'll argue that it's usually wrong to seek a common force. There are many different reasons why animals do many things that help keep them alive &mdash; and, as we'll see, there is even a reason why there are so many different reasons. But to attribute this to any single, central force or to some basic, underlying survival instinct is as foolish as believing in special powers that attract corpses to cemeteries or broken cars to scrapyards.</p>\n<p>No animal requires any central reason to survive, nor does the process of evolution itself require any reason to produce all those survival aids. On the contrary, evolution's versatility stems from its very lack of any fixed direction or constraint that might restrict its possibilities.</p>\n<p>To understand why animals survive, one must see evolution as a sieve &mdash; that only passes through its mesh those animals who leave more offspring than the rest.</p>\n<p>Many people also think that evolution favors life &mdash; although it is a painful fact that most mutated animals must die before they reproduce. But hindsight makes us tend to count only the survivors we see, while overlooking all the misfits that have disappeared; it is the same mistake that one might make from looking only at the sky &mdash; to then conclude that all the animals were birds. The animals we see today are precisely those whose ancestors accumulated a great many survival aids &mdash; and that is why so much of their behavior seems directed toward promoting their welfare &mdash; if only in the surroundings in which their ancestors evolved. It is an illusion that all those accumulated mechanisms have anything in common; actually, that seeming uniformity has no coherence of its own: it is nothing but the shadow of that evolutionary sieve. The myth of an underlying survival instinct explains nothing that cannot better be explained without it, and blinds us to the fact that each of those survival aids may exploit an entirely different mechanism.</p>\n<p>I certainly don't mean to deny that people learn to love life and to fear death. But this is no simple matter of obeying some elemental instinct. It involves the development over many years of elaborate societies of concepts. Nor do I mean to say that people are born without any instincts at all and must learn everything from experience. On the contrary, we start with many built-in fragments of machinery, and these predestine us to learn to shy away from diverse forms of pain, discomfort, insecurity, and other forms of bodily and mental harm. But compared to those instinctive fears, the state of nonexistence we call death is a far more strange and difficult idea, of which no infant can conceive.</p>\n<h2>evolution and intent</h2>\n<p>Could animals have evolved as they did, if <em>nature</em> had no sense of goal? A century ago, the world of biologists split in two on one side stood the <em>evolutionists,</em> who held that animals evolve through nothing more than accidents of chance. Their antagonists were called the <em>teleologists</em>; they disbelieved that such excellent animals could evolve without any purposeful guidance. The evolutionists turned out to be right, for now we can watch small animals and plants evolve before our very eyes and, at a correspondingly slower pace, see similar developments in creatures that have longer lives. In fact, we can actually observe how random accidents to genes lead to the selective survival of particular individuals in various environments &mdash; without the faintest reason to suspect that any goals must be involved. So why do so many people feel that evolution must have purposes? I suspect that this belief is based on combining a sound insight about problem solving with an unsound image of how evolution works. For example, common sense tells us that a person might never hit upon a design for a flying machine entirely by trial and error, without having any goals or purposes. This leads one to suppose that nature, too, must be subject to that same constraint. The error comes from thinking of <em>nature</em> as being concerned with such problems as finding a way to make animals fly.</p>\n<p>The trouble is that this confuses uses with purposes. For example, suppose one asked how birds evolved, while thoughtlessly assuming that feathers and wings evolved exclusively for use in flight. One would be confronted with a formidable problem, since any organ as complex as a wing would require too many different genes to ever appear by random chance.</p>\n<p>So long as one's mind is fixed on flight, one might feel that the only solution is to find some flight advantage in each and every earlier stage that merely produced a protofeather or protowing too small and weak for actual flight. This is why so many antievolutionists demand that evolutionary advocates must fill in every imagined <em>gap</em> along a direct path toward a specified goal. However, once we abandon that fixed idea, it is easier to see how various intermediate developments could have provided those animals with advantages quite unrelated to flying. For example, the early ancestors of birds could have accumulated genes to manufacture various sorts of feathered appendages that helped to wrap those protobirds in body cloaks that kept them warm. This sort of fortuitous <em>preparation</em> unrelated to any goal of flight would have made it much more likely that other accidents, perhaps millions of years later, might have brought a few such elements together to lend some genuine aerial advantage to an animal already prone to making leaps.</p>\n<p>Incidentally, I do not mean to say that evolutionary processes must by their nature be devoid of purposes. We can actually conceive of how machinery could exist inside an animal, to purposefully direct some aspects of its evolution in much the way a farmer can promote the evolution of chickens that bear more meat or sheep that grow more wool. Indeed, the reproductive machinery inside our cells has already evolved so as to produce variations that are more likely to be useful than would otherwise occur by purely random chance; this idea is explained in a brilliant essay by Douglas Lenat, entitled <em>The role of Heuristics in learning by Discovery,</em> in Machine Learning: An Artificial Intelligence Approach, edited by R. Z. Michalski, J. J. Carbonell, and T. M. Mitchell; Tioga Publishing Co., Palo Alto, Calif., 1983. It is even conceivable that our genetic systems might even contain some forms of difference-engine-like machinery that, over very long periods of time, generate variations in a somewhat purposeful manner. To be sure, this is mere speculation, since no such system has yet been discovered.</p>\n<p>In any case, one aftermath of the controversy with teleologists was that many scientists in other realms became so afraid of making similar mistakes that the very concept of purpose became taboo throughout science. Even today, most scientists regard it as an abomination to use <em>anthropomorphic</em> or <em>intentional</em> language in connection with anything but persons or higher animals. This burdened the science of psychology with a double-barreled handicap. On one side, it made psychologists regard many of their most important problems as outside the scope of scientific explanation. On the other side, it deprived them of many useful technical ideas &mdash; because such concept-words as <em>want,</em> <em>expect,</em> and <em>recognize</em> are among the most effective ever found for describing what happens in human minds. It was not until the <em>cybernetic revolution</em> of the 1940s that scientists finally realized there is nothing inherently unscientific about the concept of goal itself and that attributing goals to evolution was bad not because it was impossible, but simply because it was wrong. Human minds do indeed use goal-machinery, and there is nothing wrong with recognizing this and bringing technical theories about intentions and goals into psychology.</p>\n<h2>insulation and interaction</h2>\n<blockquote> The hardest thing to understand is why we can understand anything at all. &mdash;Albert Einstein</blockquote>\n<p>What hope is there for any human mind to understand a human brain? No one could ever memorize the whole of all its small details. Our only hope is in formulating their principles. It wouldn't be much use, in any case, to know how each separate part works and how it interacts with the rest &mdash; because that simply isn't practical. Even if you knew all those details, if someone asked you to describe &mdash; in general terms &mdash; how brains work and how they change, you would have no way to reply.</p>\n<p>We usually like to think in positive terms about how various parts of systems interact. But to do that, we must first have good ideas about which aspects of a system do not interact &mdash; since otherwise there would be too many possibilities to consider. In other words, we have to understand insulations before we can comprehend interactions. To put this in a stronger form: No complicated society would actually work if it really depended on interactions among most of its parts. This is because any such system would be disabled by virtually any distortion, injury, or environmental fluctuation. Nor could any such society evolve in the first place.</p>\n<p>The science of biology was itself shaped by the discovery of insulations. Plants and animals were scarcely understood at all until it was found that they were made of separate cells. Then little more was learned so long as scientists thought of cells as bags of fluid within which countless chemicals could freely interact. Today we know that cells are more like factories, containing systems that are kept apart by sturdy walls, with doors that open only to those substances that bear the proper keys. Furthermore, even within these compartments, most pairs of chemicals cannot interact except by permission of particular genes. Without those insulations, so many chemicals would interact that all our cells would die.</p>\n<p>For the purposes of this book, I have emphasized highly insulated systems &mdash; that is, mechanisms in which different functions are embodied in different agents. However, it is important to put this in perspective. For example, in chapter 19, we drew a sharp distinction between memorizers and recognizers; this made it easy to explain those ideas. However, in section 20.9 we mentioned very briefly the idea of a <em>distributed memory,</em> in which both those functions are combined in the same network of agents. Now I do not want the reader to take the brevity of that discussion to suggest the subject is not important. On the contrary, I suspect that most of the human brain is actually composed of distributed learning-systems and that it is extremely important for us to understand how they can work. It is possible to combine even more functions; for example, John Hopfield has demonstrated a single distributed network that not only combines memory and recognition, but also <em>correctly yields an entire memory from any subpart of sufficient size</em> &mdash; in other words, an agency that <em>closes the ring,</em> much as described in section 19.10. See Hopfield's article in the Proceedings of the National Academy of Science, 79, p. 2554, 1982, or the book Parallel Distributed Processing by D. E. Rumelhart and J. L. McLelland, M.I.T. Press, 1986.</p>\n<p>The advantages of distributed systems are not alternatives to the advantages of insulated systems; the two are complementary. To say that the brain may be composed of distributed systems is not the same as saying that it is a distributed system &mdash; that is, a single network in which all functions are uniformly distributed. I do not believe any brain of that sort could work, because the interactions would be uncontrollable. To be sure, we have to explain how different ideas can become connected to one another &mdash; but we must also explain what keeps our separate memories intact. For example, we have praised the power of metaphors that allow us to combine ideas from different realms &mdash; but all that power would be lost if all our metaphors got mixed! Similarly, the architecture of a mind-society must encourage the formation and maintenance of distinct levels of management by preventing the formation of connections between agencies whose messages have no mutual significance. Some theorists have assumed that distributed systems are inherently both robust and versatile, but actually those attributes are likely to conflict. Systems with too many interactions of different types will tend to be fragile, while systems with too many interactions of similar types will be too redundant to adapt to novel situations and requirements. Finally, distributed systems tend to lack explicit, articulated representations, and this makes it difficult for any such agency to discover how any other such agency works. Thus, if distributed memory-systems are widely used within our brains, as I suspect they are, that could be yet another reason for the shallowness of human consciousness.</p>\n<h2>evolution of human thought</h2>\n<p>What are the origins of human thought? Today, we're almost sure that our closest living relatives branched out according to the diagram below. It shows that none of the species that still exist are directly descended from any of the others, but that they all share common ancestors, now long extinct.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/appendix/Appendix-6.png\"/></figure>\n<p>How different are we human beings from all the other animals? We recognize how similar those various brains and bodies are. But in view of our exceptional abilities to speak and think, we certainly seem to be unique. Could chimpanzees or gorillas ever learn to speak the way we do? Experience has shown that these wonderful animals can indeed learn to make connections among hundreds of different words and ideas, enabling them to produce speechlike strings of symbol- signs for expressing Trans-actions such as <em>Put the candy into the box.</em> However, the same experiments appear to show that these animals find it much more difficult to construct language-strings in which the terminals of certain frames are filled with other filled-in frames. In other words, no one has succeeded in teaching these animals to use expressions that involve interruption clauses, such as <em>Put the candy that is in the pail into the box.</em> To be sure, our inability to teach such things does not prove that these animals are inherently incapable of them. Still, no one can doubt that we have capabilities our ancestors did not possess. What sorts of brain developments could have given rise to our new and mighty forms of thought? Here are some possible candidates:</p>\n<p>The capacity to attach new K-lines to old ones enabled us to build hierarchical memory-trees. The availability of more versatile temporary memories enabled us to pursue subgoals and to tolerate more complicated kinds of interruptions. The evolution of paranomes &mdash; that is, of isonomes that bridge across multiple realms &mdash; enabled us to examine the same problem from several viewpoints.</p>\n<p>The emergence of additional layers of agents allowed each child to grow through more stages of development.</p>\n<p>None of those advances by itself would seem to pose any special evolutionary difficulty. But what could have caused so many changes to have appeared so rapidly? Our ancestors diverged from their relatives, the gorillas and the chimpanzees, only a few million years ago, and our human brains have grown substantially in only the last few hundred thousand years. Little is known of what happened in that interval because we have found very few fossil remains of our ancestors. (This could be partly because their population was never very large but could also be because they had become too smart to permit themselves to be fossilized.) The evolutionary interval was so brief that most of our genes and brain structures remain nearly the same as those of the chimpanzees. Was it merely an increase in the brain's size and capacity that produced our new abilities? Consider that, by itself, an increase in the size of the brain might only cause the disadvantage of mental confusion and the inconvenience of a heavier head. However, if we first evolved significant advances in the ability to manage our memories, we could then take advantage of more memory. Similarly, inserting new layers of agents into old agencies might only lead to bad results &mdash; unless this were preceded by mechanisms for using such layers as <em>middle- level managers</em> without disrupting older functions. In other words, our evolution must have worked the other way: first came enhancements in abilities that made it feasible for us to manage larger agencies. Then, once we had the capability for using more machinery, natural selection could favor those who grew more massive brains.</p>",
    "text": "Sometimes we ask why people are so similar. At other times, we wonder why they differ so much from one to the next. Often we try to classify our differences into those with which we're born and those we later learn \u2014 and then we find ourselves arguing about which virtues are inherited, and which we acquire from experience. Most arguments about nature vs. nurture are based on two mistakes. One is to talk about intelligence as though a person's quality of mind were like a quantity one could pour into a cup. The other mistake is to assume that there is a clear distinction between what we learn and how we learn \u2014 as though experience had no effect on how we learn.\nChance plays a major role in human differences, because each of us starts out by drawing lots from among our parents' genes. A gene is a unit of heredity \u2014 a specific chemical whose structure affects some aspects of the construction of the body and the brain. We inherit each of our genes from one parent or the other, more or less by chance, so as to receive about half of each parent's genes. Within the population as a whole, each particular kind of gene has variants that work in somewhat different ways, and there are so many possible combinations of these alternative genes that every child is born unique \u2014 except in the case of identical twins, who carry identical copies of genes. One thing that makes people both so different and so similar is this: we are similar because those alternative genes are usually quite similar \u2014 and we are different because those genes are not identical.\nEvery cell of the body contains identical copies of all of that person's genes. But not all genes are active at the same time \u2014 and this is why the cells in our different organs do different things. When a particular gene is turned on inside a cell, that cell manufactures copies of a particular chemical (called a protein), whose structure is determined by the structure of that gene. Proteins are used in many ways. Some of them are assembled into permanent structures, some are involved in manufacturing other chemicals, and certain proteins move around in the cell, to serve as messages that alter other processes. Since certain combinations of these messages can turn other genes on and off, the gene-constructed chemicals in cells can act like small societies of agencies.\nEvery cell has windows in its walls and special genes that control which chemicals can enter or leave through those windows. Then, certain of those chemicals can act as messages that change the states of specific genes in other cells. Thus groups of cells can also be societies. The effects of most of those messages between cells are temporary and reversible, but some of them can permanently change the character of other cells by altering the types of messages they can transmit and receive. In effect, this converts them into other types of cells. When new types of cells are produced this way, certain of them remain in place, but other types proceed to move and reproduce \u2014 to form new layers, strands, or clumps. Inside the brain, certain types of cells emit specific chemicals that drift out like scents; this causes certain other types of mobile cells that are sensitive to those particular chemicals to sniff out those scents and track them back to their sources \u2014 leaving tubelike trails behind. These traces of the travels of those migratory cells then form the nerve-bundles that interconnect various pairs of far-apart brain-agencies. With all this activity, the embryonic brain resembles a complex animal ecology \u2014 which even includes predators programmed to find and kill the many cells that happen to reach wrong destinations.\nAll human brains are similar in size and shape but differ in many smaller respects because of different alternative genes. Why does the human population support so many variant genes? One reason is that genes are sometimes altered by accidents. When this happens to a gene that lies within a reproductive cell \u2014 that is, inside an ovum or a sperm \u2014 the change will be inherited. We call this a mutation. Most often, a mutant gene will simply fail to manufacture some vital chemical, and this will so badly impair the offspring that natural selection will quickly remove the mutated gene from the population. But occasionally a mutant gene will endow those offspring with some substantial advantage. Then natural selection will spread copies of that gene so widely among the population that its predecessor gene becomes extinct. Finally, a variant gene may provide an advantage only in certain circumstances; this type of mutation may spread only to a certain proportion of the population, and both the new and the old variants will continue to coexist indefinitely. The richness of this reservoir of alternative genes can determine how quickly a population adapts to changes in ecological conditions \u2014 and thus determines whether the entire species can escape extinction over longer periods of time.\nNow let's return to what genes do. Not all genes turn on at once; some start early and some start late. In general, the sooner a gene starts working, the greater its effect on what comes afterward. Accordingly, it is the early-starting genes that most affect the basic, large-scale architecture of our bodies and our brains. A mutation in an early-working gene is likely to cause such a drastic alteration of an animal's basic architecture that the embryo will not survive to be born, grow up, and reproduce successfully. Accordingly, most mutations of early-working genes are swiftly weeded out of the population by natural selection. Mutations in later-starting genes tend to cause less drastic differences, hence are not so swiftly weeded out and can accumulate in the population \u2014 for example, as variations in the genes that affect the sizes of connections between various brain-agencies. Every different combination of such variant genes produces a person with a somewhat different brain.\nThe early-starting genes thus frame the large-scale outlines of the brain \u2014 and their uniformity explains why people are so similar on the broadest scale. These must be the genes responsible for what we call human nature \u2014 that is, the predispositions every normal person shares. Generally, the uniformity of early-starting genes is what makes all the members of each animal species seem so similar; indeed, it is partly why the earth is populated with distinct, recognizable species like lions, turtles, and people, rather than with an indistinct continuum of all conceivable animals. No human mother ever bears a cat, since that would require too many different early-starting genes.\nthe genesis of mental realms\nAll normal children come to recognize the same sorts of physical objects. Is that because the concept of an object is innate in the human mind? Each of us becomes attached to certain other individuals. Does this mean that the concept of person, and the notion of love, are part of our inheritance? Every human child forms realms of thought that represent the physical, possessional, and psychological. But how could genes build concepts into minds when genes themselves are merely linked-together chemicals?\nThe problem is that thoughts proceed on levels so far removed from those of chemicals. This makes it hard for genes, which are merely chemicals, to represent such things as objects, persons, or ideas \u2014 at least in anything like the way that strings of words express our thoughts. Then how do genes encode ideas? The answer lies in the concept of predestined learning discussed in section 11.7. Although groups of genes cannot directly encode specific ideas, they can determine the architecture of agencies that are destined to learn particular kinds of processes. To illustrate this principle, we'll outline the architecture of an agency destined to learn to recognize a human individual.\nWhen we first introduced the concept of a recognizer, we suggested a simple way to represent a physical object in terms of properties like color, texture, size, and shape \u2014 by combining evidence from several agencies, each containing sensors especially designed to react to certain particular properties. Now we'll take another step, by dividing each of those agencies into two sections that are similar in architecture, and which both receive sensory inputs from the eyes, ears, skin, and nose. The first system is destined, as before, to learn to represent physical objects in terms of simple properties. However, because the second system's inputs come from different types of agents, it is destined to learn to represent social objects \u2014 that is, people.\nOur second social object agency takes all of its inputs from sensors that detect stimuli which usually signify the presence of a person \u2014 namely, human odors, voices, and faces. Because of this \u2014 and even though the genes that assemble this system know nothing of people \u2014 this system has no alternative but to learn to represent relations among the features of human individuals. Accordingly, this agency is destined to learn to recognize people!\nThe large-scale outline of this agency poses no engineering mystery \u2014 but we have to ask how genes could produce the sensory detectors that the system needs to do its job. There is ample evidence that the recognition of both voices and faces does indeed take place in special sections of the brain \u2014 since certain injuries of the brain leave their victims unable to distinguish voice sounds yet still able to recognize many other kinds of sounds, while other brain injuries destroy the ability to recognize faces but leave other visual functions intact. No one yet knows just how these recognition-systems work, but let's consider each in turn.\nODOR RECOGNITION: It is easy to build recognizers for particular odors because an odor is merely the presence of a certain combination of chemicals in the air, and a specific gene can make a cell sensitive to a particular chemical. So, to build agents for recognizing the odors of particular objects or people requires little more than connecting a variety of evidence-weighing\nagents to a variety of specific chemical detectors. VOICE RECOGNITION: To distinguish the sounds of a human voice requires more machinery because vocal expressions are complicated sequences of events. Machines have been built that can make such distinctions. FACE RECOGNITION: No one has yet been able to build vision machines that approach our human ability to distinguish faces from other objects \u2014 or even to distinguish dogs from cats. This remains a problem for research.\nIn their first few days, human infants learn to distinguish people by their odors; then, over the next few weeks, they learn to recognize individuals by sound of voice; only after several months do they start to reliably distinguish the sights of faces. Most likely we learn to make each of these distinctions by several different methods, and it is probably no accident that these abilities develop in a sequence that corresponds to their increasing complexity.\ngestures and trajectories\nTo recognize a voice or face seems hard enough; how does a child learn to recognize another person's mental state \u2014 of being angry or affectionate, for example. One way is by distinguishing trajectories. Just as we learn to interpret certain types of changes as representing the motions of objects in the physical realm, we learn to classify other types of changes as signifying mental events; these are what we call gestures and expressions. For example, to identify a certain sound to be a particular language-word, some agencies inside your brain must recognize that a certain sequence of phonetic features has occurred. At the same time, other agencies interpret sequences of vocal sounds as having significance in other realms. In particular, certain types of vocal sounds are recognized as signifying specific emotional qualities. For example, people almost universally agree on which expressions seem most angry or imperative. In general, abruptly changing sounds evoke alarm \u2014 perhaps by inducing the sort of narrowing of interest that comes with pain; in any case, sudden changes in volume and pitch demand that we attend to them. In contrast, we react to gentle sounds in ways that people label positive, as with affection, love, or reverence; the smoother time-trajectories do somehow serve to calm us down, thus frequently inducing us to set aside our other interests. It's quite the same for sight and touch; hostile persons tend to jab and shout, while friendly people speak and wave with gestures and trajectories that we perceive as signifying gentleness and tenderness. Indeed, as shown in Manfred Clynes's book, Sentics, Doubleday, New York, 1978, people show similar emotional responses to certain types of time-trajectories regardless of the sensory domain. We consistently identify certain sudden, jerky types of action patterns as indicating anger \u2014 regardless of whether these are presented as visual motions, as envelopes of voice sounds, or as pushing, shoving tactile stimuli. In the same way, we consistently identify certain other, smoother action patterns to indicate affection. Clynes concludes that at least half a dozen distinct types of trajectories are universally associated with particular emotional states. What sort of brain-machinery could cause us to react in such similar ways to such different kinds of stimuli? I propose a three-part hypothesis. First, each of our sensory-agencies is equipped with special agents that detect certain types of time-trajectories. Second, the outputs of all the agents that detect similar trajectory types in different agencies are connected, through special connection bundles, to converge upon agents in some central gesture-recognizing agency. Finally, genetically established nerve-bundles run from each gesture-recognizing agent to a particular infantile proto-specialist of the sort described in section 16. 3.\nAccording to this hypothesis, each sensory-agency contains agents that are specialized to react to various types of temporal trajectories. For example, one kind might react only to stimuli that increase slowly and then decrease quickly; another kind might react only to signals that increase quickly and decay slowly. Inside the brain, although the agencies for hearing, sight, and touch lie far apart, the signals from agents that detect similar trajectories converge on a common agency composed of evidence-weighing agents.\nNotice that the architecture of this system is so similar to that of our person-recognizing agency that the two systems could form parallel layers; however, the destiny of each central trajectory-type agent is to learn to recognize, not a particular person, but a particular type of gesture or expression. For example, one such agent might learn to react in similar ways to a snarl, grimace, or shaken fist \u2014 and thus become an anger-recognizing agent whose function is abstract in the sense of being detached from any particular class of sensations.\nTo be sure, recognizing anger is not the same as comprehending or sympathizing with anger \u2014 nor does merely learning to make such a recognition, by itself, teach us to identify an anger-type trajectory of another person with our own personal experience of being angry. But if our genes equip us with connections from particular central trajectory-type agents to specific proto-specialist agencies, then each particular trajectory-type recognition would tend to activate a specific kind of emotional reaction.\nSome of these connections could endow us with certain empathies \u2014 for example, to feel elated upon recognizing another person's joyous gestures. Other connections could make us become defensive at signs of aggression \u2014 or even aggressive at signs of weakness and withdrawal. There are innumerable examples, in animal behavior, of particular types of gestures evoking instinctive types of reactions, as when a sudden motion toward a bird provokes a fear-reaction flight. Surely our human genes provide us with a great deal of instinctive wiring. However, far more than any other kind of animal, we also have machinery that can bridge new agencies across the older ones, so that we can learn to bury ancient instincts under modern social disciplines.\nWe've seen how a gene-built agency could predispose us to use trajectory types to represent emotional and other sorts of states of mind. Once this is done, higher-level agencies could use the signals from trajectory-type agents to learn to recognize and represent more complex successions of mental states. In time, those representations could be assembled into models we could use for predicting and controlling our own mental processes. This illustrates how architectures framed by genes could serve our minds as stepping- stones toward learning how to think about ourselves.\nAs soon as you enter a certain room, you may experience the feeling that you can directly sense its history. Many people attribute such perceptions to imaginary influences with names like intuitions, spirits, atmospheres, and vibrations. Yet very likely all such perceptions come from inside the mind of those observers, as various mental agencies accomplish clever syntheses from clues derived from features and trajectories. In my view, believing in vibrations and ghosts diminishes our capabilities for mental growth by diverting attention from the mind and attributing those abilities to imaginary entities outside ourselves.\nbrain connections\nWhat possible sort of brain-machine could support a billion-agent society of mind? The human brain contains so many agencies and connections that it resembles a great nation of cities and towns, linked by vast networks of roads and highways. We are born with brain centers that are specialized for every sense and muscle group: for moving every eye and limb; for distinguishing the sounds of words, the features of faces, and all sorts of touches, tastes and smells. We're born with protospecialists involved with hunger, laughter, fear and anger, sleep, and sexual activity \u2014 and surely many other functions no one has discovered yet \u2014 each based upon a somewhat different architecture and mode of operation. Thousands of different genes must be involved in laying out these agencies and the nerve-bundles between them \u2014 and those brain-growth genes must generate at least three kinds of processes. Those genetic systems first must form the clumps and layers of brain cells that eventually become groups of agents; they must dictate the inner workings of those agencies; and, finally, they must determine the sizes and destinations of the nerve-bundles that interconnect those agencies \u2014 in order to constrain who talks to whom within each mind-society.\nNow every population will include some variants among the genes that shape those highways in the brain, and this must influence their bearers' potential styles of thought. A person born with unusually sparse connections between the agencies for sight and speech might develop powerful machinery in both those realms but find it hard to make direct connections between them. On the surface, that might seem to constitute a disability. However, it might also lead to an advantage \u2014 if it served to force one's higher-level agencies to seek out indirect connections that lead to more articulate ways to represent reality. Similarly, one might suppose there would be advantages in having an uncommonly large capacity for short-term memory. Yet for all we know, our evolution has disfavored that because it tends to lead to less effective use of hard-learned long-term memories. Other differences in how we think could stem from variations in connection paths. An individual whose K-lines had more branches than usual might become inclined to assemble larger-than-usual accumulations in cases where a person whose memory-agents had fewer branches might be more disposed toward building uniframes. But the same genetic disposition can lead to different styles of thought: one person who is genetically disposed toward making uniframes might succumb to the chronic use of superficial stereotypes, while another person similarly endowed might compensate by building more deeply layered agencies that lead to more profound ideas. Although each particular variation will dispose each individual toward certain traits of personality, the final effect of any gene depends upon how it interacts with the structures built by other genes \u2014 and upon countless other accidents. This makes it almost meaningless to ask which particular genes lead to good forms of thought. It is better to think of a developing brain as a forest within which many different creatures grow, in conflict and in harmony.\nLet's return to the architecture of machines that could hold societies of mind. How complicated this must be depends in part upon how many agents must be active at each moment. We can clarify the problem by considering two extremes. If only a few agents need to work at any time, then even an ordinary, serial, one-step-at-a-time computer could support billions of such agents \u2014 because each agent could be represented by a separate computer program. Then the computer itself could be quite simple, provided it has access to enough memory to hold all those little programs. On the other hand, no such arrangement would suffice to simulate societies of mind in which each of billions of agents constantly interact with all the others, all at once, because that would need more wires than any animal could carry in its head. I suspect that the human brain works somewhere in between; we do indeed have billions of nerve cells working at the same time, but relatively few of them have any need to communicate with more than a small proportion of the rest; this is simply because most agents are too specialized to deal with many types of messages. Accordingly, we'll propose an architecture that lies between those serial and parallel extremes \u2014 namely, a compromise in which a typical agent has comparatively few direct connections to other agents but can still influence a great many other agents through several indirect steps. For example, we can imagine a society in which each of a billion agents is connected to thirty other agents, selected at random. Then most pairs of agents should be able to communicate through merely half a dozen intermediates! This is because a typical agent can reach thirty others in one step, a thousand others in two steps, and a million others in only four steps. Thus a typical agent could reach any of the other billion agents in only six or seven steps!\nHowever, randomly selected connections would not be very useful, because very few randomly selected pairs of agents would have any messages that might be useful to one another. When we actually examine the human brain, we find that connections between cells are not made either uniformly or randomly. Instead, within any typical small region, we see a great many direct connections between nearby cells but only a relatively small number of bundles of connections to other regions of cells that lie farther away. Here is an idealized representation of this arrangement:\nAn embryonic brain might assemble such a structure by repeating a sequence of cell divisions and migrations perhaps half a dozen times. If only that were done, the resulting structure would be uselessly repetitive. However, in a real brain's growth, this underlying building plan is modified at every step by many other processes, and this produces many agencies that are similar in general form but different in specific details. Some of these gene-controlled interventions modify the properties of specific layers and clumps of cells, and this determines the internal workings of particular agencies. Other interventions affect the sizes and destinations of the nerve-bundles that interconnect particular pairs of agencies. Such highway-routing processes could be used, for example, to lead the nerves that emerge from the trajectory-type sensors in different agencies to the same central destination. This would be quite easy to arrange because the trajectory agents of similar types would tend to have similar genetic origins \u2014 and that would predispose them to be able to smell the same varieties of embryonic message chemicals and thus grow toward the same destination.\nThe same genetic argument can be applied to other aspects of a child's development \u2014 for example, to why all children seem to grow such similar Societies-of-More. When we discussed Jean Piaget's experiments, we left it as a mystery how children form the agencies called History and Appearance. What leads all those different minds to similar conceptions of comparisons? In section 10.7 we hinted that this might happen because similar agents like Tall and Thin originate in related sections of the brain. Consider that despite the fact that we do not know the brain-machinery for agents like Tall and Thin, we can be virtually sure that they are similar internally, because they both respond to the same sorts of spatial differences. Therefore, they almost surely have a common evolutionary origin and are constructed by the same or similar genes. Consequently, the embryonic brain cells that form these agents will tend to have similar senses of smell and are therefore likely to send out nerves that converge upon the same (or similar) agencies. From this point of view, the formation of a Spatial agency on which such properties converge need not be an unlikely chance event, but could be virtually predestined by inheritance.\nPapert's principle requires many agencies to grow by inserting new layers of agents into older, already working systems. But this poses a problem because, once brain cells reach maturity, they no longer have much mobility. Consequently, adding new layers to old agencies must involve using brain cells in other locations. As far as we know, the only way this could be done is by using connections already available in the neighborhood of the original agency. Here's one way embryonic cells could provide frameworks for future multilayered mind-societies:\nAny agency that is potentially capable of expanding to assimilate a lifetime of experience would need more space than any clump or layer of cells could provide in any compact neighborhood. This must be why the cerebral cortex \u2014 the newest and largest portion of the brain \u2014 evolved its convoluted form.\nIf the connections in the cortex of the brain develop this way, through sequences of cell migrations, it could provide each local neighborhood with potential access to several other areas, through fanlike bundles and arrays of nerves. I have the impression the human cortex becomes thus folded upon itself perhaps five or six times, so that agents in each neighborhood have potential access to several other levels of convolution. This makes it possible for a typical agent to become connected to millions of other agents through only a few indirect connections. Presumably, only a small minority of cells ever actually acquire many such connections for their own exclusive use; however, such an arrangement makes any particular group of cells potentially capable of acquiring more significance \u2014 for example, by gaining control of a substantial bundle of connections that comes to represent some useful microneme. In its evolutionary course of making available so many potential connections, the human brain has actually gone so far that the major portion of its substance is no longer in its agencies but constitutes the enormous bundles of nerve fibers that potentially connect those agencies. The brain of Homo sapiens is mainly composed of cabling.\nsurvival instinct\nMany people seem to think that living things are born endowed with built-in instincts to survive. And certainly all animals do many things to stay alive. They build defenses against threats; they reproduce at any cost; they shy away from extremes of cold or heat, and from unfamiliarity. Now it usually is sensible, when one sees similarities, to seek some common cause for them. But I'll argue that it's usually wrong to seek a common force. There are many different reasons why animals do many things that help keep them alive \u2014 and, as we'll see, there is even a reason why there are so many different reasons. But to attribute this to any single, central force or to some basic, underlying survival instinct is as foolish as believing in special powers that attract corpses to cemeteries or broken cars to scrapyards.\nNo animal requires any central reason to survive, nor does the process of evolution itself require any reason to produce all those survival aids. On the contrary, evolution's versatility stems from its very lack of any fixed direction or constraint that might restrict its possibilities.\nTo understand why animals survive, one must see evolution as a sieve \u2014 that only passes through its mesh those animals who leave more offspring than the rest.\nMany people also think that evolution favors life \u2014 although it is a painful fact that most mutated animals must die before they reproduce. But hindsight makes us tend to count only the survivors we see, while overlooking all the misfits that have disappeared; it is the same mistake that one might make from looking only at the sky \u2014 to then conclude that all the animals were birds. The animals we see today are precisely those whose ancestors accumulated a great many survival aids \u2014 and that is why so much of their behavior seems directed toward promoting their welfare \u2014 if only in the surroundings in which their ancestors evolved. It is an illusion that all those accumulated mechanisms have anything in common; actually, that seeming uniformity has no coherence of its own: it is nothing but the shadow of that evolutionary sieve. The myth of an underlying survival instinct explains nothing that cannot better be explained without it, and blinds us to the fact that each of those survival aids may exploit an entirely different mechanism.\nI certainly don't mean to deny that people learn to love life and to fear death. But this is no simple matter of obeying some elemental instinct. It involves the development over many years of elaborate societies of concepts. Nor do I mean to say that people are born without any instincts at all and must learn everything from experience. On the contrary, we start with many built-in fragments of machinery, and these predestine us to learn to shy away from diverse forms of pain, discomfort, insecurity, and other forms of bodily and mental harm. But compared to those instinctive fears, the state of nonexistence we call death is a far more strange and difficult idea, of which no infant can conceive.\nevolution and intent\nCould animals have evolved as they did, if nature had no sense of goal? A century ago, the world of biologists split in two on one side stood the evolutionists, who held that animals evolve through nothing more than accidents of chance. Their antagonists were called the teleologists; they disbelieved that such excellent animals could evolve without any purposeful guidance. The evolutionists turned out to be right, for now we can watch small animals and plants evolve before our very eyes and, at a correspondingly slower pace, see similar developments in creatures that have longer lives. In fact, we can actually observe how random accidents to genes lead to the selective survival of particular individuals in various environments \u2014 without the faintest reason to suspect that any goals must be involved. So why do so many people feel that evolution must have purposes? I suspect that this belief is based on combining a sound insight about problem solving with an unsound image of how evolution works. For example, common sense tells us that a person might never hit upon a design for a flying machine entirely by trial and error, without having any goals or purposes. This leads one to suppose that nature, too, must be subject to that same constraint. The error comes from thinking of nature as being concerned with such problems as finding a way to make animals fly.\nThe trouble is that this confuses uses with purposes. For example, suppose one asked how birds evolved, while thoughtlessly assuming that feathers and wings evolved exclusively for use in flight. One would be confronted with a formidable problem, since any organ as complex as a wing would require too many different genes to ever appear by random chance.\nSo long as one's mind is fixed on flight, one might feel that the only solution is to find some flight advantage in each and every earlier stage that merely produced a protofeather or protowing too small and weak for actual flight. This is why so many antievolutionists demand that evolutionary advocates must fill in every imagined gap along a direct path toward a specified goal. However, once we abandon that fixed idea, it is easier to see how various intermediate developments could have provided those animals with advantages quite unrelated to flying. For example, the early ancestors of birds could have accumulated genes to manufacture various sorts of feathered appendages that helped to wrap those protobirds in body cloaks that kept them warm. This sort of fortuitous preparation unrelated to any goal of flight would have made it much more likely that other accidents, perhaps millions of years later, might have brought a few such elements together to lend some genuine aerial advantage to an animal already prone to making leaps.\nIncidentally, I do not mean to say that evolutionary processes must by their nature be devoid of purposes. We can actually conceive of how machinery could exist inside an animal, to purposefully direct some aspects of its evolution in much the way a farmer can promote the evolution of chickens that bear more meat or sheep that grow more wool. Indeed, the reproductive machinery inside our cells has already evolved so as to produce variations that are more likely to be useful than would otherwise occur by purely random chance; this idea is explained in a brilliant essay by Douglas Lenat, entitled The role of Heuristics in learning by Discovery, in Machine Learning: An Artificial Intelligence Approach, edited by R. Z. Michalski, J. J. Carbonell, and T. M. Mitchell; Tioga Publishing Co., Palo Alto, Calif., 1983. It is even conceivable that our genetic systems might even contain some forms of difference-engine-like machinery that, over very long periods of time, generate variations in a somewhat purposeful manner. To be sure, this is mere speculation, since no such system has yet been discovered.\nIn any case, one aftermath of the controversy with teleologists was that many scientists in other realms became so afraid of making similar mistakes that the very concept of purpose became taboo throughout science. Even today, most scientists regard it as an abomination to use anthropomorphic or intentional language in connection with anything but persons or higher animals. This burdened the science of psychology with a double-barreled handicap. On one side, it made psychologists regard many of their most important problems as outside the scope of scientific explanation. On the other side, it deprived them of many useful technical ideas \u2014 because such concept-words as want, expect, and recognize are among the most effective ever found for describing what happens in human minds. It was not until the cybernetic revolution of the 1940s that scientists finally realized there is nothing inherently unscientific about the concept of goal itself and that attributing goals to evolution was bad not because it was impossible, but simply because it was wrong. Human minds do indeed use goal-machinery, and there is nothing wrong with recognizing this and bringing technical theories about intentions and goals into psychology.\ninsulation and interaction\nThe hardest thing to understand is why we can understand anything at all. \u2014Albert Einstein\nWhat hope is there for any human mind to understand a human brain? No one could ever memorize the whole of all its small details. Our only hope is in formulating their principles. It wouldn't be much use, in any case, to know how each separate part works and how it interacts with the rest \u2014 because that simply isn't practical. Even if you knew all those details, if someone asked you to describe \u2014 in general terms \u2014 how brains work and how they change, you would have no way to reply.\nWe usually like to think in positive terms about how various parts of systems interact. But to do that, we must first have good ideas about which aspects of a system do not interact \u2014 since otherwise there would be too many possibilities to consider. In other words, we have to understand insulations before we can comprehend interactions. To put this in a stronger form: No complicated society would actually work if it really depended on interactions among most of its parts. This is because any such system would be disabled by virtually any distortion, injury, or environmental fluctuation. Nor could any such society evolve in the first place.\nThe science of biology was itself shaped by the discovery of insulations. Plants and animals were scarcely understood at all until it was found that they were made of separate cells. Then little more was learned so long as scientists thought of cells as bags of fluid within which countless chemicals could freely interact. Today we know that cells are more like factories, containing systems that are kept apart by sturdy walls, with doors that open only to those substances that bear the proper keys. Furthermore, even within these compartments, most pairs of chemicals cannot interact except by permission of particular genes. Without those insulations, so many chemicals would interact that all our cells would die.\nFor the purposes of this book, I have emphasized highly insulated systems \u2014 that is, mechanisms in which different functions are embodied in different agents. However, it is important to put this in perspective. For example, in chapter 19, we drew a sharp distinction between memorizers and recognizers; this made it easy to explain those ideas. However, in section 20.9 we mentioned very briefly the idea of a distributed memory, in which both those functions are combined in the same network of agents. Now I do not want the reader to take the brevity of that discussion to suggest the subject is not important. On the contrary, I suspect that most of the human brain is actually composed of distributed learning-systems and that it is extremely important for us to understand how they can work. It is possible to combine even more functions; for example, John Hopfield has demonstrated a single distributed network that not only combines memory and recognition, but also correctly yields an entire memory from any subpart of sufficient size \u2014 in other words, an agency that closes the ring, much as described in section 19.10. See Hopfield's article in the Proceedings of the National Academy of Science, 79, p. 2554, 1982, or the book Parallel Distributed Processing by D. E. Rumelhart and J. L. McLelland, M.I.T. Press, 1986.\nThe advantages of distributed systems are not alternatives to the advantages of insulated systems; the two are complementary. To say that the brain may be composed of distributed systems is not the same as saying that it is a distributed system \u2014 that is, a single network in which all functions are uniformly distributed. I do not believe any brain of that sort could work, because the interactions would be uncontrollable. To be sure, we have to explain how different ideas can become connected to one another \u2014 but we must also explain what keeps our separate memories intact. For example, we have praised the power of metaphors that allow us to combine ideas from different realms \u2014 but all that power would be lost if all our metaphors got mixed! Similarly, the architecture of a mind-society must encourage the formation and maintenance of distinct levels of management by preventing the formation of connections between agencies whose messages have no mutual significance. Some theorists have assumed that distributed systems are inherently both robust and versatile, but actually those attributes are likely to conflict. Systems with too many interactions of different types will tend to be fragile, while systems with too many interactions of similar types will be too redundant to adapt to novel situations and requirements. Finally, distributed systems tend to lack explicit, articulated representations, and this makes it difficult for any such agency to discover how any other such agency works. Thus, if distributed memory-systems are widely used within our brains, as I suspect they are, that could be yet another reason for the shallowness of human consciousness.\nevolution of human thought\nWhat are the origins of human thought? Today, we're almost sure that our closest living relatives branched out according to the diagram below. It shows that none of the species that still exist are directly descended from any of the others, but that they all share common ancestors, now long extinct.\nHow different are we human beings from all the other animals? We recognize how similar those various brains and bodies are. But in view of our exceptional abilities to speak and think, we certainly seem to be unique. Could chimpanzees or gorillas ever learn to speak the way we do? Experience has shown that these wonderful animals can indeed learn to make connections among hundreds of different words and ideas, enabling them to produce speechlike strings of symbol- signs for expressing Trans-actions such as Put the candy into the box. However, the same experiments appear to show that these animals find it much more difficult to construct language-strings in which the terminals of certain frames are filled with other filled-in frames. In other words, no one has succeeded in teaching these animals to use expressions that involve interruption clauses, such as Put the candy that is in the pail into the box. To be sure, our inability to teach such things does not prove that these animals are inherently incapable of them. Still, no one can doubt that we have capabilities our ancestors did not possess. What sorts of brain developments could have given rise to our new and mighty forms of thought? Here are some possible candidates:\nThe capacity to attach new K-lines to old ones enabled us to build hierarchical memory-trees. The availability of more versatile temporary memories enabled us to pursue subgoals and to tolerate more complicated kinds of interruptions. The evolution of paranomes \u2014 that is, of isonomes that bridge across multiple realms \u2014 enabled us to examine the same problem from several viewpoints.\nThe emergence of additional layers of agents allowed each child to grow through more stages of development.\nNone of those advances by itself would seem to pose any special evolutionary difficulty. But what could have caused so many changes to have appeared so rapidly? Our ancestors diverged from their relatives, the gorillas and the chimpanzees, only a few million years ago, and our human brains have grown substantially in only the last few hundred thousand years. Little is known of what happened in that interval because we have found very few fossil remains of our ancestors. (This could be partly because their population was never very large but could also be because they had become too smart to permit themselves to be fossilized.) The evolutionary interval was so brief that most of our genes and brain structures remain nearly the same as those of the chimpanzees. Was it merely an increase in the brain's size and capacity that produced our new abilities? Consider that, by itself, an increase in the size of the brain might only cause the disadvantage of mental confusion and the inconvenience of a heavier head. However, if we first evolved significant advances in the ability to manage our memories, we could then take advantage of more memory. Similarly, inserting new layers of agents into old agencies might only lead to bad results \u2014 unless this were preceded by mechanisms for using such layers as middle- level managers without disrupting older functions. In other words, our evolution must have worked the other way: first came enhancements in abilities that made it feasible for us to manage larger agencies. Then, once we had the capability for using more machinery, natural selection could favor those who grew more massive brains.",
    "type": "article",
    "title": "heredity and environment",
    "tags": [
      {
        "score": 0.5647680163383484,
        "sentiment": 0,
        "count": 1,
        "label": "natural environment",
        "uri": "https://diffbot.com/entity/Xk7rZ2HE3Mr2sYzs4OcuDcA"
      },
      {
        "score": 0.5490872263908386,
        "sentiment": 0,
        "count": 8,
        "label": "chemical industry",
        "uri": "https://diffbot.com/entity/XXn2MKyK4N_aCUuuDxeWRrw"
      },
      {
        "score": 0.5251778960227966,
        "sentiment": 0.791,
        "count": 3,
        "label": "alternative rock",
        "uri": "https://diffbot.com/entity/XNg9qs2H8OS-yusyfxhVKpw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Genre",
          "http://dbpedia.org/ontology/MusicGenre",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5218461155891418,
        "sentiment": 0,
        "count": 1,
        "label": "construction",
        "uri": "https://diffbot.com/entity/XJ7QO00VVPWWbivvkif55Uw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 179110969778,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 33892729267,
    "gburl": "http://aurellem.org/society-of-mind/som-appendix.html-diffbotxyz3050535743",
    "lastCrawlTimeUTC": 1588760982,
    "timestamp": "Wed, 06 May 2020 10:29:42 GMT"
  },
  {
    "sentiment": 0.149,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-445927278",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-9.3.html",
    "html": "<p>So far, we've talked mostly of learning from success. But consider that when you succeed, you must already have had the necessary means within your grasp. If so, then making changes in your mind might only make things worse! As people often say, <em>You shouldn't argue with success.</em> For whenever you try to <em>improve</em> an already working procedure, you risk damaging whichever other skills depend on that same machinery.</p>\n<p>Accordingly, it may be more important that we learn from how we fail. What should you do if some well-established method &mdash; call it <em>M</em> &mdash; has failed to reach a certain goal? One policy would be to alter M, so it won't make the same mistake again. But even that might be dangerous because it might cause M to fail in other ways. Besides, we might not know how to change M to remove the error. A safer way to deal with this would be to modify M by adding special memory devices called <em>censors</em> and <em>suppressors</em> (we'll discuss this in detail later), which remember particular circumstances in which M fails and later proceed to suppress M when similar conditions recur. Such censors would not tell you what to do, only what you shouldn't do; still, they prevent your wasting time by repeating old mistakes.</p>\n<p>Learning has at least two sides. Some parts of our minds learn from success &mdash; by remembering when methods work. But other portions of our minds learn mainly when we make mistakes, by remembering the circumstances in which various methods failed to work. Later we'll see how this can teach not only what we shouldn't do, but also what we shouldn't think! When that happens, it can permeate our minds with prohibitions and taboos of which we're entirely unaware. Thus, learning from success tends to aim and focus how we think, while learning from failure also leads to more productive thoughts, but in a less directed way.</p>\n<p>We would not need to deal with exceptions and censors if we lived in a universe of simple, general rules with no exceptions, as in the lovely mathematical worlds of arithmetic, geometry, and logic. But perfect logic rarely works in the real worlds of people, thoughts, and things.</p>\n<p>This is because it is no accident that there are no exceptions to the rules in those mathematical worlds: there, we start with the rules and imagine only objects that obey them. But we can't so willfully make up the rules for objects that already exist, so our only course is to begin with imperfect guesses &mdash; collections of rough and ready rules &mdash; and then proceed to find out where they're wrong.</p>\n<p>Naturally, we tend to prefer learning from success rather than from failure. However, I suspect that confining ourselves to <em>positive</em> learning experiences alone leads to relatively small improvements in what we can already do. Probably, there is no way to avoid at least a certain degree of discomfort when we make substantial changes in how we think.</p>",
    "text": "So far, we've talked mostly of learning from success. But consider that when you succeed, you must already have had the necessary means within your grasp. If so, then making changes in your mind might only make things worse! As people often say, You shouldn't argue with success. For whenever you try to improve an already working procedure, you risk damaging whichever other skills depend on that same machinery.\nAccordingly, it may be more important that we learn from how we fail. What should you do if some well-established method \u2014 call it M \u2014 has failed to reach a certain goal? One policy would be to alter M, so it won't make the same mistake again. But even that might be dangerous because it might cause M to fail in other ways. Besides, we might not know how to change M to remove the error. A safer way to deal with this would be to modify M by adding special memory devices called censors and suppressors (we'll discuss this in detail later), which remember particular circumstances in which M fails and later proceed to suppress M when similar conditions recur. Such censors would not tell you what to do, only what you shouldn't do; still, they prevent your wasting time by repeating old mistakes.\nLearning has at least two sides. Some parts of our minds learn from success \u2014 by remembering when methods work. But other portions of our minds learn mainly when we make mistakes, by remembering the circumstances in which various methods failed to work. Later we'll see how this can teach not only what we shouldn't do, but also what we shouldn't think! When that happens, it can permeate our minds with prohibitions and taboos of which we're entirely unaware. Thus, learning from success tends to aim and focus how we think, while learning from failure also leads to more productive thoughts, but in a less directed way.\nWe would not need to deal with exceptions and censors if we lived in a universe of simple, general rules with no exceptions, as in the lovely mathematical worlds of arithmetic, geometry, and logic. But perfect logic rarely works in the real worlds of people, thoughts, and things.\nThis is because it is no accident that there are no exceptions to the rules in those mathematical worlds: there, we start with the rules and imagine only objects that obey them. But we can't so willfully make up the rules for objects that already exist, so our only course is to begin with imperfect guesses \u2014 collections of rough and ready rules \u2014 and then proceed to find out where they're wrong.\nNaturally, we tend to prefer learning from success rather than from failure. However, I suspect that confining ourselves to positive learning experiences alone leads to relatively small improvements in what we can already do. Probably, there is no way to avoid at least a certain degree of discomfort when we make substantial changes in how we think.",
    "type": "article",
    "title": "9.3 learning from failure",
    "tags": [
      {
        "score": 0.5400508046150208,
        "sentiment": 0.161,
        "count": 1,
        "label": "In Your Mind",
        "uri": "https://diffbot.com/entity/XolPa3hQdPEKXyAQBzSo_Gw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Single"
        ]
      }
    ],
    "docId": 194897707392,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 230932595092,
    "gburl": "http://aurellem.org/society-of-mind/som-9.3.html-diffbotxyz3177121705",
    "lastCrawlTimeUTC": 1588760900,
    "timestamp": "Wed, 06 May 2020 10:28:20 GMT"
  },
  {
    "sentiment": 0.251,
    "humanLanguage": "en",
    "diffbotUri": "article|3|704720238",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.5.html",
    "html": "<p>We take it for granted that anyone can understand a story. But every kind of narrative demands some <em>listening skills.</em> Even the best storytellers find it hard to entertain children, who are prone to interrupt with questions that make perfect sense by themselves but drift away from the story's theme. <em>Where does Mary live?</em> <em>Does she have a dog?</em> To listen well, a child must acquire potent forms of self-control.</p>\n<p>The storyteller, too, must work to fix the focus of the listener's mind. If you were speaking about something else and suddenly, completely out of context, remarked, <em>Mary was invited to Jack's party,</em> an unprepared listener might wonder, <em>Mary who?</em> and look to see if you were addressing someone else. But you can first prepare the listener by saying, <em>Would you like to hear a story?</em> or simply, <em>Once upon a time . . .</em> What is the function of such a phrase? It has a very specific effect: to set the listener into a normal and familiar state of expecting to hear a certain type of narrative &mdash; a story. In the English tradition, stories typically begin by specifying the time &mdash; if only vaguely, by saying <em>long ago.</em> I'm told that in Japan most stories start with saying where as well &mdash; if only by some empty phrase like <em>in a certain time and place.</em> The biblical book of Job begins with, <em>There was a man in the land of Uz . . .</em></p>\n<p>Most stories start with just enough to set the scene. Then they introduce some characters, with hints about their principal concerns. Next, the storyteller gives some clues about some <em>main event</em> or problem to be solved. From that point on, the listener has a general idea of what comes next: there will be more development of the problem; then it will be resolved, somehow; and then the story will end, perhaps by giving some practical or moral advice. In any case, those magic story-starting words arouse, in knowing listeners' minds, great hosts of expectation-frames to help the listeners anticipate which terminals to fill.</p>\n<p>Beyond arousing all these specific expectations, <em>once upon a time</em> plays one more crucial role: it says that what comes after it is fictional or, in any case, far too remote to activate much personal concern. Instead, it tells the listener to disregard the normal sympathies one should feel when real persons meet the monstrous destinies so usual in children's tales: to be turned into toads, imprisoned in stones, or devoured by terrible dragon beasts.</p>",
    "text": "We take it for granted that anyone can understand a story. But every kind of narrative demands some listening skills. Even the best storytellers find it hard to entertain children, who are prone to interrupt with questions that make perfect sense by themselves but drift away from the story's theme. Where does Mary live? Does she have a dog? To listen well, a child must acquire potent forms of self-control.\nThe storyteller, too, must work to fix the focus of the listener's mind. If you were speaking about something else and suddenly, completely out of context, remarked, Mary was invited to Jack's party, an unprepared listener might wonder, Mary who? and look to see if you were addressing someone else. But you can first prepare the listener by saying, Would you like to hear a story? or simply, Once upon a time . . . What is the function of such a phrase? It has a very specific effect: to set the listener into a normal and familiar state of expecting to hear a certain type of narrative \u2014 a story. In the English tradition, stories typically begin by specifying the time \u2014 if only vaguely, by saying long ago. I'm told that in Japan most stories start with saying where as well \u2014 if only by some empty phrase like in a certain time and place. The biblical book of Job begins with, There was a man in the land of Uz . . .\nMost stories start with just enough to set the scene. Then they introduce some characters, with hints about their principal concerns. Next, the storyteller gives some clues about some main event or problem to be solved. From that point on, the listener has a general idea of what comes next: there will be more development of the problem; then it will be resolved, somehow; and then the story will end, perhaps by giving some practical or moral advice. In any case, those magic story-starting words arouse, in knowing listeners' minds, great hosts of expectation-frames to help the listeners anticipate which terminals to fill.\nBeyond arousing all these specific expectations, once upon a time plays one more crucial role: it says that what comes after it is fictional or, in any case, far too remote to activate much personal concern. Instead, it tells the listener to disregard the normal sympathies one should feel when real persons meet the monstrous destinies so usual in children's tales: to be turned into toads, imprisoned in stones, or devoured by terrible dragon beasts.",
    "type": "article",
    "title": "26.5 story-frames",
    "tags": [
      {
        "score": 0.8175165057182312,
        "sentiment": 0,
        "count": 10,
        "label": "narrative",
        "uri": "https://diffbot.com/entity/X-yV-ag8aOfmhY9T-yRt-wg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6797803044319153,
        "sentiment": 0.996,
        "count": 3,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5410527586936951,
        "sentiment": 0,
        "count": 1,
        "label": "Japan",
        "uri": "https://diffbot.com/entity/AFSw0ClDyP1axnPu4r17OUw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Country"
        ]
      },
      {
        "score": 0.5226285457611084,
        "sentiment": 0.682,
        "count": 1,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5203205347061157,
        "sentiment": 0.835,
        "count": 1,
        "label": "Once upon a time",
        "uri": "https://diffbot.com/entity/Xn-XM33XNMVCgcVEp5XKvZw",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      }
    ],
    "docId": 41694986665,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 114438652292,
    "gburl": "http://aurellem.org/society-of-mind/som-26.5.html-diffbotxyz1562867170",
    "lastCrawlTimeUTC": 1588760786,
    "timestamp": "Wed, 06 May 2020 10:26:26 GMT"
  },
  {
    "date": "Tue, 15 Oct 2019 00:00:00 GMT",
    "sentiment": -0.248,
    "humanLanguage": "en",
    "estimatedDate": "Tue, 15 Oct 2019 00:00:00 GMT",
    "diffbotUri": "article|3|1246184790",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.10.html",
    "html": "<p>Whenever we solve complicated problems, we get into situations in which our agencies must keep account of many processes at once. In computer programs, the many <em>subjobs</em> often seem to pile up like the blocks of a tower. Indeed, computer programmers often use the word <em>stack</em> to describe such situations. But I doubt that untrained human minds use anything so methodical; in fact, we simply aren't very good at dealing with the kinds of situations that need such memory-stacks. This could be why we get confused when hearing sentences like this:</p>\n<p>This is the malt that the rat that the cat that the dog worried killed ate.</p>\n<p>The very same words can be rearranged to make an equivalent sentence anyone can understand:</p>\n<p>This is the dog that worried the cat that killed the rat that ate the malt.</p>\n<p>The first sentence is hard to understand because so many verb processes interrupt one another that when the end of the sentence comes, three similar processes are still active &mdash; but they have lost track of what roles should be assigned to all the remaining nouns, namely, the rat, cat, and malt. Why do visual processes so rarely encounter similar difficulties? One reason is that our visual-systems can support more simultaneously operating processes than our language-systems can, and this reduces the need for any process to interrupt another one. A second reason is that the vision-agencies can choose for themselves the sequence in which they attend to details, whereas language-agencies are controlled by the person who is speaking.</p>\n<p>It takes each person many years to learn to use those memory- systems well. Younger children certainly cannot keep track as well as adults. It's generally of little use to ask a pair of two-year-olds to play together or to take turns at using a toy. We consider them to be too self-centered and impatient for that. Surely much of their undisciplined impulsiveness comes from desires that are less regulated than our own. But that impatience could also stem from insecurity about memory: the child may fear that what it wants will slip from mind while other thoughts are entertained. In other words, the child who is asked to <em>take turns</em> might fear that by the time its turn arrives, it may not want the object anymore.</p>\n<p>When people ask, <em>Could a machine ever be conscious?</em> I'm often tempted to ask back, <em>Could a person ever be conscious?</em> I mean this as a serious reply, because we seem so ill-equipped to understand ourselves. Long before we became concerned with understanding how we work, our evolution had already constrained the architecture of our brains. However, we can design our new machines as we wish, and provide them with better ways to keep and examine records of their own activities &mdash; and this means that machines are potentially capable of far more consciousness than we are. To be sure, simply providing machines with such information would not automatically enable them to use it to promote their own development, and until we can design more sensible machines, such knowledge might only help them find more ways to fail: the easier to change themselves, the easier to wreck themselves &mdash; until they learn to train themselves. Fortunately, we can leave this problem to the designers of the future, who surely would not build such things unless they found good reasons to.</p>",
    "text": "Whenever we solve complicated problems, we get into situations in which our agencies must keep account of many processes at once. In computer programs, the many subjobs often seem to pile up like the blocks of a tower. Indeed, computer programmers often use the word stack to describe such situations. But I doubt that untrained human minds use anything so methodical; in fact, we simply aren't very good at dealing with the kinds of situations that need such memory-stacks. This could be why we get confused when hearing sentences like this:\nThis is the malt that the rat that the cat that the dog worried killed ate.\nThe very same words can be rearranged to make an equivalent sentence anyone can understand:\nThis is the dog that worried the cat that killed the rat that ate the malt.\nThe first sentence is hard to understand because so many verb processes interrupt one another that when the end of the sentence comes, three similar processes are still active \u2014 but they have lost track of what roles should be assigned to all the remaining nouns, namely, the rat, cat, and malt. Why do visual processes so rarely encounter similar difficulties? One reason is that our visual-systems can support more simultaneously operating processes than our language-systems can, and this reduces the need for any process to interrupt another one. A second reason is that the vision-agencies can choose for themselves the sequence in which they attend to details, whereas language-agencies are controlled by the person who is speaking.\nIt takes each person many years to learn to use those memory- systems well. Younger children certainly cannot keep track as well as adults. It's generally of little use to ask a pair of two-year-olds to play together or to take turns at using a toy. We consider them to be too self-centered and impatient for that. Surely much of their undisciplined impulsiveness comes from desires that are less regulated than our own. But that impatience could also stem from insecurity about memory: the child may fear that what it wants will slip from mind while other thoughts are entertained. In other words, the child who is asked to take turns might fear that by the time its turn arrives, it may not want the object anymore.\nWhen people ask, Could a machine ever be conscious? I'm often tempted to ask back, Could a person ever be conscious? I mean this as a serious reply, because we seem so ill-equipped to understand ourselves. Long before we became concerned with understanding how we work, our evolution had already constrained the architecture of our brains. However, we can design our new machines as we wish, and provide them with better ways to keep and examine records of their own activities \u2014 and this means that machines are potentially capable of far more consciousness than we are. To be sure, simply providing machines with such information would not automatically enable them to use it to promote their own development, and until we can design more sensible machines, such knowledge might only help them find more ways to fail: the easier to change themselves, the easier to wreck themselves \u2014 until they learn to train themselves. Fortunately, we can leave this problem to the designers of the future, who surely would not build such things unless they found good reasons to.",
    "type": "article",
    "title": "15.10 losing track",
    "tags": [
      {
        "score": 0.5131080746650696,
        "sentiment": -0.918,
        "count": 3,
        "label": "track and field",
        "uri": "https://diffbot.com/entity/X4_zDwLp-P6eD0jnhrKVMVg"
      }
    ],
    "docId": 91212497337,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 157655843260,
    "gburl": "http://aurellem.org/society-of-mind/som-15.10.html-diffbotxyz1265281370",
    "lastCrawlTimeUTC": 1588760815,
    "timestamp": "Wed, 06 May 2020 10:26:55 GMT"
  },
  {
    "sentiment": 0.521,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-2039206523",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.7.html",
    "html": "<p>It's hard to recognize a thing when you're presented with too much detail. To know that you are seeing a kite, it helps to look for paper, sticks, and string. But if you were to use a microscope, what you'd perceive would not be properties of kites at all, but merely features of particular bits of paper, sticks or string. These might allow you to identify a particular kite but not to recognize any other kite. Past a certain level of detail, the more one sees, the less one can tell what one is seeing! The same applies to memories; they should weaken their attachments at lower levels of detail.</p>\n<p>Lower Band: Beyond a certain level of detail, increasingly complete memories of previous situations are increasingly difficult to match to new situations.</p>\n<p>To explain why K-lines need an upper-level fringe, let's return to that example in which our child originally learned how to build a tower &mdash; but now desires to build a house. Here, we could have another kind of difficulty if we remembered too much about our previous goals!</p>\n<p>Upper Band: Memories that arouse agents at too high a level would tend to provide us with goals that are not appropriate to the present situation.</p>\n<p>To see why our K-line memories should weaken their attachments above a certain level of detail, consider this most extreme form. Suppose some memory were so complete that it made you relive, in every detail, some perfect moment of your past. That would erase your present <em>you</em> &mdash; and you'd forget what you had asked your memory to do!</p>\n<p>Both fringing effects serve to make our memories more relevant to our present purposes. The central level-band helps us find general resemblances between remembered events and present circumstances. The lower fringe supplies additional details but does not force them upon us. We use them only <em>by default</em> when actual details are not supplied. Similarly, the upper fringe recalls to mind some memories of previous goals, but again, we're not forced to use them except by default, when present circumstances do not impose more compelling goals. Seen this way, we can think of the lower fringe as concerned with the structures of things, and we can think of the upper fringe as involved with the functions of things. The lower levels represent <em>objective</em> details of reality; the upper levels represent our <em>subjective</em> concerns with goals and intentions.</p>\n<p>How could the fringes of the same K-line lie in two such different realms? Because in order to think, we need intimate connections between things and goals &mdash; between structures and their functions. What use would thinking be at all, unless we could relate each thing's details to our plans and intentions? Consider how often the English language employs the selfsame words for things and for their purposes. What tools would you use, when building your house, to saw and clamp and glue your wood? That's obvious: you'd use a saw and a clamp and some glue! Behold the wondrous force of those <em>meanings</em>: no sooner do we hear the noun form of a word than our agents strain to perform the acts that correspond to it as a verb. This phenomenon of connecting means with ends is not confined to language &mdash; we'll see many other instances of it in other kinds of agencies &mdash; but language may allow such linking with the least constraint.</p>",
    "text": "It's hard to recognize a thing when you're presented with too much detail. To know that you are seeing a kite, it helps to look for paper, sticks, and string. But if you were to use a microscope, what you'd perceive would not be properties of kites at all, but merely features of particular bits of paper, sticks or string. These might allow you to identify a particular kite but not to recognize any other kite. Past a certain level of detail, the more one sees, the less one can tell what one is seeing! The same applies to memories; they should weaken their attachments at lower levels of detail.\nLower Band: Beyond a certain level of detail, increasingly complete memories of previous situations are increasingly difficult to match to new situations.\nTo explain why K-lines need an upper-level fringe, let's return to that example in which our child originally learned how to build a tower \u2014 but now desires to build a house. Here, we could have another kind of difficulty if we remembered too much about our previous goals!\nUpper Band: Memories that arouse agents at too high a level would tend to provide us with goals that are not appropriate to the present situation.\nTo see why our K-line memories should weaken their attachments above a certain level of detail, consider this most extreme form. Suppose some memory were so complete that it made you relive, in every detail, some perfect moment of your past. That would erase your present you \u2014 and you'd forget what you had asked your memory to do!\nBoth fringing effects serve to make our memories more relevant to our present purposes. The central level-band helps us find general resemblances between remembered events and present circumstances. The lower fringe supplies additional details but does not force them upon us. We use them only by default when actual details are not supplied. Similarly, the upper fringe recalls to mind some memories of previous goals, but again, we're not forced to use them except by default, when present circumstances do not impose more compelling goals. Seen this way, we can think of the lower fringe as concerned with the structures of things, and we can think of the upper fringe as involved with the functions of things. The lower levels represent objective details of reality; the upper levels represent our subjective concerns with goals and intentions.\nHow could the fringes of the same K-line lie in two such different realms? Because in order to think, we need intimate connections between things and goals \u2014 between structures and their functions. What use would thinking be at all, unless we could relate each thing's details to our plans and intentions? Consider how often the English language employs the selfsame words for things and for their purposes. What tools would you use, when building your house, to saw and clamp and glue your wood? That's obvious: you'd use a saw and a clamp and some glue! Behold the wondrous force of those meanings: no sooner do we hear the noun form of a word than our agents strain to perform the acts that correspond to it as a verb. This phenomenon of connecting means with ends is not confined to language \u2014 we'll see many other instances of it in other kinds of agencies \u2014 but language may allow such linking with the least constraint.",
    "type": "article",
    "title": "8.7 fringes",
    "tags": [
      {
        "score": 0.7228745222091675,
        "sentiment": -0.265,
        "count": 5,
        "label": "Fringe theatre",
        "uri": "https://diffbot.com/entity/XE-9P_DujNwKc-eFPpwcBew",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      },
      {
        "score": 0.6051971316337585,
        "sentiment": 0,
        "count": 2,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      },
      {
        "score": 0.5609134435653687,
        "sentiment": -0.295,
        "count": 1,
        "label": "information technology",
        "uri": "https://diffbot.com/entity/X3ffe7fPvOfKnzD0cOCsEaQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5578494071960449,
        "sentiment": -0.25,
        "count": 1,
        "label": "Marilyn Manson",
        "uri": "https://diffbot.com/entity/OURmlsLX_NwyxcvvoH6yDfQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      }
    ],
    "docId": 114040816008,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 1944699324,
    "gburl": "http://aurellem.org/society-of-mind/som-8.7.html-diffbotxyz3807696125",
    "lastCrawlTimeUTC": 1588760841,
    "timestamp": "Wed, 06 May 2020 10:27:21 GMT"
  },
  {
    "sentiment": 0.781,
    "images": [
      {
        "naturalHeight": 153,
        "width": 265,
        "diffbotUri": "image|3|1000940143",
        "url": "http://aurellem.org/society-of-mind/illus/ch14/14-16.png",
        "naturalWidth": 265,
        "primary": true,
        "height": 153
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1358789699",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-14.8.html",
    "html": "<p>What's so special about moving left or right or up or down? At first one might suppose that these ideas work only for motions in a two- dimensional space. But we can also use this square-like frame for many other realms of thought, to represent how pairs of causes interact. What is an interaction, anyway? We say that causes interact if, when combined, they lead to effects that neither can cause separately. For example, by combining horizontal and vertical motions, we can move to places that can't be reached with either kind of motion by itself. We can represent the effects of such combinations by using a diagram with labels like those on a compass.</p>\n<p>Many of our body joints can move in two independent directions at once &mdash; not the knee, but certainly the wrist, shoulder, hip, ankle, thumb, and eye. How do we learn to control such complicated joints? My hypothesis is that we do this by training little interaction-square agencies, which begin by learning something about each of the nine possible motion combinations. I suspect that we also base many of our nonphysical skills on interaction-square arrays because that is the simplest way to represent what happens when two causes interact. (There is even some evidence that many sections of the brain are composed of square arrays of smaller agencies. )</p>\n<p>Consider that the Spatial agency in our Society-of-More is not really involved with space at all, but with interactions between agents like Tall and Thin. If you were told that one object A is both taller and wider than another object B, you could be sure that there is <em>more</em> of A. But if you were told that A is taller and thinner than B, you couldn't be sure which one is <em>more.</em> An interaction-square array provides a convenient way to represent all the possible combinations:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch14/14-16.png\"/></figure>\n<p>If square-arrays can represent how pairs of causes interact, could similar schemes be used with three or more causes? That might need too many <em>directions</em> to be practical. We'd need twenty-seven directions to represent three interacting causes this way, and eighty-one to represent four. Only rarely, it seems, do people deal with more than two causes at a time; instead, we either find ways to reformulate such situations or we accumulate disorderly societies of partially filled interaction-squares that cover only the most commonly encountered combinations.</p>",
    "text": "What's so special about moving left or right or up or down? At first one might suppose that these ideas work only for motions in a two- dimensional space. But we can also use this square-like frame for many other realms of thought, to represent how pairs of causes interact. What is an interaction, anyway? We say that causes interact if, when combined, they lead to effects that neither can cause separately. For example, by combining horizontal and vertical motions, we can move to places that can't be reached with either kind of motion by itself. We can represent the effects of such combinations by using a diagram with labels like those on a compass.\nMany of our body joints can move in two independent directions at once \u2014 not the knee, but certainly the wrist, shoulder, hip, ankle, thumb, and eye. How do we learn to control such complicated joints? My hypothesis is that we do this by training little interaction-square agencies, which begin by learning something about each of the nine possible motion combinations. I suspect that we also base many of our nonphysical skills on interaction-square arrays because that is the simplest way to represent what happens when two causes interact. (There is even some evidence that many sections of the brain are composed of square arrays of smaller agencies. )\nConsider that the Spatial agency in our Society-of-More is not really involved with space at all, but with interactions between agents like Tall and Thin. If you were told that one object A is both taller and wider than another object B, you could be sure that there is more of A. But if you were told that A is taller and thinner than B, you couldn't be sure which one is more. An interaction-square array provides a convenient way to represent all the possible combinations:\nIf square-arrays can represent how pairs of causes interact, could similar schemes be used with three or more causes? That might need too many directions to be practical. We'd need twenty-seven directions to represent three interacting causes this way, and eighty-one to represent four. Only rarely, it seems, do people deal with more than two causes at a time; instead, we either find ways to reformulate such situations or we accumulate disorderly societies of partially filled interaction-squares that cover only the most commonly encountered combinations.",
    "type": "article",
    "title": "14.8 the interaction-square",
    "tags": [
      {
        "score": 0.5663092136383057,
        "sentiment": -0.383,
        "count": 2,
        "label": "joint",
        "uri": "https://diffbot.com/entity/Xx7KwN5dlMgmXZtub4JqqjQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 264266465727,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 162220933531,
    "gburl": "http://aurellem.org/society-of-mind/som-14.8.html-diffbotxyz35941275",
    "lastCrawlTimeUTC": 1588760874,
    "timestamp": "Wed, 06 May 2020 10:27:54 GMT"
  },
  {
    "sentiment": 0.518,
    "humanLanguage": "en",
    "diffbotUri": "article|3|500116629",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-23.3.html",
    "html": "<h2><em>23.3</em> time blinking</h2>\n<p>Fortunately, there is a way to get around the duplication problem entirely. Let's take a cue from how a perfume makes a strong impression first, but then appears to fade away, or how, when you put your hand in water that is very hot or very cold, the sensation is intense at first &mdash; but will soon disappear almost entirely. As we say, we <em>get used to</em> these sensations. Why? Because our senses react mainly to how things change in time. This is true even for the sensors in our eyes &mdash; though normally we're unaware of it because our eyes are always moving imperceptibly. Most of the sensory agents that inform our brains about the world are sensitive only to various sorts of time changes &mdash; and that, surely, is also true of most of the agents inside the brain.</p>\n<p>Any agent that is sensitive to changes in time can also be used to detect differences. For whenever we expose such an agent, first to a situation A and then to a situation B, any output from</p>\n<p>that agent will signify some difference between A and B.</p>\n<p>This suggests a way to solve the duplication problem. Since most agents can be made to serve as difference-agents, we can compare two descriptions simply by presenting them to the same agency at different times. This is easily done if that agency is equipped with a pair of high-speed, temporary K-line memories. Then we need only load the two descriptions into those memories and compare them by activating first one and then the other.</p>\n<p>Store the first description in pronome p. Store the second description in pronome q. Activate p and q in rapid succession. Then any changes in the agents' outputs represent differences between A and B!</p>\n<p>We can use this trick to implement the scheme we described for escaping from a topless-arch. Suppose that p describes the present situation and q describes a box that permits no escape. Each Move agent is designed to detect the appearance of a wall. If we simply <em>blink</em></p>\n<p>from the present situation to the box frame, one of these agents will announce the appearance of any box wall that was not already apparent in the present situation. Thus, automatically, this scheme will find all the directions that are not closed off. If the outputs of the Move agents were connected to cause you to move in the corresponding direction, this agency would lead you to escape!</p>\n<p>The method of time blinking can also be used to simplify our difference-engine scheme for composing verbal expressions, since now the speaker can maintain both p and q inside the selfsame agency. If not for this, each speaker would need what would amount to a duplicate society of mind in order to simulate the listener's state. Although the method of time blinking is powerful and efficient, it has some limitations; for example, it cannot directly recognize relations among more than two things at a time. I suspect that people share this limitation, too &mdash; and this may be why we have relatively few language-forms, like <em>between</em> and <em>middle,</em> for expressing three-way comparisons and relationships.</p>",
    "text": "23.3 time blinking\nFortunately, there is a way to get around the duplication problem entirely. Let's take a cue from how a perfume makes a strong impression first, but then appears to fade away, or how, when you put your hand in water that is very hot or very cold, the sensation is intense at first \u2014 but will soon disappear almost entirely. As we say, we get used to these sensations. Why? Because our senses react mainly to how things change in time. This is true even for the sensors in our eyes \u2014 though normally we're unaware of it because our eyes are always moving imperceptibly. Most of the sensory agents that inform our brains about the world are sensitive only to various sorts of time changes \u2014 and that, surely, is also true of most of the agents inside the brain.\nAny agent that is sensitive to changes in time can also be used to detect differences. For whenever we expose such an agent, first to a situation A and then to a situation B, any output from\nthat agent will signify some difference between A and B.\nThis suggests a way to solve the duplication problem. Since most agents can be made to serve as difference-agents, we can compare two descriptions simply by presenting them to the same agency at different times. This is easily done if that agency is equipped with a pair of high-speed, temporary K-line memories. Then we need only load the two descriptions into those memories and compare them by activating first one and then the other.\nStore the first description in pronome p. Store the second description in pronome q. Activate p and q in rapid succession. Then any changes in the agents' outputs represent differences between A and B!\nWe can use this trick to implement the scheme we described for escaping from a topless-arch. Suppose that p describes the present situation and q describes a box that permits no escape. Each Move agent is designed to detect the appearance of a wall. If we simply blink\nfrom the present situation to the box frame, one of these agents will announce the appearance of any box wall that was not already apparent in the present situation. Thus, automatically, this scheme will find all the directions that are not closed off. If the outputs of the Move agents were connected to cause you to move in the corresponding direction, this agency would lead you to escape!\nThe method of time blinking can also be used to simplify our difference-engine scheme for composing verbal expressions, since now the speaker can maintain both p and q inside the selfsame agency. If not for this, each speaker would need what would amount to a duplicate society of mind in order to simulate the listener's state. Although the method of time blinking is powerful and efficient, it has some limitations; for example, it cannot directly recognize relations among more than two things at a time. I suspect that people share this limitation, too \u2014 and this may be why we have relatively few language-forms, like between and middle, for expressing three-way comparisons and relationships.",
    "type": "article",
    "title": "23.3 time blinking",
    "tags": [
      {
        "score": 0.5819284319877625,
        "sentiment": 0.213,
        "count": 2,
        "label": "Sydney Trains A & B sets",
        "uri": "https://diffbot.com/entity/LVFGdn9jYP7uPmae4XPWnRw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/MeanOfTransportation",
          "http://dbpedia.org/ontology/Train"
        ]
      },
      {
        "score": 0.5417992472648621,
        "sentiment": 0.757,
        "count": 4,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 28397502863,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 70529171847,
    "gburl": "http://aurellem.org/society-of-mind/som-23.3.html-diffbotxyz2742975245",
    "lastCrawlTimeUTC": 1588760672,
    "timestamp": "Wed, 06 May 2020 10:24:32 GMT"
  },
  {
    "date": "Tue, 22 Oct 2019 00:00:00 GMT",
    "sentiment": -0.586,
    "humanLanguage": "en",
    "estimatedDate": "Tue, 22 Oct 2019 00:00:00 GMT",
    "diffbotUri": "article|3|373169021",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.10.html",
    "html": "<p>How easily people can communicate. We listen and speak without the slightest sense of what's involved! One of us expresses an idea, the other understands it, and neither thinks anything complicated has happened; it seems as natural to talk as it is to walk. Yet both simplicities are illusions. To walk, you must engage a vast array of agencies to move your body down the street. To talk, you must engage a vast array of agencies to build new structures in another person's mind. But how do you know just what to say to affect that other person's agencies?</p>\n<p>Let's suppose that Mary wants to tell Jack something. This means there is a certain structure p somewhere inside the network of Mary's agencies &mdash; and that Mary's language-agency must construct a similar structure inside Jack's mind. To do this, Mary will need to speak words that will activate appropriate activities inside Jack's agencies, then correctly link them together. How can she do that? Here is what we'll call the <em>re-duplication</em> theory of how we formulate what we say:</p>\n<p>Mary proceeds, step by step, to construct a new version of p &mdash; call it q &mdash; inside her own mind. In doing this, she will apply various memory-control operations to activate certain isonomes and polynemes. As Mary performs each internal operation, her speech-agency selects certain corresponding verbal expressions &mdash; and these cause similar operations to occur inside Jack. As a result, Jack builds a structure similar to q. To be able to do that, Mary must have learned at least one expressive technique that corresponds to each frequently used mental operation. And Jack must have learned to recognize those expressive techniques &mdash; we'll call them grammar-tactics &mdash; and to use them to activate some corresponding isonomes and polynemes.</p>\n<p>To build her new version of p, Mary could employ a goal-achieving scheme: she keeps comparing p with the latest version of q, and whenever she senses a significant difference, she applies some operation to q that removes or reduces the difference. For example, if Mary notices that p has an Origin pronome where q lacks one, her memory-control system will focus on p's Origin. In this case, if p itself is a motion frame, the usual speech-tactic is to use the word <em>from.</em> Next she must describe the substructure attached to p's Origin pronome. If this were a simple polyneme like <em>Boston,</em> Mary's speech-agency could simply pronounce the corresponding word. But if that pronome is assigned to some more complicated structure, such as an entire frame, Mary's language-agency must interrupt itself to copy that. This is expressed, as we have seen, by using words like <em>who</em> or <em>which.</em> In any case, Mary continues this difference- duplication process until she senses no significant discrepancies between q and p. Of course, what Mary finds significant depends on what she <em>wants to say.</em></p>\n<p>This <em>re-duplication</em> theory of speech describes only the first stages of how we use language. In later stages, the mental operations we use to construct q are not always immediately applied to pronouncing words. Instead, we learn techniques for storing sequences of grammar-tactics temporarily; this makes it possible to modify and rearrange our words and sentences before we say them. Learning these arts takes a long time: most children need a decade or more to complete their language-systems and many keep learning, throughout their lives, to sense new sorts of discrepancies and discover ways to express them.</p>",
    "text": "How easily people can communicate. We listen and speak without the slightest sense of what's involved! One of us expresses an idea, the other understands it, and neither thinks anything complicated has happened; it seems as natural to talk as it is to walk. Yet both simplicities are illusions. To walk, you must engage a vast array of agencies to move your body down the street. To talk, you must engage a vast array of agencies to build new structures in another person's mind. But how do you know just what to say to affect that other person's agencies?\nLet's suppose that Mary wants to tell Jack something. This means there is a certain structure p somewhere inside the network of Mary's agencies \u2014 and that Mary's language-agency must construct a similar structure inside Jack's mind. To do this, Mary will need to speak words that will activate appropriate activities inside Jack's agencies, then correctly link them together. How can she do that? Here is what we'll call the re-duplication theory of how we formulate what we say:\nMary proceeds, step by step, to construct a new version of p \u2014 call it q \u2014 inside her own mind. In doing this, she will apply various memory-control operations to activate certain isonomes and polynemes. As Mary performs each internal operation, her speech-agency selects certain corresponding verbal expressions \u2014 and these cause similar operations to occur inside Jack. As a result, Jack builds a structure similar to q. To be able to do that, Mary must have learned at least one expressive technique that corresponds to each frequently used mental operation. And Jack must have learned to recognize those expressive techniques \u2014 we'll call them grammar-tactics \u2014 and to use them to activate some corresponding isonomes and polynemes.\nTo build her new version of p, Mary could employ a goal-achieving scheme: she keeps comparing p with the latest version of q, and whenever she senses a significant difference, she applies some operation to q that removes or reduces the difference. For example, if Mary notices that p has an Origin pronome where q lacks one, her memory-control system will focus on p's Origin. In this case, if p itself is a motion frame, the usual speech-tactic is to use the word from. Next she must describe the substructure attached to p's Origin pronome. If this were a simple polyneme like Boston, Mary's speech-agency could simply pronounce the corresponding word. But if that pronome is assigned to some more complicated structure, such as an entire frame, Mary's language-agency must interrupt itself to copy that. This is expressed, as we have seen, by using words like who or which. In any case, Mary continues this difference- duplication process until she senses no significant discrepancies between q and p. Of course, what Mary finds significant depends on what she wants to say.\nThis re-duplication theory of speech describes only the first stages of how we use language. In later stages, the mental operations we use to construct q are not always immediately applied to pronouncing words. Instead, we learn techniques for storing sequences of grammar-tactics temporarily; this makes it possible to modify and rearrange our words and sentences before we say them. Learning these arts takes a long time: most children need a decade or more to complete their language-systems and many keep learning, throughout their lives, to sense new sorts of discrepancies and discover ways to express them.",
    "type": "article",
    "title": "22.10 verbal expression",
    "tags": [
      {
        "score": 0.7031441330909729,
        "sentiment": 0.812,
        "count": 13,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.7028740048408508,
        "sentiment": 0.147,
        "count": 4,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.534812331199646,
        "sentiment": 0.118,
        "count": 1,
        "label": "Get Me Bodied",
        "uri": "https://diffbot.com/entity/XQyTksE9mP4GtiAwVcBCCDg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Single"
        ]
      }
    ],
    "docId": 146561515938,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 219909128619,
    "gburl": "http://aurellem.org/society-of-mind/som-22.10.html-diffbotxyz1723126068",
    "lastCrawlTimeUTC": 1588760699,
    "timestamp": "Wed, 06 May 2020 10:24:59 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|942721269",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-29.6.html",
    "html": "<p>Isn't it curious that infants find social goals easier to accomplish than physical goals, while adults find the social goals more difficult? One way to explain this is to say that the presence of helpful people simplifies the infant's social world &mdash; since because of them, simpler actions solve harder problems. Another explanation might be that the infant's social world is just as complicated as that of the adult, except the presence of helpful people makes the infant's mind more powerful &mdash; by making the agencies inside those other people's brains available for exploitation by the agencies in the infant's brain. Both explanations are the same, except for drawing different boundaries.</p>\n<p>How do children start on the path toward distinguishing between psychological and physical relationships? In the appendix I'll suggest that our infant brains are genetically equipped with machinery for making it easy to learn social signals. But what if that machinery should somehow fail, so that by chance &mdash; or by neglect or accident &mdash; the realm-divisions never form? Then all those different kinds of thoughts would fuse together into one &mdash; and the child would face the impossible task of formulating principles that work in all domains. A child that tried to see the world without dividing it into realms would find no simple rules at all that work across so large a range.</p>\n<p>This is why each child must learn different rules for the physical and psychological realms. But this means that the child must face not merely two formidable problems, but three. In addition to developing two different sets of concepts, the child must also develop agencies to manage those concepts by keeping them apart in different agencies, as we saw when we talked about Papert's principle.</p>\n<p>This could explain some aspects of the disorders of the children psychiatrists call <em>autistic.</em> These unhappy individuals do not establish effective communication with other people, although they may acquire some competence at dealing with physical things. No one knows the causes of those disorders. Some might begin when certain mental realms do not develop normally. Other kinds of problems could emerge after those divisions form, if their separateness were compromised by some too intense attempt to unify them. To be sure, that is what scientists do, but unlike those whom we regard as mentally ill, scientists also manage to maintain their ordinary views. Once a child is deprived of the normal ways to divide those realms &mdash; no matter what the cause of this &mdash; that hapless mind is doomed to fail.</p>",
    "text": "Isn't it curious that infants find social goals easier to accomplish than physical goals, while adults find the social goals more difficult? One way to explain this is to say that the presence of helpful people simplifies the infant's social world \u2014 since because of them, simpler actions solve harder problems. Another explanation might be that the infant's social world is just as complicated as that of the adult, except the presence of helpful people makes the infant's mind more powerful \u2014 by making the agencies inside those other people's brains available for exploitation by the agencies in the infant's brain. Both explanations are the same, except for drawing different boundaries.\nHow do children start on the path toward distinguishing between psychological and physical relationships? In the appendix I'll suggest that our infant brains are genetically equipped with machinery for making it easy to learn social signals. But what if that machinery should somehow fail, so that by chance \u2014 or by neglect or accident \u2014 the realm-divisions never form? Then all those different kinds of thoughts would fuse together into one \u2014 and the child would face the impossible task of formulating principles that work in all domains. A child that tried to see the world without dividing it into realms would find no simple rules at all that work across so large a range.\nThis is why each child must learn different rules for the physical and psychological realms. But this means that the child must face not merely two formidable problems, but three. In addition to developing two different sets of concepts, the child must also develop agencies to manage those concepts by keeping them apart in different agencies, as we saw when we talked about Papert's principle.\nThis could explain some aspects of the disorders of the children psychiatrists call autistic. These unhappy individuals do not establish effective communication with other people, although they may acquire some competence at dealing with physical things. No one knows the causes of those disorders. Some might begin when certain mental realms do not develop normally. Other kinds of problems could emerge after those divisions form, if their separateness were compromised by some too intense attempt to unify them. To be sure, that is what scientists do, but unlike those whom we regard as mentally ill, scientists also manage to maintain their ordinary views. Once a child is deprived of the normal ways to divide those realms \u2014 no matter what the cause of this \u2014 that hapless mind is doomed to fail.",
    "type": "article",
    "title": "29.6 autistic children",
    "tags": [
      {
        "score": 0.7088689804077148,
        "sentiment": 0,
        "count": 2,
        "label": "Autism spectrum",
        "uri": "https://diffbot.com/entity/Xhs-nX6pHOBSle50SVZHaIw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Disease",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6752241253852844,
        "sentiment": 0.92,
        "count": 5,
        "label": "baby",
        "uri": "https://diffbot.com/entity/X4MZ_yLLbO0ueNL-zoshUhg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6214006543159485,
        "sentiment": 0.19,
        "count": 9,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5400458574295044,
        "sentiment": 0.806,
        "count": 3,
        "label": "social change",
        "uri": "https://diffbot.com/entity/XcjlvlDYXMTKRsmzWSdKLTA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 153614909830,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 68963287481,
    "gburl": "http://aurellem.org/society-of-mind/som-29.6.html-diffbotxyz3415865082",
    "lastCrawlTimeUTC": 1588760727,
    "timestamp": "Wed, 06 May 2020 10:25:27 GMT"
  },
  {
    "sentiment": -0.46,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1938601733",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.6.html",
    "html": "<p>One thing is sure: we always find it easier to do things we've done before. What happens in our minds to make that possible? Here's one idea: In the course of solving some problem, certain agents must have aroused certain other agents. So let's take <em>reward</em> to mean that if agent A has been involved in arousing agent B, the effect of reward is, somehow, to make it easier for A to arouse B in the future and also, perhaps, to make it harder for A to arouse other agents. At one time, I was so taken with this idea that I designed a machine called the Snarc, which learned according to this principle; it was composed of forty agents, each connected to several others, more or less at random, through a <em>reward</em> system that, when activated after each success, made each agent more likely to rearouse the same recipients at later times.</p>\n<p>We presented this machine with problems like learning to find a path through a maze while avoiding a hostile predator. It quickly learned to solve easy problems but never could learn to solve hard problems like building towers or playing chess. It became clear that, in order to solve complicated problems, any machine of limited size must be able to reuse its agents in different ways in different contexts &mdash; as See must do when involved in two concurrent tasks. But when the Snarc tried to learn its way through a complicated maze, a typical agent might suggest a good direction to move in at one moment, then suggest a bad direction at another moment. Later, when we rewarded it for doing something we liked, both those decisions became more likely &mdash; and all those <em>goods</em> and <em>bads</em> tended to cancel one another out!</p>\n<p>This poses a dilemma in designing machines that learn by <em>reinforcing</em> the connections between agents. In the course of solving a hard problem, one will usually try several bad moves before finding a good one &mdash; for this is virtually what we mean by calling a problem <em>hard.</em> To avoid learning those bad moves, we could design a machine to reinforce only what happened in the last few moments before success. But such a machine would be able to learn only to solve problems whose solutions require just a few steps. Alternatively, we could design the reward to work over longer spans of time; however, that would not only reward the bad decisions along with the good but would also erase other things that it had previously learned to do. We cannot learn to solve hard problems by indiscriminately reinforcing agents or their connections. Why is it that among all the animals, only the great-brained relatives of man can learn to solve problems that require many steps or involve using the same agencies for different purposes? We'll seek the answer in the policies our agencies use for accomplishing goals.</p>\n<p>You might argue that a beaver goes through many steps to build a dam, as does a colony of termites when it builds its complex castle nest. However, these wonderful animals do not learn such accomplishments as individuals but use the procedures that have become encoded in their species' genes over millions of years of evolution. You cannot train a beaver to build a termite nest or teach termites to build beaver dams.</p>",
    "text": "One thing is sure: we always find it easier to do things we've done before. What happens in our minds to make that possible? Here's one idea: In the course of solving some problem, certain agents must have aroused certain other agents. So let's take reward to mean that if agent A has been involved in arousing agent B, the effect of reward is, somehow, to make it easier for A to arouse B in the future and also, perhaps, to make it harder for A to arouse other agents. At one time, I was so taken with this idea that I designed a machine called the Snarc, which learned according to this principle; it was composed of forty agents, each connected to several others, more or less at random, through a reward system that, when activated after each success, made each agent more likely to rearouse the same recipients at later times.\nWe presented this machine with problems like learning to find a path through a maze while avoiding a hostile predator. It quickly learned to solve easy problems but never could learn to solve hard problems like building towers or playing chess. It became clear that, in order to solve complicated problems, any machine of limited size must be able to reuse its agents in different ways in different contexts \u2014 as See must do when involved in two concurrent tasks. But when the Snarc tried to learn its way through a complicated maze, a typical agent might suggest a good direction to move in at one moment, then suggest a bad direction at another moment. Later, when we rewarded it for doing something we liked, both those decisions became more likely \u2014 and all those goods and bads tended to cancel one another out!\nThis poses a dilemma in designing machines that learn by reinforcing the connections between agents. In the course of solving a hard problem, one will usually try several bad moves before finding a good one \u2014 for this is virtually what we mean by calling a problem hard. To avoid learning those bad moves, we could design a machine to reinforce only what happened in the last few moments before success. But such a machine would be able to learn only to solve problems whose solutions require just a few steps. Alternatively, we could design the reward to work over longer spans of time; however, that would not only reward the bad decisions along with the good but would also erase other things that it had previously learned to do. We cannot learn to solve hard problems by indiscriminately reinforcing agents or their connections. Why is it that among all the animals, only the great-brained relatives of man can learn to solve problems that require many steps or involve using the same agencies for different purposes? We'll seek the answer in the policies our agencies use for accomplishing goals.\nYou might argue that a beaver goes through many steps to build a dam, as does a colony of termites when it builds its complex castle nest. However, these wonderful animals do not learn such accomplishments as individuals but use the procedures that have become encoded in their species' genes over millions of years of evolution. You cannot train a beaver to build a termite nest or teach termites to build beaver dams.",
    "type": "article",
    "title": "7.6 reinforcement and reward",
    "tags": [
      {
        "score": 0.5859079360961914,
        "sentiment": 0.305,
        "count": 5,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5650772452354431,
        "sentiment": 0.28,
        "count": 2,
        "label": "SNARC",
        "uri": "https://diffbot.com/entity/OSgQ8dC5NPOqiq4l5TkR2qQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      }
    ],
    "docId": 224221168044,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 271935422910,
    "gburl": "http://aurellem.org/society-of-mind/som-7.6.html-diffbotxyz2061020511",
    "lastCrawlTimeUTC": 1588760749,
    "timestamp": "Wed, 06 May 2020 10:25:49 GMT"
  },
  {
    "sentiment": -0.885,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1297205371",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.6.html",
    "html": "<p>Uniframing doesn't always work. We often try to make an everyday idea precise &mdash; but just can't find much unity. Then, we can only accumulate collections of examples.</p>\n<p>It certainly is hard to find any properties that all these share. Coins are hard and round and flat. Bills are thin and flexible. Bullion has unusual weight, and credits aren't even physical. We recognize them all as media of trade &mdash; but that won't help us recognize the things themselves. The situation is the same for furniture. It's not so hard to say what furniture is for &mdash; <em>things that equip a room for living in.</em> But when it comes to the objects themselves, it's even hard to find a uniframe for <em>chair.</em> Again, its function-role seems clear &mdash; <em>a thing one can sit upon.</em> The problem is that one can sit on almost anything &mdash; a bench, a floor, a tabletop, a horse, a stack of bricks, a rock. Even defining Arch has problems, since many things we recognize as arches just don't match our Block-Arch uniframe:</p>\n<p>All these shapes could be described as <em>shape with hole</em> or <em>blocks that bridge across a gap.</em> But those descriptions would also admit many things we don't want to regard as arches. The simplest way to learn, when one can't find a uniframe, is to accumulate descriptions of experiences.</p>\n<p>At first it may seem simpler to accumulate examples than to find more uniform ways to represent them. But later there's a price to pay for this: when we try to reason about things, accumulations can be nuisances &mdash; because then we'll be forced to find a different argument or explanation to justify each separate example. Most likely, different parts of our brains have evolved to use both kinds of strategies. Accumulations need not take longer to manipulate if all the examples can be handled at the same time, by separate agents that don't interfere with one another But once those processes begin to need each other's help, the whole society's efficiency will decline rapidly. Perhaps that slowing-down itself might be the stimulus that makes us start to try to unify &mdash; at least for processes we use frequently.</p>\n<p>A simpler theory of when we start new uniframes would be that in the brain, there is an architectural constraint on how many K-lines are directly accessible to various types of agents. For example, the agents in a certain agency might be able to accumulate no more than about seven branches for each classification in a certain hierarchy. When more than that accumulate, the agency would be forced either to merge some examples into uniframes or to turn for help from somewhere else.</p>",
    "text": "Uniframing doesn't always work. We often try to make an everyday idea precise \u2014 but just can't find much unity. Then, we can only accumulate collections of examples.\nIt certainly is hard to find any properties that all these share. Coins are hard and round and flat. Bills are thin and flexible. Bullion has unusual weight, and credits aren't even physical. We recognize them all as media of trade \u2014 but that won't help us recognize the things themselves. The situation is the same for furniture. It's not so hard to say what furniture is for \u2014 things that equip a room for living in. But when it comes to the objects themselves, it's even hard to find a uniframe for chair. Again, its function-role seems clear \u2014 a thing one can sit upon. The problem is that one can sit on almost anything \u2014 a bench, a floor, a tabletop, a horse, a stack of bricks, a rock. Even defining Arch has problems, since many things we recognize as arches just don't match our Block-Arch uniframe:\nAll these shapes could be described as shape with hole or blocks that bridge across a gap. But those descriptions would also admit many things we don't want to regard as arches. The simplest way to learn, when one can't find a uniframe, is to accumulate descriptions of experiences.\nAt first it may seem simpler to accumulate examples than to find more uniform ways to represent them. But later there's a price to pay for this: when we try to reason about things, accumulations can be nuisances \u2014 because then we'll be forced to find a different argument or explanation to justify each separate example. Most likely, different parts of our brains have evolved to use both kinds of strategies. Accumulations need not take longer to manipulate if all the examples can be handled at the same time, by separate agents that don't interfere with one another But once those processes begin to need each other's help, the whole society's efficiency will decline rapidly. Perhaps that slowing-down itself might be the stimulus that makes us start to try to unify \u2014 at least for processes we use frequently.\nA simpler theory of when we start new uniframes would be that in the brain, there is an architectural constraint on how many K-lines are directly accessible to various types of agents. For example, the agents in a certain agency might be able to accumulate no more than about seven branches for each classification in a certain hierarchy. When more than that accumulate, the agency would be forced either to merge some examples into uniframes or to turn for help from somewhere else.",
    "type": "article",
    "title": "12.6 accumulation",
    "tags": [
      {
        "score": 0.6224538087844849,
        "sentiment": 0.164,
        "count": 1,
        "label": "Gateway Arch",
        "uri": "https://diffbot.com/entity/Li4ey-yQzN56wwgpdmAbFoA",
        "rdfTypes": ["http://dbpedia.org/ontology/Monument"]
      },
      {
        "score": 0.5126084089279175,
        "sentiment": -0.561,
        "count": 1,
        "label": "Buffalo Bills",
        "uri": "https://diffbot.com/entity/CnBRhPHz2O3-arf50YfdPGw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/SportsTeam"
        ]
      }
    ],
    "docId": 51350749630,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 131668279741,
    "gburl": "http://aurellem.org/society-of-mind/som-12.6.html-diffbotxyz167465000",
    "lastCrawlTimeUTC": 1588760513,
    "timestamp": "Wed, 06 May 2020 10:21:53 GMT"
  },
  {
    "sentiment": 0.589,
    "images": [
      {
        "naturalHeight": 118,
        "width": 353,
        "diffbotUri": "image|3|-732496146",
        "url": "http://aurellem.org/society-of-mind/illus/ch25/25-15.png",
        "naturalWidth": 353,
        "primary": true,
        "height": 118
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1276256618",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-25.6.html",
    "html": "<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch25/25-15.png\"/></figure>\n<p>I first conceived the idea of frames in the early 1970s, while working on making a robot that could see, and I described the theory in a 1974 essay entitled <em>A Framework for Representing Knowledge.</em> The essay influenced the next decade of research on Artificial Intelligence, despite the fact that most readers complained that its explanations were too vague. In retrospect, it seems those explanations were at just the right level-bands of detail to meet the needs of that time, which is why the essay had the effect it did. If the theory had been any vaguer, it would have been ignored, but if it had been described in more detail, other scientists might have <em>tested</em> it, instead of contributing their own ideas. Then they might have found my proposals were inadequate. Instead, many versions were suggested by other people, and <em>frame-based</em> programming became popular.</p>\n<p>Two students in particular, Scott Fahlman and Ira Goldstein, claimed to understand what I had meant &mdash; and then explained many details I hadn't imagined at all. Another student, Terry Winograd, worked on making a robot that understood a certain class of English-language sentences; this led to important theories about the relation between grammar and its effect upon a listener. Then, since that robot's task was building towers of children's blocks, Winograd also worked out many details of how to make a Builder. You can see how his theories affected this book. Yet another student, Eugene Charniak, worked on the problem of how young children understand the stories they read. He spent at least a solid year thinking about one such story, which had to do with bringing a kite to a birthday party. Shortly, you'll see the influence Charniak had on this book.</p>\n<p>All along, I had felt that the frame idea itself was rather obvious and perhaps implicit in the earlier work of psychologists like Bartlett. I considered the more important concept in the 1974 essay to be the idea of a frame-system &mdash; renamed <em>frame-array</em> in this book. I was surprised that the frame idea became popular while the frame-array idea did not. The neme concept emerged in 1977 (under the term <em>C-lines</em>); the K-line idea crystallized in 1979. The concept of pronomes was in my unconscious mind for several years but did not crystallize until, while writing this book, I realized how to reformulate several of Roger Schank's early ideas into the form of Trans-frames. The scheme proposed in this book, in which the frame-terminals are controlled by bundles of nemes or isonomes, did not emerge until a full decade after the original concept of a frame-array.</p>\n<p>Many questions remain about how frames might work. For example, it should be possible to recognize several different things at once by using different frames in parallel. But how can we see many faces in a crowd at once, or bricks in a wall, or chairs in a room? Do we make many copies of the same frame? I suspect that's impractical. Instead, perhaps we match each frame only to one example at a time &mdash; and simply assume that the same frame also applies to every other visible object that shares some characteristic features with the object under attention.</p>",
    "text": "I first conceived the idea of frames in the early 1970s, while working on making a robot that could see, and I described the theory in a 1974 essay entitled A Framework for Representing Knowledge. The essay influenced the next decade of research on Artificial Intelligence, despite the fact that most readers complained that its explanations were too vague. In retrospect, it seems those explanations were at just the right level-bands of detail to meet the needs of that time, which is why the essay had the effect it did. If the theory had been any vaguer, it would have been ignored, but if it had been described in more detail, other scientists might have tested it, instead of contributing their own ideas. Then they might have found my proposals were inadequate. Instead, many versions were suggested by other people, and frame-based programming became popular.\nTwo students in particular, Scott Fahlman and Ira Goldstein, claimed to understand what I had meant \u2014 and then explained many details I hadn't imagined at all. Another student, Terry Winograd, worked on making a robot that understood a certain class of English-language sentences; this led to important theories about the relation between grammar and its effect upon a listener. Then, since that robot's task was building towers of children's blocks, Winograd also worked out many details of how to make a Builder. You can see how his theories affected this book. Yet another student, Eugene Charniak, worked on the problem of how young children understand the stories they read. He spent at least a solid year thinking about one such story, which had to do with bringing a kite to a birthday party. Shortly, you'll see the influence Charniak had on this book.\nAll along, I had felt that the frame idea itself was rather obvious and perhaps implicit in the earlier work of psychologists like Bartlett. I considered the more important concept in the 1974 essay to be the idea of a frame-system \u2014 renamed frame-array in this book. I was surprised that the frame idea became popular while the frame-array idea did not. The neme concept emerged in 1977 (under the term C-lines); the K-line idea crystallized in 1979. The concept of pronomes was in my unconscious mind for several years but did not crystallize until, while writing this book, I realized how to reformulate several of Roger Schank's early ideas into the form of Trans-frames. The scheme proposed in this book, in which the frame-terminals are controlled by bundles of nemes or isonomes, did not emerge until a full decade after the original concept of a frame-array.\nMany questions remain about how frames might work. For example, it should be possible to recognize several different things at once by using different frames in parallel. But how can we see many faces in a crowd at once, or bricks in a wall, or chairs in a room? Do we make many copies of the same frame? I suspect that's impractical. Instead, perhaps we match each frame only to one example at a time \u2014 and simply assume that the same frame also applies to every other visible object that shares some characteristic features with the object under attention.",
    "type": "article",
    "title": "25.6 the frame idea",
    "tags": [
      {
        "score": 0.7782415747642517,
        "sentiment": 0,
        "count": 1,
        "label": "artificial intelligence",
        "uri": "https://diffbot.com/entity/X_lYDrjmAMlKKwXaDf958zg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6588568687438965,
        "sentiment": 0.334,
        "count": 2,
        "label": "Eugene Charniak",
        "uri": "https://diffbot.com/entity/PkGZiYLDqNBir1Wymc-adJw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.6428539156913757,
        "sentiment": 0.479,
        "count": 2,
        "label": "Terry Winograd",
        "uri": "https://diffbot.com/entity/POckVLpdMOJOg7jaQcJWpLA",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.586664617061615,
        "sentiment": 0.744,
        "count": 4,
        "label": "essay",
        "uri": "https://diffbot.com/entity/XOGkKAyBnPYiQmW4I0_HlCw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5517730116844177,
        "sentiment": 0,
        "count": 1,
        "label": "Ira Goldstein",
        "uri": "https://diffbot.com/entity/XTjJPNIvIPSG0OyVyE1tjig",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5510432720184326,
        "sentiment": 0,
        "count": 2,
        "label": "children's literature",
        "uri": "https://diffbot.com/entity/XZZX2PiM6NXC3s1KFBNYheg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5310116410255432,
        "sentiment": 0,
        "count": 1,
        "label": "Scott Fahlman",
        "uri": "https://diffbot.com/entity/Puu8EPVaLMImiqB-3Gn5suw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 76178489763,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 39436206506,
    "gburl": "http://aurellem.org/society-of-mind/som-25.6.html-diffbotxyz2500511735",
    "lastCrawlTimeUTC": 1588760585,
    "timestamp": "Wed, 06 May 2020 10:23:05 GMT"
  },
  {
    "sentiment": 0.95,
    "humanLanguage": "en",
    "diffbotUri": "article|3|711717431",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-14.3.html",
    "html": "<p>Most people find the nine-dot problem hard to solve because they assume that the dots form a square that bounds the working space. Indeed, the problem is insoluble unless the drawing can extend outside that area. Thus the problem is easier if one does not perceive those dots as forming a square. We often self-impose assumptions that make our problems more difficult, and we can escape from this only by reformulating those problems in ways that give us more room.</p>\n<p>Really, there was never any square in the first place &mdash; that is, in the literal sense of <em>a rectangle with equal sides.</em> What makes us see so many different sorts of things as though they were squares?</p>\n<p>Some of these squares have no corners, others have no edges, and some of them have neither corners nor edges! What makes us see them all as squares? Psychologists have long wondered how we recognize such similarities but often forgot to ask how we recognize the very simplest forms of squares in the first place. Which comes first in recognition, specific features or global shapes? It must depend upon one's state of mind. The way we perceive the world, from one moment to another, depends only in part on what comes from our eyes: the rest of what we think we see comes from inside our brain; we respond not only to visual features, but also to our remembrances of things we've seen before and to our expectations of what we ought to see.</p>\n<p>It is tempting to assume that our visual processes work only in one direction, bringing information from the world into the mind:</p>\n<p>But this does not explain how what we see is influenced by what we expect to see. Human vision must somehow combine the information that comes from the outer world with the structures in our memories. The situation must be more like this:</p>",
    "text": "Most people find the nine-dot problem hard to solve because they assume that the dots form a square that bounds the working space. Indeed, the problem is insoluble unless the drawing can extend outside that area. Thus the problem is easier if one does not perceive those dots as forming a square. We often self-impose assumptions that make our problems more difficult, and we can escape from this only by reformulating those problems in ways that give us more room.\nReally, there was never any square in the first place \u2014 that is, in the literal sense of a rectangle with equal sides. What makes us see so many different sorts of things as though they were squares?\nSome of these squares have no corners, others have no edges, and some of them have neither corners nor edges! What makes us see them all as squares? Psychologists have long wondered how we recognize such similarities but often forgot to ask how we recognize the very simplest forms of squares in the first place. Which comes first in recognition, specific features or global shapes? It must depend upon one's state of mind. The way we perceive the world, from one moment to another, depends only in part on what comes from our eyes: the rest of what we think we see comes from inside our brain; we respond not only to visual features, but also to our remembrances of things we've seen before and to our expectations of what we ought to see.\nIt is tempting to assume that our visual processes work only in one direction, bringing information from the world into the mind:\nBut this does not explain how what we see is influenced by what we expect to see. Human vision must somehow combine the information that comes from the outer world with the structures in our memories. The situation must be more like this:",
    "type": "article",
    "title": "14.3 seeing squares",
    "docId": 100604445110,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 269748765066,
    "gburl": "http://aurellem.org/society-of-mind/som-14.3.html-diffbotxyz3869211393",
    "lastCrawlTimeUTC": 1588760549,
    "timestamp": "Wed, 06 May 2020 10:22:29 GMT"
  },
  {
    "sentiment": -0.272,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1749224963",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.9.html",
    "html": "<p>Most machines that people build stop working when their parts break down. Isn't it amazing that our minds can keep on functioning while they're making changes in themselves? Indeed, they must, since minds can't simply shut down work when <em>closed for renovations.</em> But how do we keep functioning while vital parts are modified &mdash; or even lost? It's a fact that our brains can keep on working well in spite of injuries in which great multitudes of cells are killed. How could anything be so robust? Here are some possibilities:</p>\n<p>Duplication. It is possible to design a machine so that every one of its functions is embodied in several duplicated agents, in different places. Then, if any agent is disabled, one of its duplicates can be made to <em>take over.</em> A machine based on this duplication-scheme could be surprisingly robust. For example, suppose that every function were duplicated in ten agents. If an accident were to destroy half the agents of that machine, the chance that any particular function would be entirely lost is the same as the chance that ten tossed coins would all come up tails &mdash; that is, less than one chance in a thousand. And many regions of the human brain do indeed have several duplicates.</p>\n<p>Self-Repair. Many of the body's organs can regenerate &mdash; that is, they can replace whichever parts are lost to injury or disease. However, brain cells do not usually share this ability. Consequently, healing cannot be the basis of much of the brain's robustness. This makes one wonder why an organ as vital as the brain has evolved to be less able than other organs to repair or replace its broken parts. Presumably, this is because it simply wouldn't help to replace individual brain-agents &mdash; unless the same healing process could also restore all the learned connections among those agents. Since it is those networks that embody what we've learned, merely to replace their separate parts would not restore the functions that were lost.</p>\n<p>Distributed Processes. It is possible to build machines in which no function is located in any one specific place. Instead, each function is <em>spread out</em> over a range of locations, so that each part's activity contributes a little to each of several different functions. Then the destruction of any small portion will not destroy any function entirely but will only cause small impairments to many different functions.</p>\n<p>Accumulation. I'm sure that all of the above methods are employed in our brains. But we also have another source of robustness that offers more advantages. Consider any learning-scheme that begins by using the method of accumulation &mdash; in which each agent tends to accumulate a family of subagents that can accomplish that agent's goals in several ways. Later, if any of those subagents become impaired, their supervisor will still be able to accomplish its job, because other of its subagents will remain to do that job, albeit in different ways. So accumulation &mdash; the very simplest kind of learning &mdash; provides both robustness and versatility. Our learning- systems can build up centers of diversity in which each agent is equipped with various alternatives. When such a center is damaged, the effects may scarcely begin to show until the system's reserves are nearly exhausted.</p>",
    "text": "Most machines that people build stop working when their parts break down. Isn't it amazing that our minds can keep on functioning while they're making changes in themselves? Indeed, they must, since minds can't simply shut down work when closed for renovations. But how do we keep functioning while vital parts are modified \u2014 or even lost? It's a fact that our brains can keep on working well in spite of injuries in which great multitudes of cells are killed. How could anything be so robust? Here are some possibilities:\nDuplication. It is possible to design a machine so that every one of its functions is embodied in several duplicated agents, in different places. Then, if any agent is disabled, one of its duplicates can be made to take over. A machine based on this duplication-scheme could be surprisingly robust. For example, suppose that every function were duplicated in ten agents. If an accident were to destroy half the agents of that machine, the chance that any particular function would be entirely lost is the same as the chance that ten tossed coins would all come up tails \u2014 that is, less than one chance in a thousand. And many regions of the human brain do indeed have several duplicates.\nSelf-Repair. Many of the body's organs can regenerate \u2014 that is, they can replace whichever parts are lost to injury or disease. However, brain cells do not usually share this ability. Consequently, healing cannot be the basis of much of the brain's robustness. This makes one wonder why an organ as vital as the brain has evolved to be less able than other organs to repair or replace its broken parts. Presumably, this is because it simply wouldn't help to replace individual brain-agents \u2014 unless the same healing process could also restore all the learned connections among those agents. Since it is those networks that embody what we've learned, merely to replace their separate parts would not restore the functions that were lost.\nDistributed Processes. It is possible to build machines in which no function is located in any one specific place. Instead, each function is spread out over a range of locations, so that each part's activity contributes a little to each of several different functions. Then the destruction of any small portion will not destroy any function entirely but will only cause small impairments to many different functions.\nAccumulation. I'm sure that all of the above methods are employed in our brains. But we also have another source of robustness that offers more advantages. Consider any learning-scheme that begins by using the method of accumulation \u2014 in which each agent tends to accumulate a family of subagents that can accomplish that agent's goals in several ways. Later, if any of those subagents become impaired, their supervisor will still be able to accomplish its job, because other of its subagents will remain to do that job, albeit in different ways. So accumulation \u2014 the very simplest kind of learning \u2014 provides both robustness and versatility. Our learning- systems can build up centers of diversity in which each agent is equipped with various alternatives. When such a center is damaged, the effects may scarcely begin to show until the system's reserves are nearly exhausted.",
    "type": "article",
    "title": "18.9 robustness and recovery",
    "tags": [
      {
        "score": 0.7029824256896973,
        "sentiment": 0.779,
        "count": 3,
        "label": "robust build",
        "uri": "https://diffbot.com/entity/X_eFC6kdjPr--yLlyCtUOlQ"
      },
      {
        "score": 0.5468411445617676,
        "sentiment": -0.711,
        "count": 2,
        "label": "injury",
        "uri": "https://diffbot.com/entity/XT9wlVdJtMRKHhzMsGcI3qw"
      }
    ],
    "docId": 189145153955,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 49866211723,
    "gburl": "http://aurellem.org/society-of-mind/som-18.9.html-diffbotxyz3702805260",
    "lastCrawlTimeUTC": 1588760624,
    "timestamp": "Wed, 06 May 2020 10:23:44 GMT"
  },
  {
    "sentiment": -0.876,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1047473643",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-9.2.html",
    "html": "<p>We all know how accomplishment can bring satisfaction, and we tend to assume a direct connection between them. In very simple animals, where <em>satisfaction</em> means no more than meeting simple, basic needs, satisfaction and accomplishment must indeed be virtually the same. But in a complex human brain, a great many layers of agencies are interposed between the ones that deal with body needs and those that represent or recognize our intellectual accomplishments. Then what is the significance, in these more complicated systems, of those pleasant feelings of accomplishment and disagreeable sensations of defeat? They must be involved with how our higher-level agencies make summaries.</p>\n<p>Suppose that you once had to send a present to a friend. You had to choose a gift and find a box in which to wrap it. Soon, each such job turned into several smaller ones &mdash; like finding strings and tying them. The only way to solve hard problems is by breaking them into smaller ones and then, when those are too difficult, dividing them in turn. So hard problems always lead to branching trees of subgoals and subproblems. To decide where resources should be applied, our problem-solving agents need simple summaries of how things are going. Let's suppose each agent's summary is based on other summaries it gets from the agents it supervises. Here is a pathological example of what could happen if every such summary were based on a simple majority decision:</p>\n<p>When all is done, if someone asked if you enjoyed the whole experience, you might say that it was <em>fun</em> or <em>terrible.</em> But no such summary can say very much of what your agencies actually learned. Your knot-tying processes learned which actions worked and failed, your paper-folding and gift-selecting processes had other failures and accomplishments; but your overall assessment of the experience cannot reflect all those details. If the entire episode left you <em>unhappy,</em> you might be less inclined to give presents in the future, but that should not have much effect on what you learned about folding paper and tying string. No single sense of <em>good</em> or <em>bad</em> can reflect much of what went on inside all your agencies; too much information must be concealed. Then why does it seem so satisfactory for us to classify our feelings into positive and negative and conclude that <em>on the whole</em> the net effect was bad or good? True, sometimes feelings are more mixed and everything seems bittersweet, but, as we'll see, there are many reasons why we have to oversimplify.</p>",
    "text": "We all know how accomplishment can bring satisfaction, and we tend to assume a direct connection between them. In very simple animals, where satisfaction means no more than meeting simple, basic needs, satisfaction and accomplishment must indeed be virtually the same. But in a complex human brain, a great many layers of agencies are interposed between the ones that deal with body needs and those that represent or recognize our intellectual accomplishments. Then what is the significance, in these more complicated systems, of those pleasant feelings of accomplishment and disagreeable sensations of defeat? They must be involved with how our higher-level agencies make summaries.\nSuppose that you once had to send a present to a friend. You had to choose a gift and find a box in which to wrap it. Soon, each such job turned into several smaller ones \u2014 like finding strings and tying them. The only way to solve hard problems is by breaking them into smaller ones and then, when those are too difficult, dividing them in turn. So hard problems always lead to branching trees of subgoals and subproblems. To decide where resources should be applied, our problem-solving agents need simple summaries of how things are going. Let's suppose each agent's summary is based on other summaries it gets from the agents it supervises. Here is a pathological example of what could happen if every such summary were based on a simple majority decision:\nWhen all is done, if someone asked if you enjoyed the whole experience, you might say that it was fun or terrible. But no such summary can say very much of what your agencies actually learned. Your knot-tying processes learned which actions worked and failed, your paper-folding and gift-selecting processes had other failures and accomplishments; but your overall assessment of the experience cannot reflect all those details. If the entire episode left you unhappy, you might be less inclined to give presents in the future, but that should not have much effect on what you learned about folding paper and tying string. No single sense of good or bad can reflect much of what went on inside all your agencies; too much information must be concealed. Then why does it seem so satisfactory for us to classify our feelings into positive and negative and conclude that on the whole the net effect was bad or good? True, sometimes feelings are more mixed and everything seems bittersweet, but, as we'll see, there are many reasons why we have to oversimplify.",
    "type": "article",
    "title": "9.2 gerrymandering",
    "tags": [
      {
        "score": 0.5080044865608215,
        "sentiment": 0,
        "count": 1,
        "label": "intellectual",
        "uri": "https://diffbot.com/entity/XProv2_W5PX6_d6ql63ksnQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 13422952848,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 47289237921,
    "gburl": "http://aurellem.org/society-of-mind/som-9.2.html-diffbotxyz3826920549",
    "lastCrawlTimeUTC": 1588760386,
    "timestamp": "Wed, 06 May 2020 10:19:46 GMT"
  },
  {
    "sentiment": -0.319,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1050914916",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-10.6.html",
    "html": "<p>Although Piaget's experiments about conservation of quantity have been confirmed as thoroughly as any in psychology, we can appreciate why many people are skeptical when they first hear of these discoveries. They contradict the traditional assumption that children are much like adults, except more ignorant. How strange it is that in all the centuries of history, these phenomena went unnoticed until Piaget &mdash; as though no one had ever watched a child carefully! But it has always been that way with science. Why did it take so long for our thinkers to discover such simple ideas as Isaac Newton's laws of motion or Darwin's idea of natural selection? Here are some frequent challenges.</p>\n<p>Parent: Couldn't it be that younger children use words in ways that do not mean the same things to adults? Perhaps they simply take <em>Which is more?</em> to mean <em>Which is higher?</em> or <em>Which is longer?</em></p>\n<p>Careful experiments show that this can't be entirely a matter of words. We can offer the same choices, wordlessly, yet most younger children will still reach out for taller, thinner jars of orange juice or stretched-out rows of candy eggs.</p>\n<p>Critic: What happens when Appearance and History conflict? Won't that paralyze your whole Society-of-More?</p>\n<p>It would indeed &mdash; unless More has yet other levels and alternatives. And adults have other kinds of explanations &mdash; like <em>magic,</em> <em>evaporation,</em> or <em>theft.</em> But indeed, stage magicians find that making things disappear does not entertain the youngest children; presumably they are too used to encountering the unexplainable. What happens when More cannot decide what to do? That depends upon the states of other agencies &mdash; including those involved in dealing with frustration, restlessness, and boredom.</p>\n<p>Psychologist: We've heard of recent evidence that, despite what Piaget said, very young children do have concepts of</p>\n<p>quantity; many of them can even count those eggs. Doesn't that refute some of Piaget's discoveries? Not necessarily. Consider that no one disputes the outcomes of those jar and cup experiments. What is the significance, then, of evidence that the young children do possess methods that could give correct answers &mdash; and yet they do not use those abilities? As far as I can see, such evidence would only further support the need for explanations like those of Papert and Piaget.</p>\n<p>Biologist: Your theory might explain how some children could acquire those concepts about quantities &mdash; but it doesn't explain why all children end up with such similar abilities! Could we be born with built-in genes that make brains do this automatically?</p>\n<p>This is a profound question. It is hard &mdash; but not impossible &mdash; to imagine how genes could directly influence the higher-level ideas and conceptions that we eventually learn. We'll discuss this in the appendix at the end of this book.</p>",
    "text": "Although Piaget's experiments about conservation of quantity have been confirmed as thoroughly as any in psychology, we can appreciate why many people are skeptical when they first hear of these discoveries. They contradict the traditional assumption that children are much like adults, except more ignorant. How strange it is that in all the centuries of history, these phenomena went unnoticed until Piaget \u2014 as though no one had ever watched a child carefully! But it has always been that way with science. Why did it take so long for our thinkers to discover such simple ideas as Isaac Newton's laws of motion or Darwin's idea of natural selection? Here are some frequent challenges.\nParent: Couldn't it be that younger children use words in ways that do not mean the same things to adults? Perhaps they simply take Which is more? to mean Which is higher? or Which is longer?\nCareful experiments show that this can't be entirely a matter of words. We can offer the same choices, wordlessly, yet most younger children will still reach out for taller, thinner jars of orange juice or stretched-out rows of candy eggs.\nCritic: What happens when Appearance and History conflict? Won't that paralyze your whole Society-of-More?\nIt would indeed \u2014 unless More has yet other levels and alternatives. And adults have other kinds of explanations \u2014 like magic, evaporation, or theft. But indeed, stage magicians find that making things disappear does not entertain the youngest children; presumably they are too used to encountering the unexplainable. What happens when More cannot decide what to do? That depends upon the states of other agencies \u2014 including those involved in dealing with frustration, restlessness, and boredom.\nPsychologist: We've heard of recent evidence that, despite what Piaget said, very young children do have concepts of\nquantity; many of them can even count those eggs. Doesn't that refute some of Piaget's discoveries? Not necessarily. Consider that no one disputes the outcomes of those jar and cup experiments. What is the significance, then, of evidence that the young children do possess methods that could give correct answers \u2014 and yet they do not use those abilities? As far as I can see, such evidence would only further support the need for explanations like those of Papert and Piaget.\nBiologist: Your theory might explain how some children could acquire those concepts about quantities \u2014 but it doesn't explain why all children end up with such similar abilities! Could we be born with built-in genes that make brains do this automatically?\nThis is a profound question. It is hard \u2014 but not impossible \u2014 to imagine how genes could directly influence the higher-level ideas and conceptions that we eventually learn. We'll discuss this in the appendix at the end of this book.",
    "type": "article",
    "title": "10.6 about piaget's experiments",
    "tags": [
      {
        "score": 0.7927961945533752,
        "sentiment": 0.383,
        "count": 5,
        "label": "Jean Piaget",
        "uri": "https://diffbot.com/entity/PDLtioHPGOfSddrmNWrXIYg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5656780004501343,
        "sentiment": 0,
        "count": 1,
        "label": "Isaac Newton",
        "uri": "https://diffbot.com/entity/Pv8uMLnmHPC-lnjo1K4Nl7Q",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5400761365890503,
        "sentiment": -0.588,
        "count": 9,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.537365198135376,
        "sentiment": 0,
        "count": 1,
        "label": "Charles Darwin",
        "uri": "https://diffbot.com/entity/P-hhIPfYhOdO2Sv0AVILRDg",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 18430263719,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 31882199467,
    "gburl": "http://aurellem.org/society-of-mind/som-10.6.html-diffbotxyz1778072494",
    "lastCrawlTimeUTC": 1588760483,
    "timestamp": "Wed, 06 May 2020 10:21:23 GMT"
  },
  {
    "sentiment": 0.99,
    "images": [
      {
        "naturalHeight": 329,
        "width": 215,
        "diffbotUri": "image|3|-1968676195",
        "url": "http://aurellem.org/society-of-mind/illus/ch28/28-1.png",
        "naturalWidth": 215,
        "primary": true,
        "height": 329
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1273855989",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-28.html",
    "html": "<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch28/28-1.png\"/></figure>",
    "text": "",
    "type": "article",
    "title": "28 the mind and the world",
    "docId": 140065325450,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 47522660754,
    "gburl": "http://aurellem.org/society-of-mind/som-28.html-diffbotxyz3559817902",
    "lastCrawlTimeUTC": 1588760412,
    "timestamp": "Wed, 06 May 2020 10:20:12 GMT"
  },
  {
    "date": "Sat, 19 Oct 2019 00:00:00 GMT",
    "sentiment": 0.413,
    "images": [
      {
        "naturalHeight": 244,
        "width": 386,
        "diffbotUri": "image|3|724693897",
        "url": "http://aurellem.org/society-of-mind/illus/ch19/19-10.png",
        "naturalWidth": 386,
        "primary": true,
        "height": 244
      }
    ],
    "estimatedDate": "Sat, 19 Oct 2019 00:00:00 GMT",
    "diffbotUri": "article|3|-1615844210",
    "siteName": "aurellem.org",
    "type": "article",
    "title": "19.10 closing the ring",
    "tags": [
      {
        "score": 0.7165446281433105,
        "sentiment": 0,
        "count": 1,
        "label": "Closing the Ring",
        "uri": "https://diffbot.com/entity/XDVRn3H_0Mn6DiY475wH9AQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Film"
        ]
      },
      {
        "score": 0.5872602462768555,
        "sentiment": 0,
        "count": 1,
        "label": "Fly Me to the Moon",
        "uri": "https://diffbot.com/entity/XRZrQ-OahMnaptNVys6VVhw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5608402490615845,
        "sentiment": 0.909,
        "count": 10,
        "label": "apple",
        "uri": "https://diffbot.com/entity/XgrXdplpfMRWDa_VBAOd2oA"
      }
    ],
    "humanLanguage": "en",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.10.html",
    "html": "<p>Now let's redraw the diagram for the language-agency, but fill in more details from the last few sections.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch19/19-10.png\"/></figure>\n<p>Something amazing happens when you go around a loop like this! Suppose you were to imagine three properties of an apple &mdash; for example, its substance, taste, and thin-peeled structure. Then, even if there were no apple on the scene &mdash; and even if you had not yet thought of the word <em>apple</em> &mdash; the recognition-agent on the left will be aroused enough to excite your <em>apple</em> polyneme. (This is because I used the number three for the required sum in the apple polyneme's recognizer instead of demanding that all five properties be present. ) That agent can then arouse the K-lines in other agencies, like those for color and shape &mdash; and thus evoke your memories of other apple properties! In other words, if you start with enough clues to arouse one of your apple-nemes, it will automatically arouse memories of the other properties and qualities of apples and create a more complete impression, <em>simulus,</em> or hallucination of the experience of seeing, feeling, and even of eating an apple. This way, a simple loop machine can reconstruct a larger whole from clues about only certain of its parts!</p>\n<p>Many thinkers have assumed that such abilities lie beyond the reach of all machines. Yet here we see that retrieving the whole from a few of its parts requires no magic leap past logic and necessity, but only simple societies of agents that <em>recognize</em> when certain requirements are met. If something is red and round and has the right size and shape for an apple &mdash; and nothing else seems wrong &mdash; then one will probably think <em>apple.</em></p>\n<p>This method for arousing complete recollections from incomplete clues &mdash; we could call it <em>reminding</em> &mdash; is powerful but imperfect. Our speaker might have had in mind not an apple, but some other round, red, fruit, such as a tomato or a pomegranate. Any such process leads only to guesses &mdash; and frequently these will be wrong. Nonetheless, to think effectively, we often have to turn aside from certainty &mdash; to take some chance of being wrong. Our memory systems are powerful because they're not constrained to be perfect!</p>",
    "text": "Now let's redraw the diagram for the language-agency, but fill in more details from the last few sections.\nSomething amazing happens when you go around a loop like this! Suppose you were to imagine three properties of an apple \u2014 for example, its substance, taste, and thin-peeled structure. Then, even if there were no apple on the scene \u2014 and even if you had not yet thought of the word apple \u2014 the recognition-agent on the left will be aroused enough to excite your apple polyneme. (This is because I used the number three for the required sum in the apple polyneme's recognizer instead of demanding that all five properties be present. ) That agent can then arouse the K-lines in other agencies, like those for color and shape \u2014 and thus evoke your memories of other apple properties! In other words, if you start with enough clues to arouse one of your apple-nemes, it will automatically arouse memories of the other properties and qualities of apples and create a more complete impression, simulus, or hallucination of the experience of seeing, feeling, and even of eating an apple. This way, a simple loop machine can reconstruct a larger whole from clues about only certain of its parts!\nMany thinkers have assumed that such abilities lie beyond the reach of all machines. Yet here we see that retrieving the whole from a few of its parts requires no magic leap past logic and necessity, but only simple societies of agents that recognize when certain requirements are met. If something is red and round and has the right size and shape for an apple \u2014 and nothing else seems wrong \u2014 then one will probably think apple.\nThis method for arousing complete recollections from incomplete clues \u2014 we could call it reminding \u2014 is powerful but imperfect. Our speaker might have had in mind not an apple, but some other round, red, fruit, such as a tomato or a pomegranate. Any such process leads only to guesses \u2014 and frequently these will be wrong. Nonetheless, to think effectively, we often have to turn aside from certainty \u2014 to take some chance of being wrong. Our memory systems are powerful because they're not constrained to be perfect!",
    "docId": 166595101117,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 264494498223,
    "gburl": "http://aurellem.org/society-of-mind/som-19.10.html-diffbotxyz3274350394",
    "lastCrawlTimeUTC": 1588760459,
    "timestamp": "Wed, 06 May 2020 10:20:59 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1756968294",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.13.html",
    "html": "<p>To &ldquo;know oneself&amp;rdquo more perfectly might seem to promise something powerful and good. But there are fallacies concealed behind that happy thought. No doubt, a mind that wants to change itself could benefit from knowing how it works. But such knowledge might as easily encourage us to wreck ourselves &mdash; if we had ways to poke our clumsy mental fingers into the tricky circuits of the mind's machinery. Could this be why our brains force us to play those games of mental hide and seek?</p>\n<p>Just see how prone we are to risk experiments that change ourselves; how fatally we're drawn to drugs, to meditation, music, even conversation &mdash; all powerful addictions that can change our very personalities. Just see how everyone is entranced by any promise to transgress the bounds of normal pleasure and reward.</p>\n<p>In ordinary life, our pleasure systems help us learn &mdash; and, therefore, to behave ourselves &mdash; by forcing checks and balances on us. Why, for example, do we become bored when doing the same thing over and over, even if that activity was pleasant at first? This appears to be one property of our pleasure systems; without enough variety, they tend to satiate. Every learning machine must have some such protective scheme, since otherwise it could get trapped into endlessly repeating the same activity. We are fortunate to be equipped with mechanisms that keep us from wasting too much time, and it is fortunate, too, that we find it hard to suppress such mechanisms.</p>\n<p>If we could deliberately seize control of our pleasure systems, we could reproduce the pleasure of success without the need for any actual accomplishment. And that would be the end of everything.</p>\n<p>What prevents such meddling? Our minds are bound by many self-constraints. For example, we find it hard to determine what's happening inside the mind. Later, when we talk about infant development, we'll see that even if our inner eyes could see what's there, we'd find it singularly hard to change the agents we might want most to change &mdash; the ones that, in our infancy, helped shape our longest-lasting self-ideals.</p>\n<p>These agents are hard to change because of their special evolutionary origin. The long-term stability of many other mental agencies depends on how slowly we change our images of what we ought to be like. Few of us would survive if, left to random chance, our most adventurous impulses could tamper freely with the basis of our personalities. Why would that be such a bad thing to do? Because an ordinary <em>change of mind</em> can be reversed if it leads to a bad result. But when you change your self-ideals &mdash; then nothing is left to turn you back.</p>\n<p>Sigmund Freud theorized that each person's growth is governed by unconscious needs to please, placate, oppose, or terminate our images of parental authority. If we recognized the influence of those old images, however, we might consider them too infantile or too unworthy to tolerate and seek to replace them with something better. But then what would we substitute for them &mdash; once we divested ourselves of all those ties to instinct and society? We'd each end up as instruments of even more capricious sorts of self-invented goals.</p>\n<p>It's mainly when our systems fail that consciousness becomes engaged. For example, we walk and talk without much sense of how we actually do those things. But a person with an injured leg may, for the first time, begin to formulate theories about how walking works (<em>To turn to the left, I'll have to push myself that way,</em>) and then perhaps consider which muscles might accomplish that goal. When we recognize that we're confused, we begin to reflect on how our minds solve problems and engage the little we know about our strategies of thought. Then we find ourselves saying things like this:</p>\n<p><em>Now I must get organized. Why can't I concentrate on the important questions and not get distracted by those other nonessential details?</em></p>\n<p>Paradoxically, it is smart to realize that one is confused &mdash; as opposed to being confused without knowing it. For that stimulates us to apply our intellect to altering or repairing the defective process. Yet we dislike and disparage the sense of confusion, not appreciating the quality of this recognition.</p>\n<p>However, once your B-brains make you start to ask yourself <em>What was I really attempting to do?</em> you can exploit that as an opportunity to change your goals or change how you describe your situation. That way, you can escape the distress of feeling trapped because there seem to be no adequate alternatives. The conscious experience of confusion can resemble pain; perhaps this is because of how they both impel us to discover ways to escape from a predicament. The difference is that confusion is directed against a person's own failing state of mind, whereas pain reflects exterior disturbances. In either case, internal processes must be demolished and rebuilt.</p>\n<p>Both confusion and pain have injurious effects when they lead us to abandon goals on larger scales than appropriate: <em>The entire subject makes me feel ill. Perhaps I should abandon the whole project, occupation, or relationship.</em> But even such dispiriting thoughts can serve as probes for finding other agencies that might be engaged for help.</p>",
    "text": "To \u201cknow oneself&rdquo more perfectly might seem to promise something powerful and good. But there are fallacies concealed behind that happy thought. No doubt, a mind that wants to change itself could benefit from knowing how it works. But such knowledge might as easily encourage us to wreck ourselves \u2014 if we had ways to poke our clumsy mental fingers into the tricky circuits of the mind's machinery. Could this be why our brains force us to play those games of mental hide and seek?\nJust see how prone we are to risk experiments that change ourselves; how fatally we're drawn to drugs, to meditation, music, even conversation \u2014 all powerful addictions that can change our very personalities. Just see how everyone is entranced by any promise to transgress the bounds of normal pleasure and reward.\nIn ordinary life, our pleasure systems help us learn \u2014 and, therefore, to behave ourselves \u2014 by forcing checks and balances on us. Why, for example, do we become bored when doing the same thing over and over, even if that activity was pleasant at first? This appears to be one property of our pleasure systems; without enough variety, they tend to satiate. Every learning machine must have some such protective scheme, since otherwise it could get trapped into endlessly repeating the same activity. We are fortunate to be equipped with mechanisms that keep us from wasting too much time, and it is fortunate, too, that we find it hard to suppress such mechanisms.\nIf we could deliberately seize control of our pleasure systems, we could reproduce the pleasure of success without the need for any actual accomplishment. And that would be the end of everything.\nWhat prevents such meddling? Our minds are bound by many self-constraints. For example, we find it hard to determine what's happening inside the mind. Later, when we talk about infant development, we'll see that even if our inner eyes could see what's there, we'd find it singularly hard to change the agents we might want most to change \u2014 the ones that, in our infancy, helped shape our longest-lasting self-ideals.\nThese agents are hard to change because of their special evolutionary origin. The long-term stability of many other mental agencies depends on how slowly we change our images of what we ought to be like. Few of us would survive if, left to random chance, our most adventurous impulses could tamper freely with the basis of our personalities. Why would that be such a bad thing to do? Because an ordinary change of mind can be reversed if it leads to a bad result. But when you change your self-ideals \u2014 then nothing is left to turn you back.\nSigmund Freud theorized that each person's growth is governed by unconscious needs to please, placate, oppose, or terminate our images of parental authority. If we recognized the influence of those old images, however, we might consider them too infantile or too unworthy to tolerate and seek to replace them with something better. But then what would we substitute for them \u2014 once we divested ourselves of all those ties to instinct and society? We'd each end up as instruments of even more capricious sorts of self-invented goals.\nIt's mainly when our systems fail that consciousness becomes engaged. For example, we walk and talk without much sense of how we actually do those things. But a person with an injured leg may, for the first time, begin to formulate theories about how walking works (To turn to the left, I'll have to push myself that way,) and then perhaps consider which muscles might accomplish that goal. When we recognize that we're confused, we begin to reflect on how our minds solve problems and engage the little we know about our strategies of thought. Then we find ourselves saying things like this:\nNow I must get organized. Why can't I concentrate on the important questions and not get distracted by those other nonessential details?\nParadoxically, it is smart to realize that one is confused \u2014 as opposed to being confused without knowing it. For that stimulates us to apply our intellect to altering or repairing the defective process. Yet we dislike and disparage the sense of confusion, not appreciating the quality of this recognition.\nHowever, once your B-brains make you start to ask yourself What was I really attempting to do? you can exploit that as an opportunity to change your goals or change how you describe your situation. That way, you can escape the distress of feeling trapped because there seem to be no adequate alternatives. The conscious experience of confusion can resemble pain; perhaps this is because of how they both impel us to discover ways to escape from a predicament. The difference is that confusion is directed against a person's own failing state of mind, whereas pain reflects exterior disturbances. In either case, internal processes must be demolished and rebuilt.\nBoth confusion and pain have injurious effects when they lead us to abandon goals on larger scales than appropriate: The entire subject makes me feel ill. Perhaps I should abandon the whole project, occupation, or relationship. But even such dispiriting thoughts can serve as probes for finding other agencies that might be engaged for help.",
    "type": "article",
    "title": "6.13 self-knowledge is dangerous",
    "docId": 119115219334,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 91404812692,
    "gburl": "http://aurellem.org/society-of-mind/som-6.13.html-diffbotxyz1951983952",
    "lastCrawlTimeUTC": 1588760313,
    "timestamp": "Wed, 06 May 2020 10:18:33 GMT"
  },
  {
    "sentiment": 0.644,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1213514352",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.6.html",
    "html": "<p>How do we learn about the real, three-dimensional world? We've seen how certain agencies might map the layout of the skin. But how could we progress from that to learn about the world of space beyond the skin? One might ask why infants can't simply <em>look around</em> to see what's really going on. Unfortunately, the easy-sounding phrase <em>simply look</em> conceals too many hard problems. When you look at an object, some light from it shines into your eye and stimulates some sensors there. However, every motion of your body, head, or eye makes drastic changes to the image in your eye. How can we extract any useful information when everything changes so rapidly? Although it should be possible, in principle, to design a machine that could eventually learn to relate those motions to the resulting changes in the images, this would surely take a long time, and it appears that our brains have evolved with special mechanisms that help us compensate for motions of the body, head, and eye. This makes it easier for other agencies to learn to use visual information.</p>\n<p>Later we'll discuss some other realms of thought in which we use analogies and metaphors to change our <em>points of view.</em> Perhaps those wonderful abilities evolved in similar ways, since recognizing that an object is the same when seen from different views is not so different from being able to <em>imagine</em> things that are not in view at all.</p>\n<p>In any case, we really do not understand how the child learns to understand space. Perhaps we start by doing many small experiments that lead to our first, crude maps of the skin. Next we might start to correlate these with the motions of our eyes and limbs; two different actions that lead to similar sensations are likely to have passed through the same locations in space. A critical step would be developing some agents that <em>represent</em> a few <em>places</em> outside the skin. Once those places are established (the first ones might be near the infant's face), one could proceed to another stage: the assembly of an agency that represents a network of relationships, trajectories, and directions between those places. Once this is accomplished, the network could continue to extend to include new places and relationships.</p>\n<p>However, this would be only the beginning. Long ago, psychologists like Freud and Piaget observed that children seem to recapitulate the history of astronomy: first they imagine the world as centered around themselves &mdash; and only later do they start to view themselves as moving within a stationary universe, in which the body is just like any other object. It takes several years to reach that stage, and even in their adolescent years, children are still improving their abilities to envision how things appear from other viewpoints.</p>",
    "text": "How do we learn about the real, three-dimensional world? We've seen how certain agencies might map the layout of the skin. But how could we progress from that to learn about the world of space beyond the skin? One might ask why infants can't simply look around to see what's really going on. Unfortunately, the easy-sounding phrase simply look conceals too many hard problems. When you look at an object, some light from it shines into your eye and stimulates some sensors there. However, every motion of your body, head, or eye makes drastic changes to the image in your eye. How can we extract any useful information when everything changes so rapidly? Although it should be possible, in principle, to design a machine that could eventually learn to relate those motions to the resulting changes in the images, this would surely take a long time, and it appears that our brains have evolved with special mechanisms that help us compensate for motions of the body, head, and eye. This makes it easier for other agencies to learn to use visual information.\nLater we'll discuss some other realms of thought in which we use analogies and metaphors to change our points of view. Perhaps those wonderful abilities evolved in similar ways, since recognizing that an object is the same when seen from different views is not so different from being able to imagine things that are not in view at all.\nIn any case, we really do not understand how the child learns to understand space. Perhaps we start by doing many small experiments that lead to our first, crude maps of the skin. Next we might start to correlate these with the motions of our eyes and limbs; two different actions that lead to similar sensations are likely to have passed through the same locations in space. A critical step would be developing some agents that represent a few places outside the skin. Once those places are established (the first ones might be near the infant's face), one could proceed to another stage: the assembly of an agency that represents a network of relationships, trajectories, and directions between those places. Once this is accomplished, the network could continue to extend to include new places and relationships.\nHowever, this would be only the beginning. Long ago, psychologists like Freud and Piaget observed that children seem to recapitulate the history of astronomy: first they imagine the world as centered around themselves \u2014 and only later do they start to view themselves as moving within a stationary universe, in which the body is just like any other object. It takes several years to reach that stage, and even in their adolescent years, children are still improving their abilities to envision how things appear from other viewpoints.",
    "type": "article",
    "title": "11.6 the centered self",
    "tags": [
      {
        "score": 0.5341129899024963,
        "sentiment": 0,
        "count": 1,
        "label": "Change",
        "uri": "https://diffbot.com/entity/X61h5J17CNXuG0rhtJQ_O6A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 154784514456,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 204574261681,
    "gburl": "http://aurellem.org/society-of-mind/som-11.6.html-diffbotxyz3042893903",
    "lastCrawlTimeUTC": 1588760282,
    "timestamp": "Wed, 06 May 2020 10:18:02 GMT"
  },
  {
    "sentiment": 0.281,
    "images": [
      {
        "naturalHeight": 143,
        "width": 317,
        "diffbotUri": "image|3|1788021249",
        "url": "http://aurellem.org/society-of-mind/illus/ch21/21-5.png",
        "naturalWidth": 317,
        "primary": true,
        "height": 143
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-297974946",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.5.html",
    "html": "<p>How do higher-level agencies tell lower-level agents what to do? We might expect this problem to be harder for smaller agencies because they can understand so much less. However, smaller agencies also have fewer concerns and hence need fewer instructions. Indeed, the smallest agencies may need scarcely any messages at all. For example, there's little need to tell Get, Put, or Find what to <em>get,</em> <em>put,</em> or <em>find</em> &mdash; since each can exploit the outcome of the Look-for agency's activity. But how can Look-for know what to look for? We'll see the answer in a trick that makes the problem disappear. In ordinary life this trick is referred to by names like <em>expectation</em> or <em>context.</em></p>\n<p>Whenever someone says a word like <em>apple,</em> you find yourself peculiarly disposed to <em>notice</em> any apple in the present scene. Your eyes tend to turn in the direction of that apple, your arm will prepare itself to reach out in that direction, and your hand will be disposed to form the corresponding grasping shape. This is because many of your agencies have become immersed in the <em>context</em> produced by the agents directly involved with whatever subject was mentioned recently. Thus, the polyneme for the word <em>apple</em> will arouse certain states of agencies that represent an object's color, shape, and size, and these will automatically affect the Look-for agency &mdash; simply because that agency must have been formed in the first place to depend upon the states of our object-description agencies. Accordingly, we can assume that Look-for is part of a larger society that includes connections like these:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch21/21-5.png\"/></figure>\n<p>This diagram portrays an automatic <em>finding machine.</em> Whether an apple was actually seen, imagined, or suggested by naming it, the agents for Color, Shape, and Size will be set into states that correspond to <em>red, round, and apple-sized.</em> Accordingly, when Look-for becomes active, it cannot help but seek an object with those properties. Then, according to our diagram, once such a thing is found, its location will automatically be represented by the state of an agency called Place &mdash; again, because this is the environment within which Look-for grew. The same must be true of the agency Move-arm-to, which must also have grown in the context of some location-representing agency like Place. So when Move-arm-to is aroused, it will automatically tend to move the arm and hand toward that location without needing to be told. Thus, such an arrangement of agencies can carry out the entire apple-moving script with virtually no <em>general-purpose</em> communication at all.</p>\n<p>This could be one explanation of what we call <em>focus of mental attention.</em> Because the agency that represents locations has a limited capacity, whenever some object is seen or heard &mdash; or merely imagined &mdash; other agencies that share the same representation of location are likely to be forced to become engaged with the same object. Then this becomes the momentary <em>it</em> of one's immediate concern.</p>",
    "text": "How do higher-level agencies tell lower-level agents what to do? We might expect this problem to be harder for smaller agencies because they can understand so much less. However, smaller agencies also have fewer concerns and hence need fewer instructions. Indeed, the smallest agencies may need scarcely any messages at all. For example, there's little need to tell Get, Put, or Find what to get, put, or find \u2014 since each can exploit the outcome of the Look-for agency's activity. But how can Look-for know what to look for? We'll see the answer in a trick that makes the problem disappear. In ordinary life this trick is referred to by names like expectation or context.\nWhenever someone says a word like apple, you find yourself peculiarly disposed to notice any apple in the present scene. Your eyes tend to turn in the direction of that apple, your arm will prepare itself to reach out in that direction, and your hand will be disposed to form the corresponding grasping shape. This is because many of your agencies have become immersed in the context produced by the agents directly involved with whatever subject was mentioned recently. Thus, the polyneme for the word apple will arouse certain states of agencies that represent an object's color, shape, and size, and these will automatically affect the Look-for agency \u2014 simply because that agency must have been formed in the first place to depend upon the states of our object-description agencies. Accordingly, we can assume that Look-for is part of a larger society that includes connections like these:\nThis diagram portrays an automatic finding machine. Whether an apple was actually seen, imagined, or suggested by naming it, the agents for Color, Shape, and Size will be set into states that correspond to red, round, and apple-sized. Accordingly, when Look-for becomes active, it cannot help but seek an object with those properties. Then, according to our diagram, once such a thing is found, its location will automatically be represented by the state of an agency called Place \u2014 again, because this is the environment within which Look-for grew. The same must be true of the agency Move-arm-to, which must also have grown in the context of some location-representing agency like Place. So when Move-arm-to is aroused, it will automatically tend to move the arm and hand toward that location without needing to be told. Thus, such an arrangement of agencies can carry out the entire apple-moving script with virtually no general-purpose communication at all.\nThis could be one explanation of what we call focus of mental attention. Because the agency that represents locations has a limited capacity, whenever some object is seen or heard \u2014 or merely imagined \u2014 other agencies that share the same representation of location are likely to be forced to become engaged with the same object. Then this becomes the momentary it of one's immediate concern.",
    "type": "article",
    "title": "21.5 automatism",
    "tags": [
      {
        "score": 0.51162189245224,
        "sentiment": -0.122,
        "count": 5,
        "label": "apple",
        "uri": "https://diffbot.com/entity/XgrXdplpfMRWDa_VBAOd2oA"
      }
    ],
    "docId": 162550645133,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 147896254888,
    "gburl": "http://aurellem.org/society-of-mind/som-21.5.html-diffbotxyz1270591557",
    "lastCrawlTimeUTC": 1588760355,
    "timestamp": "Wed, 06 May 2020 10:19:15 GMT"
  },
  {
    "sentiment": 0.919,
    "humanLanguage": "en",
    "diffbotUri": "article|3|173348765",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-9.html",
    "text": "",
    "type": "article",
    "title": "9 summaries",
    "docId": 253886529947,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 127860425117,
    "gburl": "http://aurellem.org/society-of-mind/som-9.html-diffbotxyz191824114",
    "lastCrawlTimeUTC": 1588760343,
    "timestamp": "Wed, 06 May 2020 10:19:03 GMT"
  },
  {
    "sentiment": 0.56,
    "humanLanguage": "en",
    "diffbotUri": "article|3|394147887",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.5.html",
    "html": "<p>That old idea of classifying things by properties is not entirely satisfactory, because so many kinds of qualities interact in complicated ways. Every situation or condition we experience is influenced or, so to speak, colored, by thousands of contextual shades and hues, just as looking through a tinted glass has faint effects on everything we see.</p>\n<p>material: animate, inanimate; natural, artificial; ideal, actual perceptual: color, texture, taste, sound, temperature solidity: hardness, density, flexibility, strength shape: angularity, curvature, symmetry, verticality permanence: rarity, age, fragility, replaceability location: office, home, vehicle, theater, city, forest, farm environment: indoors, outdoors; public, private</p>\n<p>activity: hunting, gambling, working, entertaining relationship: cooperation, conflict; negotiating, confronting security: safety, danger; refuge, exposure; escape, defeat Some of these conditions and relationships may correspond to language-words, but for most of them we have no words, just as we have no expressions for most flavors and aromas, gestures and intonations, attitudes and dispositions. I'll call them <em>micronemes</em> &mdash; those inner mental context clues that shade our minds' activities in ways we can rarely express. There is a somewhat different microstructure to each person's thoughts; indeed, their inexpressibility reflects our individuality. Nevertheless, we can clarify our image of the mind by envisioning these unknown influences as embodied in the forms of particular agents. Accordingly, in the next few sections, we'll envision our micronemes as K-lines that reach into many agencies with widespread effects on the arousal and suppression of other agents &mdash; including other micronemes. These micronemes participate in all those <em>locking-in</em> and <em>weeding-out</em> processes, so that, for example, the activity of your microneme for <em>outdoors</em> makes a small contribution to arousing your <em>hunting</em> microneme. While each such effect may be relatively small, the effects of activating many micronemes will usually combine to establish a context within which most words are understood unambiguously.</p>\n<p>For example, in one context the word <em>Boston</em> might bring to mind some thoughts about the American Revolution. In a different setting, the same word might lead one to think instead of a geographic location. Other contexts might yield thoughts about famous universities, sporting teams, life-styles, accents of speech, or traditional meals. Each of these concepts must be represented by a certain network of agents that are connected, directly or indirectly, to the word-agent for Boston. But hearing or thinking that word by itself is not enough to determine which of those word-sense networks to activate; this must also depend upon other aspects of the present context. Our hypothesis is that this comes about principally through each word-sense agent learning to recognize the activation of certain combinations of micronemes.</p>\n<p>Even modest families of micronemes could span vast ranges of contexts. A mere forty independent micronemes could specify a trillion different contexts &mdash; and we surely have thousands, and perhaps millions of different micronemes.</p>",
    "text": "That old idea of classifying things by properties is not entirely satisfactory, because so many kinds of qualities interact in complicated ways. Every situation or condition we experience is influenced or, so to speak, colored, by thousands of contextual shades and hues, just as looking through a tinted glass has faint effects on everything we see.\nmaterial: animate, inanimate; natural, artificial; ideal, actual perceptual: color, texture, taste, sound, temperature solidity: hardness, density, flexibility, strength shape: angularity, curvature, symmetry, verticality permanence: rarity, age, fragility, replaceability location: office, home, vehicle, theater, city, forest, farm environment: indoors, outdoors; public, private\nactivity: hunting, gambling, working, entertaining relationship: cooperation, conflict; negotiating, confronting security: safety, danger; refuge, exposure; escape, defeat Some of these conditions and relationships may correspond to language-words, but for most of them we have no words, just as we have no expressions for most flavors and aromas, gestures and intonations, attitudes and dispositions. I'll call them micronemes \u2014 those inner mental context clues that shade our minds' activities in ways we can rarely express. There is a somewhat different microstructure to each person's thoughts; indeed, their inexpressibility reflects our individuality. Nevertheless, we can clarify our image of the mind by envisioning these unknown influences as embodied in the forms of particular agents. Accordingly, in the next few sections, we'll envision our micronemes as K-lines that reach into many agencies with widespread effects on the arousal and suppression of other agents \u2014 including other micronemes. These micronemes participate in all those locking-in and weeding-out processes, so that, for example, the activity of your microneme for outdoors makes a small contribution to arousing your hunting microneme. While each such effect may be relatively small, the effects of activating many micronemes will usually combine to establish a context within which most words are understood unambiguously.\nFor example, in one context the word Boston might bring to mind some thoughts about the American Revolution. In a different setting, the same word might lead one to think instead of a geographic location. Other contexts might yield thoughts about famous universities, sporting teams, life-styles, accents of speech, or traditional meals. Each of these concepts must be represented by a certain network of agents that are connected, directly or indirectly, to the word-agent for Boston. But hearing or thinking that word by itself is not enough to determine which of those word-sense networks to activate; this must also depend upon other aspects of the present context. Our hypothesis is that this comes about principally through each word-sense agent learning to recognize the activation of certain combinations of micronemes.\nEven modest families of micronemes could span vast ranges of contexts. A mere forty independent micronemes could specify a trillion different contexts \u2014 and we surely have thousands, and perhaps millions of different micronemes.",
    "type": "article",
    "title": "20.5 micronemes",
    "tags": [
      {
        "score": 0.5506625175476074,
        "sentiment": 0,
        "count": 2,
        "label": "Boston",
        "uri": "https://diffbot.com/entity/A7vnJ0j-OP4qXP4s9wNDbEQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Locality",
          "http://dbpedia.org/ontology/Settlement",
          "http://dbpedia.org/ontology/City"
        ]
      },
      {
        "score": 0.5204195380210876,
        "sentiment": 0,
        "count": 1,
        "label": "American Revolutionary War",
        "uri": "https://diffbot.com/entity/X45XPkgPkPXieM0M8pPp7Qw",
        "rdfTypes": ["http://dbpedia.org/ontology/Event"]
      },
      {
        "score": 0.5094258785247803,
        "sentiment": 0.954,
        "count": 2,
        "label": "hunting",
        "uri": "https://diffbot.com/entity/X_icJX2QxPaGdB92UxtUXdQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Skill",
          "http://dbpedia.org/ontology/Activity"
        ]
      }
    ],
    "docId": 68624187821,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 45676855730,
    "gburl": "http://aurellem.org/society-of-mind/som-20.5.html-diffbotxyz1995629013",
    "lastCrawlTimeUTC": 1588760147,
    "timestamp": "Wed, 06 May 2020 10:15:47 GMT"
  },
  {
    "sentiment": -0.406,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1236806664",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.5.html",
    "html": "<p>Linking structures together into chains is one of our most useful kinds of reasoning. Suppose you learned that <em>John gave the kite to Mary</em> and then <em>Mary gave the kite to Jack.</em> You could then conclude that the kite went from John to Jack. How do we draw such conclusions? Some people think we use <em>logic</em> for this. A simpler theory is that we do it by fitting together Trans-frames into chains. Suppose you see two frames like these:</p>\n<p>All A's are B' s and All B 's are C's.</p>\n<p>Then just combine the first Origin with the second Destination to make this new <em>deduction-frame</em>:</p>\n<p>All A's are C's.</p>\n<p>To do this sort of <em>reasoning,</em> we have to use our isonomes to rearrange our short-term memories. But this requires more than simple chaining. For example, all older children can infer that Tweety can fly from <em>Tweety is a bird</em> and <em>All birds can fly.</em> To do this, though, one has to deal with a disparity: the first B is <em>a bird</em> while the second B is <em>all birds.</em> To be able to make such chains would be virtually useless if we could do it only when both pronome assignments were absolutely identical. Over the years, children improve their abilities to decide when two different structures are similar enough to justify making chain-links. This often requires us to recall and apply other types of knowledge at appropriate level-bands of detail.</p>\n<p>Children take many years to learn effective ways to use their pronomes and isonomes. The youngest ones can neither rearrange their representations of physical scenes nor make the kinds of inference we're discussing here. To think like adults, we must develop and learn to use memory-controlling processes that manipulate several sets of pronome values at the same time. Just such a process was concealed in our simple script for <em>Put the apple in the pail</em> &mdash; which first appears to be merely a matter of assigning <em>apple</em> to the Origin and <em>pail</em> to the Destination. But you can't Put something until you Get it, so this must actually involve two Trans-frame operations. The first is for moving your hand to the apple, and the second is for moving the apple to the pail. During the transition, your pronomes have to change their roles since the apple's location is the Destination of the first Trans, but then becomes the Origin of the second Trans. No matter that this seems too obvious to state; some mental process has to switch that pronome's role.</p>\n<p>By learning to manipulate our isonomes, we become able to combine mental representations into structures that resemble bridges, chains, and towers. Our language-agencies learn to express these in the form of compound sentences, by using conjunctive grammar words like <em>and,</em> <em>because,</em> or <em>or.</em> But language is not the only realm in which we learn to <em>conceptualize</em> &mdash; that is, to treat our mental processes almost as though they were object-things. After you solve a hard problem, you may find yourself representing the steps you took as if they were the parts of a physical structure. Doing this can enable you to reassemble them into other forms that achieve the same results with much more speed and much less thought.</p>",
    "text": "Linking structures together into chains is one of our most useful kinds of reasoning. Suppose you learned that John gave the kite to Mary and then Mary gave the kite to Jack. You could then conclude that the kite went from John to Jack. How do we draw such conclusions? Some people think we use logic for this. A simpler theory is that we do it by fitting together Trans-frames into chains. Suppose you see two frames like these:\nAll A's are B' s and All B 's are C's.\nThen just combine the first Origin with the second Destination to make this new deduction-frame:\nAll A's are C's.\nTo do this sort of reasoning, we have to use our isonomes to rearrange our short-term memories. But this requires more than simple chaining. For example, all older children can infer that Tweety can fly from Tweety is a bird and All birds can fly. To do this, though, one has to deal with a disparity: the first B is a bird while the second B is all birds. To be able to make such chains would be virtually useless if we could do it only when both pronome assignments were absolutely identical. Over the years, children improve their abilities to decide when two different structures are similar enough to justify making chain-links. This often requires us to recall and apply other types of knowledge at appropriate level-bands of detail.\nChildren take many years to learn effective ways to use their pronomes and isonomes. The youngest ones can neither rearrange their representations of physical scenes nor make the kinds of inference we're discussing here. To think like adults, we must develop and learn to use memory-controlling processes that manipulate several sets of pronome values at the same time. Just such a process was concealed in our simple script for Put the apple in the pail \u2014 which first appears to be merely a matter of assigning apple to the Origin and pail to the Destination. But you can't Put something until you Get it, so this must actually involve two Trans-frame operations. The first is for moving your hand to the apple, and the second is for moving the apple to the pail. During the transition, your pronomes have to change their roles since the apple's location is the Destination of the first Trans, but then becomes the Origin of the second Trans. No matter that this seems too obvious to state; some mental process has to switch that pronome's role.\nBy learning to manipulate our isonomes, we become able to combine mental representations into structures that resemble bridges, chains, and towers. Our language-agencies learn to express these in the form of compound sentences, by using conjunctive grammar words like and, because, or or. But language is not the only realm in which we learn to conceptualize \u2014 that is, to treat our mental processes almost as though they were object-things. After you solve a hard problem, you may find yourself representing the steps you took as if they were the parts of a physical structure. Doing this can enable you to reassemble them into other forms that achieve the same results with much more speed and much less thought.",
    "type": "article",
    "title": "22.5 inference",
    "tags": [
      {
        "score": 0.6503326892852783,
        "sentiment": 0,
        "count": 3,
        "label": "Origin Systems",
        "uri": "https://diffbot.com/entity/Ccb_oHREFPvC2aSvyOr652A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.6453104019165039,
        "sentiment": 0,
        "count": 2,
        "label": "Tweety",
        "uri": "https://diffbot.com/entity/XBaKjminqOkS_B5qBSLB21A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6416433453559875,
        "sentiment": 0.453,
        "count": 2,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5852598547935486,
        "sentiment": 0.35,
        "count": 2,
        "label": "John",
        "uri": "https://diffbot.com/entity/PcuxmIVeuOsGta8LRkjfytQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5674619078636169,
        "sentiment": 0,
        "count": 1,
        "label": "C",
        "uri": "https://diffbot.com/entity/X4K2_qY_jNBGAZG6xOsO29A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Language",
          "http://dbpedia.org/ontology/ProgrammingLanguage",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5673826336860657,
        "sentiment": 0,
        "count": 2,
        "label": "Captain Jack Harkness",
        "uri": "https://diffbot.com/entity/Xsn0ojKAUP1SBQiNs0rC4Hg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5453634858131409,
        "sentiment": 0,
        "count": 2,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      },
      {
        "score": 0.5326040387153625,
        "sentiment": 0,
        "count": 1,
        "label": "Short-Term Memories",
        "uri": "https://diffbot.com/entity/X_T2HkD74PFWvQ0Z-k1c_dQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5219104290008545,
        "sentiment": 0,
        "count": 3,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 117829861822,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 83329597860,
    "gburl": "http://aurellem.org/society-of-mind/som-22.5.html-diffbotxyz2528328292",
    "lastCrawlTimeUTC": 1588760170,
    "timestamp": "Wed, 06 May 2020 10:16:10 GMT"
  },
  {
    "sentiment": 0.273,
    "humanLanguage": "fr",
    "diffbotUri": "article|3|1035142873",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-27.html",
    "text": "",
    "type": "article",
    "title": "27 censors and jokes",
    "tags": [
      {
        "score": 0.5567688345909119,
        "sentiment": 0.302,
        "count": 1,
        "label": "Eure",
        "uri": "https://diffbot.com/entity/AFGSqjyq8MpiX7aDdpLXshA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 231587463600,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 79306326456,
    "gburl": "http://aurellem.org/society-of-mind/som-27.html-diffbotxyz1369567816",
    "lastCrawlTimeUTC": 1588760232,
    "timestamp": "Wed, 06 May 2020 10:17:12 GMT"
  },
  {
    "sentiment": -0.4,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-877298425",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-4.4.html",
    "html": "<p>How do we control our minds? Ideally, we first choose what we want to do, then make ourselves do it. But that's harder than it sounds: we spend our lives in search of schemes for self-control. We celebrate when we succeed, and when we fail, we're angry with ourselves for not behaving as we wanted to &mdash; and then we try to scold or shame or bribe ourselves to change our ways. But wait! How could a self be angry with itself? Who would be mad at whom? Consider an example from everyday life.</p>\n<p>I was trying to concentrate on a certain problem but was getting bored and sleepy. Then I imagined that one of my competitors, Professor Challenger, was about to solve the same problem. An angry wish to frustrate Challenger then kept me working on the problem for a while. The strange thing was, this problem was not of the sort that ever interested Challenger.</p>\n<p>What makes us use such roundabout techniques to influence ourselves? Why be so indirect, inventing misrepresentations, fantasies, and outright lies? Why can't we simply tell ourselves to do the things we want to do?</p>\n<p>To understand how something works, one has to know its purposes. Once, no one understood the heart. But as soon as it was seen that hearts move blood, a lot of other things made sense: those things that looked like pipes and valves were really pipes and valves indeed &mdash; and anxious, pounding, pulsing hearts were recognized as simple pumps. New speculations could then be formed: was this to give our tissues drink or food? Was it to keep our bodies warm or cool? For sending messages from place to place? In fact, all those hypotheses were correct, and when that surge of functional ideas led to the guess that blood can carry air as well, more puzzle parts fell into place.</p>\n<p>To understand what we call the Self, we first must see what Selves are for. One function of the Self is to keep us from changing too rapidly. Each person must make some long-range plans in order to balance single-purposeness against attempts to do everything at once. But it is not enough simply to instruct an agency to start to carry out our plans. We also have to find some ways to constrain the changes we might later make &mdash; to prevent ourselves from turning those plan-agents off again! If we changed our minds too recklessly, we could never know what we might want next. We'd never get much done because we could never depend on ourselves.</p>\n<p>Those ordinary views are wrong that hold that Selves are magic, self-indulgent luxuries that enable our minds to break the bonds of natural cause and law. Instead, those Selves are practical necessities. The myths that say that Selves embody special kinds of liberty are merely masquerades. Part of their function is to hide from us the nature of our self-ideals &mdash; the chains we forge to keep ourselves from wrecking all the plans we make.</p>",
    "text": "How do we control our minds? Ideally, we first choose what we want to do, then make ourselves do it. But that's harder than it sounds: we spend our lives in search of schemes for self-control. We celebrate when we succeed, and when we fail, we're angry with ourselves for not behaving as we wanted to \u2014 and then we try to scold or shame or bribe ourselves to change our ways. But wait! How could a self be angry with itself? Who would be mad at whom? Consider an example from everyday life.\nI was trying to concentrate on a certain problem but was getting bored and sleepy. Then I imagined that one of my competitors, Professor Challenger, was about to solve the same problem. An angry wish to frustrate Challenger then kept me working on the problem for a while. The strange thing was, this problem was not of the sort that ever interested Challenger.\nWhat makes us use such roundabout techniques to influence ourselves? Why be so indirect, inventing misrepresentations, fantasies, and outright lies? Why can't we simply tell ourselves to do the things we want to do?\nTo understand how something works, one has to know its purposes. Once, no one understood the heart. But as soon as it was seen that hearts move blood, a lot of other things made sense: those things that looked like pipes and valves were really pipes and valves indeed \u2014 and anxious, pounding, pulsing hearts were recognized as simple pumps. New speculations could then be formed: was this to give our tissues drink or food? Was it to keep our bodies warm or cool? For sending messages from place to place? In fact, all those hypotheses were correct, and when that surge of functional ideas led to the guess that blood can carry air as well, more puzzle parts fell into place.\nTo understand what we call the Self, we first must see what Selves are for. One function of the Self is to keep us from changing too rapidly. Each person must make some long-range plans in order to balance single-purposeness against attempts to do everything at once. But it is not enough simply to instruct an agency to start to carry out our plans. We also have to find some ways to constrain the changes we might later make \u2014 to prevent ourselves from turning those plan-agents off again! If we changed our minds too recklessly, we could never know what we might want next. We'd never get much done because we could never depend on ourselves.\nThose ordinary views are wrong that hold that Selves are magic, self-indulgent luxuries that enable our minds to break the bonds of natural cause and law. Instead, those Selves are practical necessities. The myths that say that Selves embody special kinds of liberty are merely masquerades. Part of their function is to hide from us the nature of our self-ideals \u2014 the chains we forge to keep ourselves from wrecking all the plans we make.",
    "type": "article",
    "title": "4.4 the conservative self",
    "tags": [
      {
        "score": 0.7132629156112671,
        "sentiment": 0,
        "count": 1,
        "label": "conservatism",
        "uri": "https://diffbot.com/entity/XQRx2hpoyPeW3kqmh6Xooyg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Ideology",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6355576515197754,
        "sentiment": 0,
        "count": 1,
        "label": "Professor Challenger",
        "uri": "https://diffbot.com/entity/XobmEtwweOk688vIQ90PUWw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.613384485244751,
        "sentiment": 0,
        "count": 2,
        "label": "Challenger",
        "uri": "https://diffbot.com/entity/XkzzHeCalMbWxeZ0vxplZLg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/MeanOfTransportation",
          "http://dbpedia.org/ontology/Rocket"
        ]
      },
      {
        "score": 0.6036149263381958,
        "sentiment": 0.713,
        "count": 4,
        "label": "Selves",
        "uri": "https://diffbot.com/entity/LZZ_hTp8HNlaiToC41UDbsw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/NaturalPlace",
          "http://dbpedia.org/ontology/BodyOfWater",
          "http://dbpedia.org/ontology/Stream",
          "http://dbpedia.org/ontology/River"
        ]
      },
      {
        "score": 0.584476113319397,
        "sentiment": 0.27,
        "count": 1,
        "label": "Self",
        "uri": "https://diffbot.com/entity/XVTcSMEEgNYi4-xRQmuUv2g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Language",
          "http://dbpedia.org/ontology/ProgrammingLanguage"
        ]
      }
    ],
    "docId": 258046230942,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 196734026129,
    "gburl": "http://aurellem.org/society-of-mind/som-4.4.html-diffbotxyz4170333211",
    "lastCrawlTimeUTC": 1588760195,
    "timestamp": "Wed, 06 May 2020 10:16:35 GMT"
  },
  {
    "sentiment": 0.986,
    "humanLanguage": "en",
    "diffbotUri": "article|3|261162836",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-5.4.html",
    "html": "<p>Why do we accept that paradoxical image of a central Self inside the self? Because it serves us well in many spheres of practical life. Here are some reasons to regard a person as a single thing.</p>\n<p>The Physical World: Our bodies act like other objects that take up space. Because of that, we must base our plans and decisions on having a single body. Two people cannot fit where there is room for only one &mdash; nor can a person walk through walls or stay aloft without support.</p>\n<p>Personal Privacy: When Mary tells Jack something, she must remember to <em>whom</em> it was told, and she must not assume that every other person knows it, too. Also, without the concept of an individual, we could have no sense of responsibility. Mental Activity: We often find it hard to think two different thoughts at once particularly when they're similar, because we get <em>confused</em> when the same agencies are asked to do different jobs at the same time.</p>\n<p>Why do our mental processes so often seem to us to flow in <em>streams of consciousness</em>? Perhaps because, in order to keep control, we have to simplify how we represent what's happening. Then, when that complicated mental scene is <em>straightened out,</em> it seems as though a single pipeline of ideas were flowing through the mind.</p>\n<p>These are all compelling reasons why it helps to see ourselves as singletons. Still, each of us must also learn not only that different people have their own identities, but that the same person can entertain different beliefs, plans, and dispositions at the same time.</p>\n<p>For finding good ideas about psychology, the single-agent image has become a grave impediment. To comprehend the human mind is surely one of the hardest tasks any mind can face. The legend of the single Self can only divert us from the target of that inquiry.</p>",
    "text": "Why do we accept that paradoxical image of a central Self inside the self? Because it serves us well in many spheres of practical life. Here are some reasons to regard a person as a single thing.\nThe Physical World: Our bodies act like other objects that take up space. Because of that, we must base our plans and decisions on having a single body. Two people cannot fit where there is room for only one \u2014 nor can a person walk through walls or stay aloft without support.\nPersonal Privacy: When Mary tells Jack something, she must remember to whom it was told, and she must not assume that every other person knows it, too. Also, without the concept of an individual, we could have no sense of responsibility. Mental Activity: We often find it hard to think two different thoughts at once particularly when they're similar, because we get confused when the same agencies are asked to do different jobs at the same time.\nWhy do our mental processes so often seem to us to flow in streams of consciousness? Perhaps because, in order to keep control, we have to simplify how we represent what's happening. Then, when that complicated mental scene is straightened out, it seems as though a single pipeline of ideas were flowing through the mind.\nThese are all compelling reasons why it helps to see ourselves as singletons. Still, each of us must also learn not only that different people have their own identities, but that the same person can entertain different beliefs, plans, and dispositions at the same time.\nFor finding good ideas about psychology, the single-agent image has become a grave impediment. To comprehend the human mind is surely one of the hardest tasks any mind can face. The legend of the single Self can only divert us from the target of that inquiry.",
    "type": "article",
    "title": "5.4 personal identity",
    "tags": [
      {
        "score": 0.7444378733634949,
        "sentiment": 0,
        "count": 1,
        "label": "The Physical World",
        "uri": "https://diffbot.com/entity/XsRe-MpfpOyyu8TJ22zWbdg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.6285340189933777,
        "sentiment": 0.772,
        "count": 1,
        "label": "cultural identity",
        "uri": "https://diffbot.com/entity/Xan9aqh0jOn-5YJhgTZXcoQ"
      },
      {
        "score": 0.6156511306762695,
        "sentiment": 0.891,
        "count": 1,
        "label": "Personal Privacy",
        "uri": "https://diffbot.com/entity/OkY9ynVUyN1-kpPHLnzeXnw",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      },
      {
        "score": 0.5080839991569519,
        "sentiment": 0.713,
        "count": 1,
        "label": "Mary I of England",
        "uri": "https://diffbot.com/entity/PdCs9CVcgM4mS1SzREIM_wA",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5054129958152771,
        "sentiment": 0.404,
        "count": 1,
        "label": "Jack Shephard",
        "uri": "https://diffbot.com/entity/XPb-cc-yAPbK9HeP1VhSLpQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      }
    ],
    "docId": 45889536439,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 219331674523,
    "gburl": "http://aurellem.org/society-of-mind/som-5.4.html-diffbotxyz1940525143",
    "lastCrawlTimeUTC": 1588760010,
    "timestamp": "Wed, 06 May 2020 10:13:30 GMT"
  },
  {
    "sentiment": 0.937,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1520945186",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-30.html",
    "text": "",
    "type": "article",
    "title": "30 mental models",
    "tags": [
      {
        "score": 0.6329976320266724,
        "sentiment": 0.91,
        "count": 1,
        "label": "art model",
        "uri": "https://diffbot.com/entity/XyK2hsDg8MjKr3TKGeJTpgA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession"
        ]
      }
    ],
    "docId": 105080406462,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 213590737297,
    "gburl": "http://aurellem.org/society-of-mind/som-30.html-diffbotxyz945164736",
    "lastCrawlTimeUTC": 1588760119,
    "timestamp": "Wed, 06 May 2020 10:15:19 GMT"
  },
  {
    "sentiment": -0.352,
    "humanLanguage": "en",
    "diffbotUri": "article|3|850314540",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.5.html",
    "html": "<blockquote> Everyone will readily allow that there is a considerable difference between the perceptions of the mind, when a man feels the pain of excessive heat, or the pleasure of moderate warmth, and when he afterwards recalls to his memory this sensation, or anticipates it by his imagination. These faculties may mimic or copy the perceptions of the senses; but they never can entirely reach the force and vivacity of the original sentiment&hellip;The most lively thought is still inferior to the dullest sensation. &mdash;Marcel proust </blockquote>\n<p>We like to think of memories as though they could restore to us things we've known in the past. But memories can't really bring things back; they only reproduce some fragments of our former states of mind, when various sights, sounds, touches, smells, and tastes affected us.</p>\n<p>Then what makes some recollections seem so real? The secret is that real-time experience is just as indirect! The closest we can come to apprehending the world, in any case, is through the descriptions our agents make. In fact, if we inquired instead about why real things seem real, we would see that this depends, as well, on memories of things we've already known.</p>\n<p>For instance, when you see a telephone, you have a sense not only of the aspects you can see &mdash; its color, texture, size, and shape &mdash; but also of how it feels to hold the instrument to your ear. You also seem to know at once what telephones are for: that you speak into here and listen there; that when it rings you answer it; that when you want to call, you dial it. You have a sense of what it weighs, of whether it is soft or hard, of what its other side is like &mdash; although you haven't even touched it yet. These apprehensions come from memories.</p>\n<p>The Immanence Illusion: Whenever you can answer a question without a noticeable delay, it seems as though that answer were already active in your mind.</p>\n<p>This is part of why we feel that what we see is <em>present</em> in the here and now. But it isn't really true that whenever a real object appears before our eyes, its full description is instantly available. Our sense of momentary mental time is flawed; our vision-agencies begin arousing memories before their own work is fully done. For example, when you see a horse, a preliminary recognition of its general shape may lead some vision-agents to start evoking memories about horses before the other vision-agents have discerned its head or tail. Perceptions can evoke our memories so quickly that we can't distinguish what we've seen from what we've been led to recollect.</p>\n<p>This explains some of the subjective difference between seeing and remembering. If you first imagined a black telephone, you probably would not find it hard to reimagine it as red. But when you see a black telephone and then attempt to think of it as red, your vision-systems swiftly change it back! So the experience of seeing things has a relatively rigid character, in contrast to the experience of imagining things. Every change that the rest of your mind tries to impose upon your vision-agencies is resisted and usually reversed. Perhaps it is this descriptive rigidity that we identify with <em>vividness</em> or <em>objectivity.</em> I do not mean to suggest that this is usually an illusion, because it often truly reflects the persistency and permanence of actual physical objects. Sometimes, though, our sense of objectivity can get reversed &mdash; as when an attitude or memory becomes more stable and persistent than what it represents. For example, our attitudes toward things we love or loathe are often much less changeable than those things themselves &mdash; particularly in the case of other people's personalities. In instances like these, our private memories can be more rigid than reality.</p>",
    "text": "Everyone will readily allow that there is a considerable difference between the perceptions of the mind, when a man feels the pain of excessive heat, or the pleasure of moderate warmth, and when he afterwards recalls to his memory this sensation, or anticipates it by his imagination. These faculties may mimic or copy the perceptions of the senses; but they never can entirely reach the force and vivacity of the original sentiment\u2026The most lively thought is still inferior to the dullest sensation. \u2014Marcel proust\nWe like to think of memories as though they could restore to us things we've known in the past. But memories can't really bring things back; they only reproduce some fragments of our former states of mind, when various sights, sounds, touches, smells, and tastes affected us.\nThen what makes some recollections seem so real? The secret is that real-time experience is just as indirect! The closest we can come to apprehending the world, in any case, is through the descriptions our agents make. In fact, if we inquired instead about why real things seem real, we would see that this depends, as well, on memories of things we've already known.\nFor instance, when you see a telephone, you have a sense not only of the aspects you can see \u2014 its color, texture, size, and shape \u2014 but also of how it feels to hold the instrument to your ear. You also seem to know at once what telephones are for: that you speak into here and listen there; that when it rings you answer it; that when you want to call, you dial it. You have a sense of what it weighs, of whether it is soft or hard, of what its other side is like \u2014 although you haven't even touched it yet. These apprehensions come from memories.\nThe Immanence Illusion: Whenever you can answer a question without a noticeable delay, it seems as though that answer were already active in your mind.\nThis is part of why we feel that what we see is present in the here and now. But it isn't really true that whenever a real object appears before our eyes, its full description is instantly available. Our sense of momentary mental time is flawed; our vision-agencies begin arousing memories before their own work is fully done. For example, when you see a horse, a preliminary recognition of its general shape may lead some vision-agents to start evoking memories about horses before the other vision-agents have discerned its head or tail. Perceptions can evoke our memories so quickly that we can't distinguish what we've seen from what we've been led to recollect.\nThis explains some of the subjective difference between seeing and remembering. If you first imagined a black telephone, you probably would not find it hard to reimagine it as red. But when you see a black telephone and then attempt to think of it as red, your vision-systems swiftly change it back! So the experience of seeing things has a relatively rigid character, in contrast to the experience of imagining things. Every change that the rest of your mind tries to impose upon your vision-agencies is resisted and usually reversed. Perhaps it is this descriptive rigidity that we identify with vividness or objectivity. I do not mean to suggest that this is usually an illusion, because it often truly reflects the persistency and permanence of actual physical objects. Sometimes, though, our sense of objectivity can get reversed \u2014 as when an attitude or memory becomes more stable and persistent than what it represents. For example, our attitudes toward things we love or loathe are often much less changeable than those things themselves \u2014 particularly in the case of other people's personalities. In instances like these, our private memories can be more rigid than reality.",
    "type": "article",
    "title": "15.5 the immanence illusion",
    "tags": [
      {
        "score": 0.6115967035293579,
        "sentiment": 0,
        "count": 1,
        "label": "Illusion",
        "uri": "https://diffbot.com/entity/OwrQP05rmNdOzkeMC8cS_4A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      },
      {
        "score": 0.6051356792449951,
        "sentiment": 0,
        "count": 1,
        "label": "Immanence",
        "uri": "https://diffbot.com/entity/Bv_Bj90w6OYW8FbpaK1D_CQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      }
    ],
    "docId": 113377608072,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 20942848435,
    "gburl": "http://aurellem.org/society-of-mind/som-15.5.html-diffbotxyz2184904162",
    "lastCrawlTimeUTC": 1588760053,
    "timestamp": "Wed, 06 May 2020 10:14:13 GMT"
  },
  {
    "sentiment": 0.914,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1077121714",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-23.4.html",
    "html": "<p>Let's return just one time more to all the things that More can mean. Each usage has a different sense &mdash; more powerful; more meaningful &mdash; and each such meaning must be learned. In other words, each use of More involves a connection with an agent for some adjective. But More must also engage some systematic use of isonomes, since all the different meanings share a certain common character.</p>\n<p>When we hear the word <em>more,</em> we become disposed to make comparisons.</p>\n<p>This suggests that More engages both an accumulation of different meanings and also some systematic, isonomelike effect. Indeed, More could exploit our time-blinking mechanism, which already uses isonomes to make comparisons. To do that, More would have to activate a memory-control process that <em>blinks</em> whichever pronomes have been assigned to the things to be compared. Then their differences would be computed automatically.</p>\n<p>More needs two additional ingredients. We'd never ask, all by itself, <em>Which one is more, an apple or a pear?</em> &mdash; because our general- purpose comparison script would generate difference-descriptions in too many agencies. We also need to know which kind of difference is of concern at the moment. So we scarcely ever say <em>more</em> by itself but usually attach some modifier &mdash; more red, say, or more expensive. Of course, if our focus of concern is already clear from the context &mdash; for example, if it is clear that we want to know whether apples or pears are more expensive &mdash; then we need not express this explicitly.</p>\n<p>Finally, it is one thing to find a difference, but another to know whether to call it <em>more</em> or <em>less.</em> It may seem self-evident that Taller corresponds to <em>more,</em> and Thinner corresponds to <em>less</em> &mdash; yet that is something we once had to learn. This is the other ingredient of More: we need another polyneme to say which sorts of differences should be considered positive. In English we sometimes encode such preferences as choices between pairs of adjectives like <em>large</em> and</p>\n<p><em>small,</em> but we do not have pairs of words for concepts such as <em>triangular</em> or <em>red,</em> presumably because we do not think of them as having <em>natural</em> opposites. Instead, we can use word-pairs like <em>more red</em> and <em>less triangular.</em> We can even modify the words themselves; we often say <em>redder</em> or <em>rounder</em> &mdash; but for some reason we never say <em>triangularer.</em></p>\n<p>How does one answer a question like <em>Which is bigger, a large mouse or a small elephant?</em> We can't compare two descriptions until we engage enough knowledge to construct suitable representations of them. One way to compare mouse and elephant would be to envision another entity of intermediate size. A suitcase would be suitable for this, since it could hold the largest mouse but not the smallest elephant. How do you find such standards for comparison? That can take considerable time, during which you have to search your memories for structures that can serve as links for longer chains of comparisons. As life proceeds, each person's concept of More grows more and more elaborate. When it comes to notions like more similar, more interesting, or <em>more difficult,</em> there seems no limit to the complexity of what a word like <em>more</em> can represent.</p>",
    "text": "Let's return just one time more to all the things that More can mean. Each usage has a different sense \u2014 more powerful; more meaningful \u2014 and each such meaning must be learned. In other words, each use of More involves a connection with an agent for some adjective. But More must also engage some systematic use of isonomes, since all the different meanings share a certain common character.\nWhen we hear the word more, we become disposed to make comparisons.\nThis suggests that More engages both an accumulation of different meanings and also some systematic, isonomelike effect. Indeed, More could exploit our time-blinking mechanism, which already uses isonomes to make comparisons. To do that, More would have to activate a memory-control process that blinks whichever pronomes have been assigned to the things to be compared. Then their differences would be computed automatically.\nMore needs two additional ingredients. We'd never ask, all by itself, Which one is more, an apple or a pear? \u2014 because our general- purpose comparison script would generate difference-descriptions in too many agencies. We also need to know which kind of difference is of concern at the moment. So we scarcely ever say more by itself but usually attach some modifier \u2014 more red, say, or more expensive. Of course, if our focus of concern is already clear from the context \u2014 for example, if it is clear that we want to know whether apples or pears are more expensive \u2014 then we need not express this explicitly.\nFinally, it is one thing to find a difference, but another to know whether to call it more or less. It may seem self-evident that Taller corresponds to more, and Thinner corresponds to less \u2014 yet that is something we once had to learn. This is the other ingredient of More: we need another polyneme to say which sorts of differences should be considered positive. In English we sometimes encode such preferences as choices between pairs of adjectives like large and\nsmall, but we do not have pairs of words for concepts such as triangular or red, presumably because we do not think of them as having natural opposites. Instead, we can use word-pairs like more red and less triangular. We can even modify the words themselves; we often say redder or rounder \u2014 but for some reason we never say triangularer.\nHow does one answer a question like Which is bigger, a large mouse or a small elephant? We can't compare two descriptions until we engage enough knowledge to construct suitable representations of them. One way to compare mouse and elephant would be to envision another entity of intermediate size. A suitcase would be suitable for this, since it could hold the largest mouse but not the smallest elephant. How do you find such standards for comparison? That can take considerable time, during which you have to search your memories for structures that can serve as links for longer chains of comparisons. As life proceeds, each person's concept of More grows more and more elaborate. When it comes to notions like more similar, more interesting, or more difficult, there seems no limit to the complexity of what a word like more can represent.",
    "type": "article",
    "title": "23.4 the meanings of more",
    "tags": [
      {
        "score": 0.6579387187957764,
        "sentiment": 0.376,
        "count": 1,
        "label": "Fly Me to the Moon",
        "uri": "https://diffbot.com/entity/XRZrQ-OahMnaptNVys6VVhw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6026995182037354,
        "sentiment": 0.943,
        "count": 2,
        "label": "systematics",
        "uri": "https://diffbot.com/entity/XuHluo-ZZPk-JfsViXn8HFg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 154450772394,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 130430763424,
    "gburl": "http://aurellem.org/society-of-mind/som-23.4.html-diffbotxyz1197204262",
    "lastCrawlTimeUTC": 1588760112,
    "timestamp": "Wed, 06 May 2020 10:15:12 GMT"
  },
  {
    "sentiment": -0.465,
    "images": [
      {
        "naturalHeight": 128,
        "width": 338,
        "diffbotUri": "image|3|222924235",
        "url": "http://aurellem.org/society-of-mind/illus/ch6/6-1.png",
        "naturalWidth": 338,
        "primary": true,
        "height": 128
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1611907950",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.4.html",
    "html": "<p>There is one way for a mind to watch itself and still keep track of what's happening. Divide the brain into two parts, A and B. Connect the A-brain's inputs and outputs to the real world &mdash; so it can sense what happens there. But don't connect the B-brain to the outer world at all; instead, connect it so that the A-brain is the B-brain's world!</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch6/6-1.png\"/></figure>\n<p>Now A can see and act upon what happens in the outside world &mdash; while B can <em>see</em> and influence what happens inside A. What uses could there be for such a B? Here are some A-activities that B might learn to recognize and influence.</p>\n<p>A seems disordered and confused. Inhibit that activity. A appears to be repeating itself. Make A stop. Do something else. A does something B considers good. Make A remember this. A is occupied with too much detail. Make A take a higher-level view A is not being specific enough. Focus A on lower-level details.</p>\n<p>This two-part arrangement could be a step toward having a more <em>reflective</em> mind-society. The B-brain could do experiments with the A-brain, just as the A-brain can experiment with the body or with the objects and people surrounding it. And just as A can attempt to predict and control what happens in the outer world, B can try to predict and control what A will do. For example, the B-brain could supervise how the A-brain learns, either by making changes in A directly or by influencing A's own learning processes.</p>\n<p>Even though B may have no concept of what A's activities mean in relation to the outer world, it is still possible for B to be useful to A.</p>\n<p>This is because a B-brain could learn to play a role somewhat like that of a counselor, psychologist, or management consultant, who can assess a client's mental strategy without having to understand all the details of that client's profession. Without having any idea of what A's goals are, B might be able to learn to tell when A is not accomplishing them but only going around in circles or wandering, confused because certain A-agents are repeating the same things over and over again. Then B might try some simple remedies, like suppressing some of those A-agents. To be sure, this could also result in B's activities becoming nuisances to A. For example, if A had the goal of adding up a long column of numbers, B might start to interfere with this because, from B's point of view, A appears to have become trapped in a repetitive loop. This could cause a person accustomed to more variety to find it difficult to concentrate on such a task and complain of being bored.</p>\n<p>To the extent that the B-brain knows what is happening in A, the entire system could be considered to be partly <em>self-aware.</em> However, if we connect A and B to <em>watch</em> each other too closely, then anything could happen, and the entire system might become unstable. In any case,</p>\n<p>there is no reason to stop with only two levels; we could connect a C-brain to watch the B-brain, and so on.</p>",
    "text": "There is one way for a mind to watch itself and still keep track of what's happening. Divide the brain into two parts, A and B. Connect the A-brain's inputs and outputs to the real world \u2014 so it can sense what happens there. But don't connect the B-brain to the outer world at all; instead, connect it so that the A-brain is the B-brain's world!\nNow A can see and act upon what happens in the outside world \u2014 while B can see and influence what happens inside A. What uses could there be for such a B? Here are some A-activities that B might learn to recognize and influence.\nA seems disordered and confused. Inhibit that activity. A appears to be repeating itself. Make A stop. Do something else. A does something B considers good. Make A remember this. A is occupied with too much detail. Make A take a higher-level view A is not being specific enough. Focus A on lower-level details.\nThis two-part arrangement could be a step toward having a more reflective mind-society. The B-brain could do experiments with the A-brain, just as the A-brain can experiment with the body or with the objects and people surrounding it. And just as A can attempt to predict and control what happens in the outer world, B can try to predict and control what A will do. For example, the B-brain could supervise how the A-brain learns, either by making changes in A directly or by influencing A's own learning processes.\nEven though B may have no concept of what A's activities mean in relation to the outer world, it is still possible for B to be useful to A.\nThis is because a B-brain could learn to play a role somewhat like that of a counselor, psychologist, or management consultant, who can assess a client's mental strategy without having to understand all the details of that client's profession. Without having any idea of what A's goals are, B might be able to learn to tell when A is not accomplishing them but only going around in circles or wandering, confused because certain A-agents are repeating the same things over and over again. Then B might try some simple remedies, like suppressing some of those A-agents. To be sure, this could also result in B's activities becoming nuisances to A. For example, if A had the goal of adding up a long column of numbers, B might start to interfere with this because, from B's point of view, A appears to have become trapped in a repetitive loop. This could cause a person accustomed to more variety to find it difficult to concentrate on such a task and complain of being bored.\nTo the extent that the B-brain knows what is happening in A, the entire system could be considered to be partly self-aware. However, if we connect A and B to watch each other too closely, then anything could happen, and the entire system might become unstable. In any case,\nthere is no reason to stop with only two levels; we could connect a C-brain to watch the B-brain, and so on.",
    "type": "article",
    "title": "6.4 B-Brains",
    "tags": [
      {
        "score": 0.7576130032539368,
        "sentiment": 0,
        "count": 0,
        "label": "Brains",
        "uri": "https://diffbot.com/entity/XH5DMIFB_M_ut9sZgVG1gpw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6122217178344727,
        "sentiment": 0.709,
        "count": 1,
        "label": "What's Going On",
        "uri": "https://diffbot.com/entity/XsuJv6OJPNxumjrik0fQ8Fg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5811071395874023,
        "sentiment": 0,
        "count": 0,
        "label": "Oakland Athletics",
        "uri": "https://diffbot.com/entity/CdyNbfl6AOCaxkS30dWs5wQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/SportsTeam"
        ]
      }
    ],
    "docId": 172550128026,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 244439155079,
    "gburl": "http://aurellem.org/society-of-mind/som-6.4.html-diffbotxyz4122960507",
    "lastCrawlTimeUTC": 1588760028,
    "timestamp": "Wed, 06 May 2020 10:13:48 GMT"
  },
  {
    "date": "Sat, 26 Oct 2019 00:00:00 GMT",
    "sentiment": 0.939,
    "humanLanguage": "en",
    "estimatedDate": "Sat, 26 Oct 2019 00:00:00 GMT",
    "diffbotUri": "article|3|-643378430",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.10.html",
    "html": "<p>The vocabulary of a language &mdash; the words themselves &mdash; is the product of a project that spans the history of a culture and can involve millions of person years of work. Every sense of every word records some intellectual discovery that now outlives the myriad other, less distinguished thoughts that never earned a name.</p>\n<p>Each person invents some new ideas, but most of these will die when their owners do, except for those that make their way into the culture's lexicon. Still, from that ever-growing reservoir we each inherit many thousands of powerful ideas that all our predecessors found. Yet it is no paradox to say that even as we inherit those ideas from our culture, we each must reinvent them for ourselves. We cannot learn meanings only by memorizing definitions: we must also <em>understand</em> them. Each situation in which a word is used must suggest some mixture of materials already in the mind of a listener, who then, alone, must attempt to assemble these ingredients into something that will work in consonance with other things already learned. Definitions sometimes help &mdash; but still one must separate the essences from the accidents of the context, link together structures and functions, and build connections to the other things one knows.</p>\n<p>A word can only serve to indicate that someone else may have a valuable idea &mdash; that is, some useful structure to be built inside the mind. Each new word only plants a seed: to make it grow, a listener's mind must find a way to build inside itself some structure that appears to work like the one in the mind from which it was <em>learned.</em></p>\n<p>Along with the words, we also have to learn the grammar-tactics for using them. Most children start by using only one or two words at a time. Then, over the next two or three years, they learn to speak in sentences. It usually takes a full decade to learn most of the conventions of adult speech, but we often see relatively sudden advances over concentrated periods of time. How do children learn such complicated skills so quickly? Some language theorists have suggested that children learn to use grammar so readily that our brains must be born with built-in grammar-machinery. However, we've seen that our visual-systems solve many similar problems in even earlier years &mdash; and we've also seen that when they learn to play with spoons and pails, children must learn yet other languagelike skills for managing the Origins, Destinations, Recipients, and Instruments of their actions. Thus, many sections of our brains appear to demonstrate capacities for rearranging pronome roles even before we learn to speak. If so, perhaps we ought not to wonder so much about how children learn to speak so readily. Instead, we ought to wonder why it takes so long, when they already do so many similar things inside their heads.</p>",
    "text": "The vocabulary of a language \u2014 the words themselves \u2014 is the product of a project that spans the history of a culture and can involve millions of person years of work. Every sense of every word records some intellectual discovery that now outlives the myriad other, less distinguished thoughts that never earned a name.\nEach person invents some new ideas, but most of these will die when their owners do, except for those that make their way into the culture's lexicon. Still, from that ever-growing reservoir we each inherit many thousands of powerful ideas that all our predecessors found. Yet it is no paradox to say that even as we inherit those ideas from our culture, we each must reinvent them for ourselves. We cannot learn meanings only by memorizing definitions: we must also understand them. Each situation in which a word is used must suggest some mixture of materials already in the mind of a listener, who then, alone, must attempt to assemble these ingredients into something that will work in consonance with other things already learned. Definitions sometimes help \u2014 but still one must separate the essences from the accidents of the context, link together structures and functions, and build connections to the other things one knows.\nA word can only serve to indicate that someone else may have a valuable idea \u2014 that is, some useful structure to be built inside the mind. Each new word only plants a seed: to make it grow, a listener's mind must find a way to build inside itself some structure that appears to work like the one in the mind from which it was learned.\nAlong with the words, we also have to learn the grammar-tactics for using them. Most children start by using only one or two words at a time. Then, over the next two or three years, they learn to speak in sentences. It usually takes a full decade to learn most of the conventions of adult speech, but we often see relatively sudden advances over concentrated periods of time. How do children learn such complicated skills so quickly? Some language theorists have suggested that children learn to use grammar so readily that our brains must be born with built-in grammar-machinery. However, we've seen that our visual-systems solve many similar problems in even earlier years \u2014 and we've also seen that when they learn to play with spoons and pails, children must learn yet other languagelike skills for managing the Origins, Destinations, Recipients, and Instruments of their actions. Thus, many sections of our brains appear to demonstrate capacities for rearranging pronome roles even before we learn to speak. If so, perhaps we ought not to wonder so much about how children learn to speak so readily. Instead, we ought to wonder why it takes so long, when they already do so many similar things inside their heads.",
    "type": "article",
    "title": "26.10 learning language",
    "docId": 247731405194,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 253140697483,
    "gburl": "http://aurellem.org/society-of-mind/som-26.10.html-diffbotxyz3791273086",
    "lastCrawlTimeUTC": 1588760087,
    "timestamp": "Wed, 06 May 2020 10:14:47 GMT"
  },
  {
    "sentiment": -0.534,
    "images": [
      {
        "naturalHeight": 151,
        "width": 372,
        "diffbotUri": "image|3|-436746082",
        "url": "http://aurellem.org/society-of-mind/illus/ch24/24-2.png",
        "naturalWidth": 372,
        "primary": true,
        "height": 151
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|1011639008",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-24.3.html",
    "html": "<p>In order to be more concrete, let's make a little theory of how a frame might actually work. Consider, for example, a Trans-frame that is filled in to represent this sentence:</p>\n<p>Jack drove from Boston to New York on the turnpike with Mary.</p>\n<p>Whenever this particular frame is active, if you wonder about the Destination of that trip, you'll almost instantly think of New York. This suggests that the polyneme for New York must be aroused by the coincidence of two mental events, namely, the arousal of this particular travel-frame and the arousal of the pronome for Destination. Now how could a brain-agent recognize such a coincidence? Simple: we need only assume that the polyneme for New York is attached to an AND-agent with two inputs; one of them represents the arousal of the travel-frame itself, and the other represents the arousal of the Destination pronome. Accordingly, each terminal of our frame could simply be an AND-agent with two inputs.</p>\n<p>According to this simple scheme, a frame could consist of little more than a collection of AND-agents, one for each of the frame's pronome terminals! Then the entire frame for the New York trip would look like this:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch24/24-2.png\"/></figure>\n<p>When a frame-agent is activated &mdash; either by seeing, hearing, or imagining something &mdash; this supplies each of those AND-agents with one of these two inputs. The second input is provided by some pronome which can thereby activate whichever agent or frame is presently assigned to that terminal. If several pronomes are active at the same time, all the corresponding agents will be activated, too. When the frame above is active, the pronome for Origin will activate the K-line for Boston, and the pronome for Vehicle will activate the K-line for car.</p>\n<p>How could such a frame be made to learn which polynemes should fill its terminals? We could begin with each terminal initially connected to a virgin K-line; then each terminal will represent whatever the corresponding K-line learns. Notice that to build frames this way, we need only connect AND-agents to K-lines that can in turn be constructed from little more than simple AND-type agents. One of the great surprises of modern computer science was the discovery that so much can be done with so few kinds of ingredients.</p>",
    "text": "In order to be more concrete, let's make a little theory of how a frame might actually work. Consider, for example, a Trans-frame that is filled in to represent this sentence:\nJack drove from Boston to New York on the turnpike with Mary.\nWhenever this particular frame is active, if you wonder about the Destination of that trip, you'll almost instantly think of New York. This suggests that the polyneme for New York must be aroused by the coincidence of two mental events, namely, the arousal of this particular travel-frame and the arousal of the pronome for Destination. Now how could a brain-agent recognize such a coincidence? Simple: we need only assume that the polyneme for New York is attached to an AND-agent with two inputs; one of them represents the arousal of the travel-frame itself, and the other represents the arousal of the Destination pronome. Accordingly, each terminal of our frame could simply be an AND-agent with two inputs.\nAccording to this simple scheme, a frame could consist of little more than a collection of AND-agents, one for each of the frame's pronome terminals! Then the entire frame for the New York trip would look like this:\nWhen a frame-agent is activated \u2014 either by seeing, hearing, or imagining something \u2014 this supplies each of those AND-agents with one of these two inputs. The second input is provided by some pronome which can thereby activate whichever agent or frame is presently assigned to that terminal. If several pronomes are active at the same time, all the corresponding agents will be activated, too. When the frame above is active, the pronome for Origin will activate the K-line for Boston, and the pronome for Vehicle will activate the K-line for car.\nHow could such a frame be made to learn which polynemes should fill its terminals? We could begin with each terminal initially connected to a virgin K-line; then each terminal will represent whatever the corresponding K-line learns. Notice that to build frames this way, we need only connect AND-agents to K-lines that can in turn be constructed from little more than simple AND-type agents. One of the great surprises of modern computer science was the discovery that so much can be done with so few kinds of ingredients.",
    "type": "article",
    "title": "24.3 How trans-frames work",
    "tags": [
      {
        "score": 0.7772705554962158,
        "sentiment": 0,
        "count": 5,
        "label": "New York City",
        "uri": "https://diffbot.com/entity/AcMmgf99wMQ6XYnbChv5HiQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Settlement",
          "http://dbpedia.org/ontology/City",
          "http://dbpedia.org/ontology/Locality"
        ]
      },
      {
        "score": 0.6734621524810791,
        "sentiment": 0,
        "count": 2,
        "label": "Boston",
        "uri": "https://diffbot.com/entity/A7vnJ0j-OP4qXP4s9wNDbEQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Locality",
          "http://dbpedia.org/ontology/Settlement",
          "http://dbpedia.org/ontology/City"
        ]
      },
      {
        "score": 0.6649354696273804,
        "sentiment": 0,
        "count": 4,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      },
      {
        "score": 0.5406374335289001,
        "sentiment": 0,
        "count": 0,
        "label": "travel",
        "uri": "https://diffbot.com/entity/XV-0mON9CMja-roXKlqy3gw"
      },
      {
        "score": 0.5328863859176636,
        "sentiment": 0,
        "count": 1,
        "label": "Virgin Mary",
        "uri": "https://diffbot.com/entity/PLb8YmCyaN5GiLVpK2kokHw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      }
    ],
    "docId": 30607049118,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 192655442352,
    "gburl": "http://aurellem.org/society-of-mind/som-24.3.html-diffbotxyz448946672",
    "lastCrawlTimeUTC": 1588759878,
    "timestamp": "Wed, 06 May 2020 10:11:18 GMT"
  },
  {
    "sentiment": 0.629,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1054297983",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-11.5.html",
    "html": "<p>Our ways to think depend in part on how we're raised. But at the start, much more depends upon the wiring in our brains. How do those microscopic features work to influence what happens in our mental worlds? The answer is, our thoughts are largely shaped by which things seem most similar. Which colors seem the most alike? Which forms and shapes, which smells and tastes, which timbres, pitches, pains and aches, which feelings and sensations seem most similar? Such judgments have a huge effect at every stage of mental growth &mdash; since what we learn depends on how we classify.</p>\n<p>For example, a child who classified each fire just by the color of its light might learn to be afraid of everything of orange hue. Then we'd complain that the child had <em>generalized</em> too much. But if that child classified each flame, instead, by features that were never twice the same, that child would often be burned &mdash; and we'd complain that it hadn't generalized enough.</p>\n<p>Our genes supply our bodies with many kinds of sensors &mdash; external event-detecting agents &mdash; each of which sends signals to the nervous system when it detects certain physical conditions. We have sensory-agents in our eyes, ears, nose, and mouth that discern light, sound, odors, and tastes; we have agents in the skin that sense pressure, touch, vibration, heat, and cold; we have internal agents that sense tensions in our muscles, tendons, and ligaments; and we have many other sensors of which we're normally unaware, such as those that detect the direction of gravity and sense the amounts of various chemicals in different parts of the body.</p>\n<p>The agents that sense the colors of light in human eyes are much more complex than the <em>redness agents</em> of our toy machine. But this is not the reason that simple machine can't grasp what Redness means to us &mdash; for neither can the sense detectors in our human eyes. For just as there is nothing to say about a single point, there's nothing to be said about an isolated sensory signal. When our Redness, Touch, or Toothache agents send their signals to our brains, each by itself can only say, <em>I'm here.</em> The rest of what such signals <em>mean</em> to us depends on how they're linked to all our other agencies.</p>\n<p>In other words, the <em>qualities</em> of signals sent to brains depend only on relationships &mdash; the same as with the shapeless points of space. This is the problem Dr. Johnson faced when creating definitions for his dictionary: each separate word like <em>bitter,</em> <em>bright,</em> <em>salt,</em> or <em>sweet</em> attempts to speak about a quality of a sensory signal. But all that a separate signal can do is announce its own activity &mdash; perhaps with some expression of intensity. Your tooth can't ache (it can only send signals); only you can ache, once your higher-level agencies interpret those signals. Beyond the raw distinctiveness of every separate stimulus, all other aspects of its character or quality &mdash; be it of touch, taste, sound, or light &mdash; depend entirely on its relationships with the other agents of your mind.</p>",
    "text": "Our ways to think depend in part on how we're raised. But at the start, much more depends upon the wiring in our brains. How do those microscopic features work to influence what happens in our mental worlds? The answer is, our thoughts are largely shaped by which things seem most similar. Which colors seem the most alike? Which forms and shapes, which smells and tastes, which timbres, pitches, pains and aches, which feelings and sensations seem most similar? Such judgments have a huge effect at every stage of mental growth \u2014 since what we learn depends on how we classify.\nFor example, a child who classified each fire just by the color of its light might learn to be afraid of everything of orange hue. Then we'd complain that the child had generalized too much. But if that child classified each flame, instead, by features that were never twice the same, that child would often be burned \u2014 and we'd complain that it hadn't generalized enough.\nOur genes supply our bodies with many kinds of sensors \u2014 external event-detecting agents \u2014 each of which sends signals to the nervous system when it detects certain physical conditions. We have sensory-agents in our eyes, ears, nose, and mouth that discern light, sound, odors, and tastes; we have agents in the skin that sense pressure, touch, vibration, heat, and cold; we have internal agents that sense tensions in our muscles, tendons, and ligaments; and we have many other sensors of which we're normally unaware, such as those that detect the direction of gravity and sense the amounts of various chemicals in different parts of the body.\nThe agents that sense the colors of light in human eyes are much more complex than the redness agents of our toy machine. But this is not the reason that simple machine can't grasp what Redness means to us \u2014 for neither can the sense detectors in our human eyes. For just as there is nothing to say about a single point, there's nothing to be said about an isolated sensory signal. When our Redness, Touch, or Toothache agents send their signals to our brains, each by itself can only say, I'm here. The rest of what such signals mean to us depends on how they're linked to all our other agencies.\nIn other words, the qualities of signals sent to brains depend only on relationships \u2014 the same as with the shapeless points of space. This is the problem Dr. Johnson faced when creating definitions for his dictionary: each separate word like bitter, bright, salt, or sweet attempts to speak about a quality of a sensory signal. But all that a separate signal can do is announce its own activity \u2014 perhaps with some expression of intensity. Your tooth can't ache (it can only send signals); only you can ache, once your higher-level agencies interpret those signals. Beyond the raw distinctiveness of every separate stimulus, all other aspects of its character or quality \u2014 be it of touch, taste, sound, or light \u2014 depend entirely on its relationships with the other agents of your mind.",
    "type": "article",
    "title": "11.5 sensing similarities",
    "tags": [
      {
        "score": 0.5210007429122925,
        "sentiment": 0.372,
        "count": 3,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5185642838478088,
        "sentiment": -0.132,
        "count": 4,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 95630066071,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 208207511954,
    "gburl": "http://aurellem.org/society-of-mind/som-11.5.html-diffbotxyz1328843553",
    "lastCrawlTimeUTC": 1588759985,
    "timestamp": "Wed, 06 May 2020 10:13:05 GMT"
  },
  {
    "sentiment": 0.398,
    "images": [
      {
        "naturalHeight": 209,
        "width": 294,
        "diffbotUri": "image|3|1096107552",
        "url": "http://aurellem.org/society-of-mind/illus/ch20/20-4.png",
        "naturalWidth": 294,
        "primary": true,
        "height": 209
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-173175165",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-20.6.html",
    "html": "<p>Our polynemes and micronemes grow into great branching networks that reach every level of every agency. They approximate the general form of a hierarchy, but one that is riddled with shortcuts, cross- connections, and exceptions. No one could ever comprehend all the details of the connections that develop inside a single human individual; that would amount to grasping how all that person's thoughts and inclinations interact. At best, we can envision only the broadest outlines of such structures:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch20/20-4.png\"/></figure>\n<p>In regions near the agencies for speech, some elements of this network might signify or represent ideas and thoughts we can easily express in words. But because speaking is a social act, we are far less able to express the significance of the nemes involved with agencies that are not directly concerned with communication. This is because those agencies are less constrained by the discipline of public language; accordingly, the nemes inside those agencies can vary more from one person to the next.</p>\n<p>In any case, our higher-level agencies are generally unaware of what our lower-level agents do; they supervise and regulate &mdash; but scarcely comprehend at all &mdash; what happens among their subordinates. For example, a high-level agency might find that a certain subagency is unproductive, because it is responding to too many micronemes &mdash; or to too few &mdash; and adjust its sensitivity accordingly. Like a B-brain, a controlling agency could make such judgments without comprehending the local meanings of those micronemes. This could also provide a basis for controlling the levels of the activities of those other agencies &mdash; just as we suggested when we discussed the idea of a <em>spiral</em> computation within an agency and its K-line trees. When the work appears to be going well, the supervisor can direct the lower-level processes to <em>spiral down</em> toward small details. But when there seem to be too many obstacles, the level of activity would be made to <em>spiral up</em> instead, to levels capable of diagnosing and altering an ineffective plan.</p>",
    "text": "Our polynemes and micronemes grow into great branching networks that reach every level of every agency. They approximate the general form of a hierarchy, but one that is riddled with shortcuts, cross- connections, and exceptions. No one could ever comprehend all the details of the connections that develop inside a single human individual; that would amount to grasping how all that person's thoughts and inclinations interact. At best, we can envision only the broadest outlines of such structures:\nIn regions near the agencies for speech, some elements of this network might signify or represent ideas and thoughts we can easily express in words. But because speaking is a social act, we are far less able to express the significance of the nemes involved with agencies that are not directly concerned with communication. This is because those agencies are less constrained by the discipline of public language; accordingly, the nemes inside those agencies can vary more from one person to the next.\nIn any case, our higher-level agencies are generally unaware of what our lower-level agents do; they supervise and regulate \u2014 but scarcely comprehend at all \u2014 what happens among their subordinates. For example, a high-level agency might find that a certain subagency is unproductive, because it is responding to too many micronemes \u2014 or to too few \u2014 and adjust its sensitivity accordingly. Like a B-brain, a controlling agency could make such judgments without comprehending the local meanings of those micronemes. This could also provide a basis for controlling the levels of the activities of those other agencies \u2014 just as we suggested when we discussed the idea of a spiral computation within an agency and its K-line trees. When the work appears to be going well, the supervisor can direct the lower-level processes to spiral down toward small details. But when there seem to be too many obstacles, the level of activity would be made to spiral up instead, to levels capable of diagnosing and altering an ineffective plan.",
    "type": "article",
    "title": "20.6 the nemeic spiral",
    "docId": 177908302208,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 92228321681,
    "gburl": "http://aurellem.org/society-of-mind/som-20.6.html-diffbotxyz2303398346",
    "lastCrawlTimeUTC": 1588759945,
    "timestamp": "Wed, 06 May 2020 10:12:25 GMT"
  },
  {
    "sentiment": 0.211,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-146122593",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-22.6.html",
    "html": "<p>Language lets us treat our thoughts as though they were much like ordinary things. Suppose you meet someone who is trying to solve a problem. You ask what's happening. <em>I'm thinking,</em> you are told. <em>I can see that,</em> you say, <em>but what are you thinking about?</em> <em>Well, I was looking for a way to solve this problem, and I think I've just found one.</em> We speak as though ideas resemble building-blocks that one can find and grasp!</p>\n<p>Why do we <em>thing-ify</em> our thoughts? One reason is that this enables us to reapply the wonderful machines our brains contain for understanding worldly things. Another thing it does is help us organize our expeditions in the mental world, much as we find our ways through space. Consider how the strategies we use to <em>find</em> ideas resemble the strategies we use for finding real things: Look in the places they used to be or where they're usually found &mdash; but don't keep looking again and again in the same place. Indeed, for many centuries our memory-training arts have been dominated by two techniques. One is based on similarities of sounds, exploiting the capacities of our language-agencies to make connections between words. The other method is based on imagining the items we want to remember as placed in some familiar space, such as a road or room one knows particularly well. This way, we can apply our thing-location skills to keeping track of our ideas.</p>\n<p>Our ability to treat ideas as though they were objects goes together with our abilities to reuse our brain-machinery over and over again. Whenever an agency becomes overburdened by a large and complicated structure, we may be able to treat that structure as a simple, single unit by thing-ifying &mdash; or, as we usually say, <em>conceptualizing</em> &mdash; it. Then, once we replace a larger structure by representing it with a compact symbol-sign, that overloaded agency may be able to continue its work. This way, we can build grand structures of ideas &mdash; much as we can build great towers from smaller parts.</p>\n<p>I suspect that, as they're represented in the mind, there's little difference between a physical object and an idea. Worldly things are useful to us because they are <em>substantial</em> &mdash; that is, because their properties are relatively permanent. Now we don't usually think of ideas as substantial, because they don't have the usual properties of worldly things &mdash; such as color, shape, and weight. Yet <em>good ideas</em> must also have substantiality, albeit of a different sort:</p>\n<p>No conception or idea could have much use unless it could remain unchanged &mdash; and stay in some kind of mental <em>place</em> &mdash; for long enough for us to find it when we need it. Nor could we ever achieve a goal unless it could persist for long enough. In short, no mind can work without some stable states or memories.</p>\n<p>This may sound as though I'm speaking metaphorically, since a mental <em>place</em> is not exactly like a worldly place. But then, when you think of a place you know, that thought itself is not a worldly place, but only a linkage of memories and processes inside your mind. This wonderful capacity &mdash; to think about thoughts as though they were things &mdash; is also what enables us to contemplate the products of our thoughts. Without that ability to reflect, we would have no general intelligence &mdash; however large our repertoire of special-purpose skills might grow. Of course this same capacity enables us to think such empty thoughts as <em>This statement is about itself,</em> which is true but useless, or <em>This statement is not about itself,</em> which is false and useless, or <em>This statement is false,</em> which is downright paradoxical. Yet the benefit of being able to conceptualize is surely worth the risk that we may sometimes be nonsensical.</p>",
    "text": "Language lets us treat our thoughts as though they were much like ordinary things. Suppose you meet someone who is trying to solve a problem. You ask what's happening. I'm thinking, you are told. I can see that, you say, but what are you thinking about? Well, I was looking for a way to solve this problem, and I think I've just found one. We speak as though ideas resemble building-blocks that one can find and grasp!\nWhy do we thing-ify our thoughts? One reason is that this enables us to reapply the wonderful machines our brains contain for understanding worldly things. Another thing it does is help us organize our expeditions in the mental world, much as we find our ways through space. Consider how the strategies we use to find ideas resemble the strategies we use for finding real things: Look in the places they used to be or where they're usually found \u2014 but don't keep looking again and again in the same place. Indeed, for many centuries our memory-training arts have been dominated by two techniques. One is based on similarities of sounds, exploiting the capacities of our language-agencies to make connections between words. The other method is based on imagining the items we want to remember as placed in some familiar space, such as a road or room one knows particularly well. This way, we can apply our thing-location skills to keeping track of our ideas.\nOur ability to treat ideas as though they were objects goes together with our abilities to reuse our brain-machinery over and over again. Whenever an agency becomes overburdened by a large and complicated structure, we may be able to treat that structure as a simple, single unit by thing-ifying \u2014 or, as we usually say, conceptualizing \u2014 it. Then, once we replace a larger structure by representing it with a compact symbol-sign, that overloaded agency may be able to continue its work. This way, we can build grand structures of ideas \u2014 much as we can build great towers from smaller parts.\nI suspect that, as they're represented in the mind, there's little difference between a physical object and an idea. Worldly things are useful to us because they are substantial \u2014 that is, because their properties are relatively permanent. Now we don't usually think of ideas as substantial, because they don't have the usual properties of worldly things \u2014 such as color, shape, and weight. Yet good ideas must also have substantiality, albeit of a different sort:\nNo conception or idea could have much use unless it could remain unchanged \u2014 and stay in some kind of mental place \u2014 for long enough for us to find it when we need it. Nor could we ever achieve a goal unless it could persist for long enough. In short, no mind can work without some stable states or memories.\nThis may sound as though I'm speaking metaphorically, since a mental place is not exactly like a worldly place. But then, when you think of a place you know, that thought itself is not a worldly place, but only a linkage of memories and processes inside your mind. This wonderful capacity \u2014 to think about thoughts as though they were things \u2014 is also what enables us to contemplate the products of our thoughts. Without that ability to reflect, we would have no general intelligence \u2014 however large our repertoire of special-purpose skills might grow. Of course this same capacity enables us to think such empty thoughts as This statement is about itself, which is true but useless, or This statement is not about itself, which is false and useless, or This statement is false, which is downright paradoxical. Yet the benefit of being able to conceptualize is surely worth the risk that we may sometimes be nonsensical.",
    "type": "article",
    "title": "22.6 expression",
    "tags": [
      {
        "score": 0.5944219827651978,
        "sentiment": 0.314,
        "count": 1,
        "label": "What's Going On",
        "uri": "https://diffbot.com/entity/XsuJv6OJPNxumjrik0fQ8Fg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      },
      {
        "score": 0.5779368281364441,
        "sentiment": -0.231,
        "count": 1,
        "label": "Ordinary Things",
        "uri": "https://diffbot.com/entity/XjgkCTHEWOMKyJombCZ59XA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.5492838025093079,
        "sentiment": 0,
        "count": 1,
        "label": "language",
        "uri": "https://diffbot.com/entity/XZLUv8RZWNw62dTgKTTa7JQ"
      }
    ],
    "docId": 253846159786,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 54717170055,
    "gburl": "http://aurellem.org/society-of-mind/som-22.6.html-diffbotxyz316312048",
    "lastCrawlTimeUTC": 1588759908,
    "timestamp": "Wed, 06 May 2020 10:11:48 GMT"
  },
  {
    "sentiment": 0.823,
    "humanLanguage": "en",
    "diffbotUri": "article|3|138653241",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-15.6.html",
    "html": "<p>We often talk of &ldquo;memory&rdquo; as though it were a single definite thing. But everyone has many kinds of memories. Some things we know seem totally detached from time, like such facts as that twelve inches make a foot or that a bull has dangerous horns. Other things we know seem linked to definite spans of time or space, like memories of places where we've lived. Still other recollections seem like souvenirs of episodes we can reexperience: <em>Once, when visiting my grand- parents, I climbed an old apple tree.</em></p>\n<p>A brain has no single, common memory system. Instead, each part of the brain has several types of memory-agencies that work in somewhat different ways, to suit particular purposes.</p>\n<p>Why do we have so many kinds of memory? If memories are records of our mental states of earlier times, how are those records stored and kept? A popular image of memories is that they are like objects we store away in various <em>places</em> in the brain. But then what are those places like? How do memories get into them and come out again? And what takes place inside of the vaults in which they're stored? Are memory banks like freezer chests where time stands still, or do their contents interact? How long can our old memories remain; do some of them grow old and die, do they get weak and fade away or just get lost and never found?</p>\n<p>We have the impression that even our long-term memories become harder to recall as time goes on, and that might lead us to suppose that they have some inherent tendency to fade away. But even that is uncertain; it could simply be because so many other memories begin to interfere with them. Most likely, some types of memory mechanisms retain the records of sensations only for seconds; we use others to adopt habits, goals, and styles that we hold only for days or weeks; and we make personal attachments that endure through many months or years. Yet suddenly, from time to time, we'll modify some memories that seemed, till then, quite permanent.</p>\n<p>More evidence that there are many kinds of memory has come from accidental injuries to brains. One injury may cause the loss of abilities to deal with names; another injury can make you lose some capacity to recognize faces or to remember musical tunes; still other kinds of injuries can leave unchanged what you have learned in earlier times but keep you from learning anything more in some particular domain. There is evidence that long-term memories can never form at all unless their short-term antecedents are permitted to persist for certain intervals; this process can also be blocked by various drugs and injuries, and this is why some people can never recollect what happened in the minutes before a brain concussion.</p>\n<p>Finally, it appears that there are strong limitations on how rapidly we can construct our long-term memories. Despite all the legends about prodigies, there seems to be no evidence from any well-designed experiments that any human being can continue to construct long-term memories, over any substantial interval of time, more than two or three times faster than the average person.</p>",
    "text": "We often talk of \u201cmemory\u201d as though it were a single definite thing. But everyone has many kinds of memories. Some things we know seem totally detached from time, like such facts as that twelve inches make a foot or that a bull has dangerous horns. Other things we know seem linked to definite spans of time or space, like memories of places where we've lived. Still other recollections seem like souvenirs of episodes we can reexperience: Once, when visiting my grand- parents, I climbed an old apple tree.\nA brain has no single, common memory system. Instead, each part of the brain has several types of memory-agencies that work in somewhat different ways, to suit particular purposes.\nWhy do we have so many kinds of memory? If memories are records of our mental states of earlier times, how are those records stored and kept? A popular image of memories is that they are like objects we store away in various places in the brain. But then what are those places like? How do memories get into them and come out again? And what takes place inside of the vaults in which they're stored? Are memory banks like freezer chests where time stands still, or do their contents interact? How long can our old memories remain; do some of them grow old and die, do they get weak and fade away or just get lost and never found?\nWe have the impression that even our long-term memories become harder to recall as time goes on, and that might lead us to suppose that they have some inherent tendency to fade away. But even that is uncertain; it could simply be because so many other memories begin to interfere with them. Most likely, some types of memory mechanisms retain the records of sensations only for seconds; we use others to adopt habits, goals, and styles that we hold only for days or weeks; and we make personal attachments that endure through many months or years. Yet suddenly, from time to time, we'll modify some memories that seemed, till then, quite permanent.\nMore evidence that there are many kinds of memory has come from accidental injuries to brains. One injury may cause the loss of abilities to deal with names; another injury can make you lose some capacity to recognize faces or to remember musical tunes; still other kinds of injuries can leave unchanged what you have learned in earlier times but keep you from learning anything more in some particular domain. There is evidence that long-term memories can never form at all unless their short-term antecedents are permitted to persist for certain intervals; this process can also be blocked by various drugs and injuries, and this is why some people can never recollect what happened in the minutes before a brain concussion.\nFinally, it appears that there are strong limitations on how rapidly we can construct our long-term memories. Despite all the legends about prodigies, there seems to be no evidence from any well-designed experiments that any human being can continue to construct long-term memories, over any substantial interval of time, more than two or three times faster than the average person.",
    "type": "article",
    "title": "15.6 many kinds of memory",
    "tags": [
      {
        "score": 0.8588467240333557,
        "sentiment": 0.126,
        "count": 7,
        "label": "computer memory",
        "uri": "https://diffbot.com/entity/XWsavN-0wOLquQDMlQ-MDYw"
      }
    ],
    "docId": 60440412552,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 116970455440,
    "gburl": "http://aurellem.org/society-of-mind/som-15.6.html-diffbotxyz4213987460",
    "lastCrawlTimeUTC": 1588759836,
    "timestamp": "Wed, 06 May 2020 10:10:36 GMT"
  },
  {
    "sentiment": 0.997,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-699859550",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-18.html",
    "html": "<blockquote> Machines&mdash;with their irrefutable logic, their cold preciseness of figures, their tireless, utterly exact observation, their absolute knowledge of mathematics&mdash;they could elaborate any idea, however simple its beginning, and reach the conclusion. From any three facts they even then could have built in mind all the Universe. Machines had imagination of the ideal sort. They had the ability to construct a necessary future result from a present fact. But Man had imagination of a different kind, theirs was the illogical, brilliant imagination that sees the future result vaguely, without knowing the why, nor the how, and imagination that outstrips the machine in its preciseness. Man might reach the conclusion more swiftly, but the machine always reached the conclusion eventually, and it was always the correct conclusion. By leaps and bounds man advanced. By steady, irresistible steps the machine marched forward. &mdash;John W. Campbell, Jr.</blockquote>",
    "text": "Machines\u2014with their irrefutable logic, their cold preciseness of figures, their tireless, utterly exact observation, their absolute knowledge of mathematics\u2014they could elaborate any idea, however simple its beginning, and reach the conclusion. From any three facts they even then could have built in mind all the Universe. Machines had imagination of the ideal sort. They had the ability to construct a necessary future result from a present fact. But Man had imagination of a different kind, theirs was the illogical, brilliant imagination that sees the future result vaguely, without knowing the why, nor the how, and imagination that outstrips the machine in its preciseness. Man might reach the conclusion more swiftly, but the machine always reached the conclusion eventually, and it was always the correct conclusion. By leaps and bounds man advanced. By steady, irresistible steps the machine marched forward. \u2014John W. Campbell, Jr.",
    "type": "article",
    "title": "18 reasoning",
    "tags": [
      {
        "score": 0.6477497816085815,
        "sentiment": 0.53,
        "count": 1,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      },
      {
        "score": 0.5080994963645935,
        "sentiment": 0.317,
        "count": 1,
        "label": "Orphans of the Sky",
        "uri": "https://diffbot.com/entity/XbwHQaCvTO3Kc2TSdq9RBcQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      }
    ],
    "docId": 61366780335,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 107474649512,
    "gburl": "http://aurellem.org/society-of-mind/som-18.html-diffbotxyz2392918429",
    "lastCrawlTimeUTC": 1588759854,
    "timestamp": "Wed, 06 May 2020 10:10:54 GMT"
  },
  {
    "date": "Sun, 17 Nov 2019 00:00:00 GMT",
    "sentiment": 0.714,
    "humanLanguage": "en",
    "estimatedDate": "Sun, 17 Nov 2019 00:00:00 GMT",
    "diffbotUri": "article|3|13232427",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.11.html",
    "html": "<p>How do we deal with thoughts that lead to frightening results? What should one think about the <em>nearly</em> paradox that threatens to imply that all things, large and small, might be the same size? One strategy would be to constrain that kind of reasoning, by learning never to chain together more than two or three such nearness links. Then,</p>\n<p>perhaps, one might proceed to generalize that strategy, in fear that it's unsafe to chain together too many instances of any form of inference.</p>\n<p>But what could the phrase <em>too many</em> mean? There is no universal answer. Just as in the case of More, we have to learn this separately in each important realm of thought: what are the limitations of each type and style of reasoning? Human thought is not based on any single and uniform kind of <em>logic,</em> but upon myriad processes, scripts, stereotypes, critics and censors, analogies and metaphors. Some are acquired through the operation of our genes, others are learned from our environments, and yet others we construct for ourselves. But even inside the mind, no one really learns alone, since every step employs many things we've learned before, from language, family, and friends &mdash; as well as from our former Selves. Without each stage to teach the next, no one could construct anything as complex as a mind.</p>\n<p>There is another way our intellectual growth is not so different from our emotional development: we can make intellectual attachments, too, and want to think the way certain other persons do. These intellectual ideals may stem from parents, teachers, and friends; from persons one has never met, such as writers; even from legendary heroes who did not exist. I suspect we depend as much on images of how we ought to think as we do on images of how we ought to feel. Some of our most persistent memories are about certain teachers, but not about what was taught. (At the moment I'm writing this, I feel as though my hero Warren McCulloch were watching disapprovingly; he would not have liked these neo-Freudian ideas.) No matter how emotionally neutral an enterprise may seem, there's no such thing as being <em>purely rational.</em> One must always approach each situation with some personal style and disposition. Even scientists have to make stylistic choices:</p>\n<p>Is there enough evidence yet, or should I seek more? Is it time to make a uniframe &mdash; or should I accumulate more examples? Can I rely on older theories here, or should I trust my latest guess? Should I try to be Reductionist or Novelist?</p>\n<p>At every step, the choices we make depend on what we have become.</p>\n<p>Our sciences, arts, and moral skills do not originate from detached ideals of truth, beauty, or virtue but stem partly from our endeavors to placate or please the images established in earlier years. Our adult dispositions thus evolve from impulses so infantile that we would surely censure them, if they were not by now transformed, disguised, or &mdash; as Freud said &mdash; <em>sublimated.</em></p>",
    "text": "How do we deal with thoughts that lead to frightening results? What should one think about the nearly paradox that threatens to imply that all things, large and small, might be the same size? One strategy would be to constrain that kind of reasoning, by learning never to chain together more than two or three such nearness links. Then,\nperhaps, one might proceed to generalize that strategy, in fear that it's unsafe to chain together too many instances of any form of inference.\nBut what could the phrase too many mean? There is no universal answer. Just as in the case of More, we have to learn this separately in each important realm of thought: what are the limitations of each type and style of reasoning? Human thought is not based on any single and uniform kind of logic, but upon myriad processes, scripts, stereotypes, critics and censors, analogies and metaphors. Some are acquired through the operation of our genes, others are learned from our environments, and yet others we construct for ourselves. But even inside the mind, no one really learns alone, since every step employs many things we've learned before, from language, family, and friends \u2014 as well as from our former Selves. Without each stage to teach the next, no one could construct anything as complex as a mind.\nThere is another way our intellectual growth is not so different from our emotional development: we can make intellectual attachments, too, and want to think the way certain other persons do. These intellectual ideals may stem from parents, teachers, and friends; from persons one has never met, such as writers; even from legendary heroes who did not exist. I suspect we depend as much on images of how we ought to think as we do on images of how we ought to feel. Some of our most persistent memories are about certain teachers, but not about what was taught. (At the moment I'm writing this, I feel as though my hero Warren McCulloch were watching disapprovingly; he would not have liked these neo-Freudian ideas.) No matter how emotionally neutral an enterprise may seem, there's no such thing as being purely rational. One must always approach each situation with some personal style and disposition. Even scientists have to make stylistic choices:\nIs there enough evidence yet, or should I seek more? Is it time to make a uniframe \u2014 or should I accumulate more examples? Can I rely on older theories here, or should I trust my latest guess? Should I try to be Reductionist or Novelist?\nAt every step, the choices we make depend on what we have become.\nOur sciences, arts, and moral skills do not originate from detached ideals of truth, beauty, or virtue but stem partly from our endeavors to placate or please the images established in earlier years. Our adult dispositions thus evolve from impulses so infantile that we would surely censure them, if they were not by now transformed, disguised, or \u2014 as Freud said \u2014 sublimated.",
    "type": "article",
    "title": "17.11 intellectual ideals",
    "tags": [
      {
        "score": 0.7454077005386353,
        "sentiment": 0.877,
        "count": 4,
        "label": "intellectual",
        "uri": "https://diffbot.com/entity/XProv2_W5PX6_d6ql63ksnQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 64643432856,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 102862553517,
    "gburl": "http://aurellem.org/society-of-mind/som-17.11.html-diffbotxyz4243668330",
    "lastCrawlTimeUTC": 1588759800,
    "timestamp": "Wed, 06 May 2020 10:10:00 GMT"
  },
  {
    "sentiment": -0.277,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1917389671",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-glossary.html",
    "html": "<p>Because I thought this theory of the mind might interest not only specialists but everyone who thinks, I favored ordinary words over the technical language of psychology. This was rarely any sacrifice because so many psychological terms already stood for obsolete ideas. But since I also wished to speak to specialists, I tried to hide more technical ideas between the lines; I hope this second level does not show. However there still were certain points at which no ordinary words seemed satisfactory, and I had to invent new terms or assign new meanings to old ones.</p>\n<h3>Accumulation</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-12.6.html\">12.6</a>) A type of learning based on collecting examples of an idea without attempting to describe what they have in common. Contrast with <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Uniframe</a>.</p>\n<h3>Agency</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-1.6.html\">1.6</a>) Any assembly of parts considered in terms of what it can accomplish as a unit, without regard to what each of its parts does by itself.</p>\n<h3>Agent</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-1.4.html\">1.4</a>) Any part or process of the mind that by itself is simple enough to understand &mdash; even though the interactions among groups of such agents may produce phenomena that are much harder to understand.</p>\n<h3>Artificial Intelligence</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.4.html\">7.4</a>) The field of research concerned with making machines do things that people consider to require intelligence. There is no clear boundary between psychology and Artificial Intelligence because the brain itself is a kind of machine. For an introduction to this field, I recommend Patrick Winston's textbook Artificial Intelligence, Addison-Wesley, 1984. For more connections with psychology, see Roger Schank and Kenneth Colby (eds.), Computer Models of Thought and Language, Freeman, 1973. Some influential early ideas about brains and machines can be seen in Warren McCulloch's Embodiments of Mind, MIT Press, Cambridge, Mass., 1966. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Intelligence</a>.</p>\n<h3>Attachment Learning</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-17.2.html\">17.2</a>) The specific theory, proposed in this book, that the presence of someone to whom we are emotionally attached has a special effect on how we learn, especially in infancy. Attachment learning tends to cause us to modify our goals &mdash; rather than merely improve our methods for achieving the goals we already have.</p>\n<h3>B-Brain</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-6.4.html\">6.4</a>) Any part of the brain connected not to the outside world, but only to another part of the same brain. Like a manager, a B-brain can supervise an A-brain without understanding either how the A-brain works or the problems with which the A-brain is involved &mdash; for example, by recognizing patterns of activity that indicate the A-brain is confused, wasting time in repetitive activity, or focused on an unproductive level of detail.</p>\n<h3>Block-Arch</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-12.1.html\">12.1</a>) A scenario adapted from Patrick Winston's doctoral thesis, <em>Learning Structural Descriptions by Examples,</em> in <i>The Psychology of Computer Vision</i>, P. H. Winston (ed.), McGraw-Hill, 1975. The study of the world of children's building-blocks may at first seem childishly simple, but it has been one of the most productive areas of research about Artificial Intelligence, child psychology, and modern robotics engineering.</p>\n<h3>Censor</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-27.2.html\">27.2</a>) An agent that inhibits or suppresses the operation of other agents. Censorlike agents are involved with how we learn from our mistakes. This idea played a prominent role in Freud's theories but has been virtually ignored by modern experimental psychologists &mdash; presumably because it is hard to study what people do not think. See Freud's 1905 book Jokes and Their Relation to the Unconscious. I suspect censorlike agents may constitute the larger part of human memory. The discussion of censors and jokes in chapter 27 is based on my essay <em>Jokes and Their Relation to the Cognitive Unconscious,</em> published in Cognitive Constraints on Communication, Representations and Processes, L. Vaina and J.K.K. Hintikka (eds.), . Reidel, 1981. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Suppressors</a>.</p>\n<h3>Challenger, Professor</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-4.4.html\">4.4</a>) A rival of mine, disguised as the treacherous archaeologist in Arthur Conan Doyle's novel The Lost World, who resembles Sherlock Holmes's nemesis, the mathematician Moriarty, except for being somewhat more honorable.</p>\n<h3>Closing the Ring</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-19.10.html\">19.10</a>) A technique by which an agency can recall many details of a memory from being given only a few <em>cues.</em></p>\n<h3>Common Sense</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-1.5.html\">1.5</a>) The mental skills that most people share. Commonsense thinking is actually more complex than many of the intellectual accomplishments that attract more attention and respect, because the mental skills we call <em>expertise</em> often engage large amounts of knowledge but usually employ only a few types of representations. In contrast, common sense involves many different kinds of representations and thus requires a larger range of different skills.</p>\n<h3>Computer Science</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-6.8.html\">6.8</a>) A science still in its infancy. While other sciences study how particular types of objects interact, computer science studies how interactions work in general &mdash; that is, how societies of parts can accomplish what those parts cannot do separately. Although computer science began with the study of serial computers &mdash; that is, of machines that could do only one thing at a time &mdash; it has grown to the point of studying the sorts of interconnected networks of processes that must go on inside societies of mind. (For an introduction to the theory of single-process machines, see my book <i>Computation: Finite and Infinite Machines</i>, Prentice-Hall, 1967.)</p>\n<h3>Consciousness</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-6.1.html\">6.1</a>) In this book, the word is used mainly for the myth that human minds are <em>self-aware</em> in the sense of perceiving what happens inside themselves. I maintain that human consciousness can never represent what is occurring at the present moment, but only a little of the recent past &mdash; partly because each agency has a limited capacity to represent what happened recently and partly because it takes time for agencies to communicate with one another. Consciousness is peculiarly hard to describe because each attempt to examine temporary memories distorts the very records it is trying to inspect. The description of consciousness in section <a href=\"http://aurellem.org/society-of-mind/som-6.1.html\">6.1</a> was adapted from my epilogue to Vernor Vinge's novel <i>True Names</i>, Bluejay Books, New York, 1984.</p>\n<h3>Context</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-20.2.html\">20.2</a>) The effect upon one's state of mind of all the influences present at the time. At each moment, the context within which each agency works is determined by the activity of the nemes that reach that agency. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Neme</a>.</p>\n<h3>Cross-Exclusion</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-16.4.html\">16.4</a>) An arrangement in which each of several agents is connected so as to inhibit all the others &mdash; so that only one of them can remain active at a time.</p>\n<h3>Cross-Realm Correspondence</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-29.4.html\">29.4</a>) A structure that has useful applications in two or more different mental realms. Such correspondences sometimes enable us to transfer knowledge and skill from one domain to another &mdash; without needing to accumulate experience in that other realm. This is the basis of certain important kinds of analogies and metaphors.</p>\n<h3>Creativity</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.10.html\">7.10</a>) The myth that the production of novel ideas, artistic or otherwise, comes from some distinctive form of thought. I recommend the discussion of this subject in the chapter <em>Variations on a Theme as the Crux of Creativity,</em> in Douglas Hofstadter's <i>Metamagical Themas</i>, Basic Books, 1985.</p>\n<h3>Default Assumption</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-8.5.html\">8.5</a>, <a href=\"http://aurellem.org/society-of-mind/som-12.12.html\">12.12</a>) The kind of assumption we make when we lack reasons to think otherwise. For example, we assume <em>by default</em> that an unfamiliar individual who belongs to a familiar class will think and act like a <em>typical</em> member of that class. Default assumptions are more than mere conveniences; they constitute our most productive way to make generalizations. Although such assumptions are frequently wrong, they usually do little harm because they are automatically displaced when more specific information becomes available. However, they can do incalculable harm when they are held too rigidly.</p>\n<h3>Demon</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-27.1.html\">27.1</a>) An agent that constantly watches for a certain condition and intervenes when it occurs. Our discussion of demons is partly based on Eugene Charniak's doctoral thesis, <em>Toward a Model of Children's Story Comprehension,</em> MIT, 1972.</p>\n<h3>Difference-Engine</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.8.html\">7.8</a>) An agency whose actions tend to make the present state of affairs more like some goal or <em>desired state</em> whose description is represented in that agency. This idea was developed by Allen Newell, C. J. Shaw, and Herbert A. Simon into an important theory about human problem solving. See G. Ernst and Allen Newell, <i>GPS, A Case Study in Generality and Problem Solving</i>, Academic Press, 1969.</p>\n<h3>Direction-Neme</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-24.6.html\">24.6</a>) An agent associated with a particular direction or region in space. I suspect that bundles of direction-nemes are used inside our brains for representing not only spatial locations and directions, but also for representing many nonspatial concepts. Direction-nemes resemble isonomes in spatial realms but more resemble polynemes in other realms. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Interaction-Square</a> and <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Frame-Array</a>.</p>\n<h3>Distributed Memory</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-20.9.html\">20.9</a>) A representation in which each fragment of information is stored, not by making a single, substantial change in one agent, but by making small changes in many different agents. Many theorists have been led to believe that the construction of distributed memory-systems must involve <em>nondigital</em> devices such as holograms; that this is not so was shown by P. J. Willshaw, 0. P. Buneman, and H. C. Longuet-Higgins in <em>Non-Holographic Associative Memory,</em> Nature, 222, 1969. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Memorizers</a>.</p>\n<h3>Duplication Problem</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-23.2.html\">23.2</a>) The question of how a mind could compare two similar ideas without possessing two identical agencies for representing both of them at the same time. This problem was never recognized in older theories of psychology, and I suspect it will be the downfall of most <em>holistic</em> theories of higher-level thought. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Time Blinking</a>.</p>\n<h3>Emotion</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-16.1.html\">16.1</a>) A term used for too many different purposes. There is a popular view that emotions are inherently more complex and harder to understand than other aspects of human thought. I maintain that infantile emotions are comparatively simple in character and that the complexity of adult emotions results from accumulating networks of mutual exploitations. In adults, these networks eventually become indescribably complicated, but no more so than the networks of our adult <em>intellectual</em> structures. Beyond a certain point, to distinguish between the emotional and intellectual structures of an adult is merely to describe the same structures from different points of view. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Proto-specialist</a>.</p>\n<h3>Exploitation</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-4.5.html\">4.5</a>) The act of one agency making use of the activity of another agency, without understanding how it works. Exploitation is the most usual relationship among agencies because it is so difficult for them to understand one another.</p>\n<h3>Exception Principle</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-12.9.html\">12.9</a>) The concept that it may not pay to change a well-established skill in order to accommodate an exception. The more one builds upon a certain foundation, the greater the disruption upon changing it. A system's growth will tend to cease, past the point at which the damage caused by any change outweighs the immediate gain. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Investment Principle</a>.</p>\n<h3>Frame</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-24.2.html\">24.2</a>) A representation based on a set of terminals to which other structures can be attached. Normally, each terminal is connected to a default assumption, which is easily displaced by more specific information. Other ideas about frames that are not discussed within this book were published in my chapter <em>A Framework for Representing Knowledge,</em> in Psychology of Computer Vision, P. H. Winston (ed.), McGraw-Hill, 1975. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Picture-Frames</a>, <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Trans-frame</a>.</p>\n<h3>Frame-Array</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-25.2.html\">25.2</a>) A family of frames that share the same terminals. Information attached to any terminal of a frame-array automatically becomes available to all the frames of that array. This makes it easy to change perspective, not only in regard to a physical viewpoint, but in other mental realms as well. Frame-arrays are often controlled by bundles of direction-nemes.</p>\n<h3>Functional Autonomy</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-17.4.html\">17.4</a>) The idea that specific goals can lead to subgoals of broader character. For example, in order to please another individual, a child might develop more general goals of acquiring knowledge, power, or wealth &mdash; yet the very same subgoals might serve equally well an initial wish to injure that other individual. The term <em>functional autonomy</em> derives from Gordon Allport, who was one of my professors at Harvard.</p>\n<h3>Functional Definition</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-12.4.html\">12.4</a>) Specifying something in terms of how it might be used, rather than in terms of its parts and their relationships. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Structural Definition</a>.</p>\n<h3>Generate and Test</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.3.html\">7.3</a>) Solving a problem by trial and error &mdash; that is, by proposing solutions recklessly, then rejecting those that do not work.</p>\n<h3>Genius</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.10.html\">7.10</a>) An individual of prodigious mental accomplishment. Although even the most outstanding human prodigies rarely develop even twice as quickly as their peers, many people feel that their existence demands a special explanation. I suspect that the answer is to be found not in the superficial skills such people learn, but in the early accidents that lead them into learning better ways to learn.</p>\n<h3>Goal</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.8.html\">7.8</a>) The representation in a difference-engine of an imagined final state of affairs. This definition of goal may at first seem too impersonal because it does not explain either the elation that comes with achieving a human goal or the frustration that accompanies failure. However, we should not expect to explain such complicated phenomena of adult psychology directly in terms of simple principles, since they also depend on many other aspects of our mental architecture. Basing our concept of goal on the difference-engine idea helps us to avoid the single-agent fallacy by permitting us to speak about a goal without needing to refer to the person who entertains that goal; a person's many agencies may each have different goals &mdash; without that person being <em>aware</em> of them.</p>\n<h3>Grammar-Tactic</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-22.10.html\">22.10</a>) An operation involved with speech that corresponds to a step in a process of constructing a mental representation. Grammar-tactics are not the same as <em>grammar rules,</em> although these have a close relation. The difference is that grammar rules are both superficial and subjective &mdash; in the sense that they purport to describe regularities in one person's behavior as observed by someone else &mdash; while grammar-tactics are objective in the sense that they are defined to be the underlying processes that actually produce speech. Although it may be more difficult to discover just what those processes do, it is better to speculate on how language is produced and used than merely to describe its observed, external forms.</p>\n<h3>Homunculus</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-5.2.html\">5.2</a>) Literally, a tiny person. In psychology, the unproductive and paradoxical idea that a person's behavior depends upon the behavior of another personlike entity located deeper inside that person.</p>\n<h3>Interaction-Square</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-14.9.html\">14.9</a>) The idea of representing the interaction between two processes by linking pairs of examples to direction-nemes. We can use this same technique not only for representing spatial relationships, but for causal, temporal, and many other kinds of interactions. This makes the interaction-square idea a powerful scheme for representing cross-realm correspondences.</p>\n<h3>Interaction</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-2.1.html\">2.1</a>) The effect of one part of a system upon another part. It is remarkable that in the history of science virtually all phenomena have eventually been explained in terms of interactions between parts taken two at a time. For example, Newton's law of gravity, which describes the mutual attraction of two particles, enables us to predict the motions of all the planets, stars, and galaxies &mdash; without any need to consider three or more objects at a time! One could conceive of a universe in which whenever three stars formed an equilateral triangle, one of them would instantly disappear &mdash; but virtually no three-part interactions have ever been observed in the physical world.</p>\n<h3>Interruption</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-15.9.html\">15.9</a>) A term used in this book to refer to any process that can be suspended while the agency involved can do some other job &mdash; yet later return to where it left off. The ability to do this requires some sort of temporary memory. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Recursion Principle</a>.</p>\n<h3>Intelligence</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.1.html\">7.1</a>) A term frequently used to express the myth that some single entity or element is responsible for the quality of a person's ability to reason. I prefer to think of this word as representing not any particular power or phenomenon, but simply all the mental skills that, at any particular moment, we admire but don't yet understand.</p>\n<h3>Introspection</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-6.5.html\">6.5</a>) The myth that our minds possess the ability directly to perceive or apprehend their own operations.</p>\n<h3>Intuition</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-12.10.html\">12.10</a>) The myth that the mind possesses some immediate (and hence inexplicable) abilities to solve problems or perceive truths. This belief is based on naive views of how we get ideas. For example, we often experience a moment of excitement or exhilaration at the moment of completing a complex and pro longed but nonconscious analysis of a problem. The myth of intuition wrongly attributes the solution to what happened in that final moment. As for being able directly to apprehend what is true, we simply forget how frequently our <em>intuitions</em> turn out wrong.</p>\n<h3>Investment Principle</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-14.6.html\">14.6</a>) The tendency of any well-developed skill to retard the growth of similar skills because the latter work less well in their early forms &mdash; and hence are used so infrequently that they never reach maturity. Because of this, we tend to invest most of our time and effort on elaborating a comparatively few techniques, rather than on accumulating many different ones. This can lead, at the same time, both to the formation of a coherent and effective personal style and to a decline in flexibility that may be wrongly attributed to aging. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Exception Principle</a>.</p>\n<h3>Isonome</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-22.1.html\">22.1</a>) A signal or pathway in the brain that has similar effects on several different agencies.</p>\n<h3>K-Line</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-8.1.html\">8.1</a>) The theory that certain kinds of memories are based on turning on sets of agents that reactivate one's previous partial mental states. This idea was first described in my essay <em>K-lines: A Theory of Memory,</em> Cognitive Science, 4 (2), April 1980.</p>\n<h3>Learning</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.5.html\">7.5</a>) An omnibus word for all the processes that lead to long-term changes in our minds.</p>\n<h3>Level-Band</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-8.5.html\">8.5</a>) The idea that a typical mental process tends to operate, at each moment, only within a certain range or portion of the structure of each agency. This makes it possible for one process to work on small details without disrupting other processes that are concerned with large-scale plans.</p>\n<h3>Logical Thinking</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-18.1.html\">18.1</a>) The popular but unsound theory that much of human reasoning proceeds in accord with clear-cut rules that lead to foolproof conclusions. In my view, we employ logical reasoning only in special forms of adult thought, which are used mainly to summarize what has already been discovered. Most of our ordinary mental work &mdash; that is, our commonsense reasoning &mdash; is based more on <em>thinking by analogy</em> &mdash; that is, applying to our present circumstances our representations of seemingly similar previous experiences.</p>\n<h3>Memorizer</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-19.5.html\">19.5</a>) An agent that can reset an agency into some previously useful state. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Recognizer</a> and <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Distributed Memory</a>.</p>\n<h3>Memory</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-15.3.html\">15.3</a>) An omnibus term for a great many structures and processes that have ill-defined boundaries in both everyday and technical psychology; these include what we call <em>re-membering,</em> <em>re-collecting,</em> <em>re-minding,</em> and <em>recognizing.</em> This book suggests that what these share in common is their involvement with how we reproduce our former partial mental states.</p>\n<h3>Mental State</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-8.4.html\">8.4</a>) The condition of activity of a group of agents at a certain moment. In this book we have assumed that every agent, at any moment, is either completely aroused or completely quiescent; in other words, we ignore the possibility of different degrees of arousal. This kind of <em>two-state</em> or <em>digital</em> assumption is characteristic of computer science and, at first, may seem too simplistic. However, experience has shown that the so-called analog theories that are alleged to be more realistic quickly become so complicated that, in the end, the simpler two-state models actually lead to deeper understandings &mdash; at least about basic principles. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Partial Mental State</a>.</p>\n<h3>Metaphor</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-29.8.html\">29.8</a>) The myth that there is a clear distinction between representations that are <em>realistic</em> and those that are merely suggestive. In their book Metaphors We Live By, University of Chicago Press, 1980, Mark Johnson and George Lakoff demonstrate that metaphor is no mere special device of literary expression but permeates virtually every aspect of human thought.</p>\n<h3>Micromemory</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-15.8.html\">15.8</a>) The smallest components of our short-term memory-systems.</p>\n<h3>Microneme</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-20.5.html\">20.5</a>) A neme involved with agents at a relatively low level. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Neme</a>.</p>\n<h3>Model</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-30.3.html\">30.3</a>) Any structure that a person can use to simulate or anticipate the behavior of something else.</p>\n<h3>Neme</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-25.6.html\">25.6</a>) An agent whose output represents a fragment of an idea or state of mind. The <em>context</em> within which a typical agent works is largely determined by the activity of the nemes that reach it. I called nemes <em>C-lines</em> in <em>Plain Talk About Neuro- developmental Epistemology,</em> in Proceedings of the Fifth International Joint Conference on Artificial Intelligence, Cambridge, Mass., 1977; the description in section 20. 5 is also based on the idea of <em>microfeature</em> developed by David L.Waltz and Jordan Pollack in <em>Massively Parallel Parsing,</em> Cognitive Science, 9 (1).</p>\n<h3>Nome</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-25.6.html\">25.6</a>) An agent whose outputs affect an agency in some predetermined manner, such as a pronome, isonome, or paranome; an agent whose effect depends more on genetic architecture than on learning from experience. The suffix -nome was chosen to suggest an atom-like, unchanging quality.</p>\n<h3>Noncompromise Principle</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-3.2.html\">3.2</a>) The idea that when two agencies conflict it may be better to ignore them both and yield control to yet another, independent agency.</p>\n<h3>Papert's Principle</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-10.4.html\">10.4</a>) The hypothesis that many steps in mental growth are based less on the acquisition of new skills than on building new administrative systems for managing already established abilities.</p>\n<h3>Paranome</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-29.3.html\">29.3</a>) An agent that operates on agencies of several different mental realms at once, with similar effects on all of them.</p>\n<h3>Partial Mental State</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-8.4.html\">8.4</a>) A description of the state of activity of some particular group of mental agents. This technical but simple idea makes it easy to understand how one can entertain and combine several ideas at the same time. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Mental State</a>.</p>\n<h3>Perceptron</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-19.7.html\">19.7</a>) A type of recognition machine that learns to weigh evidence. Invented by Frank Rosenblatt in the late 1950s, Perceptrons use singularly simple procedures for learning which weights to assign to various fragments of evidence. Seymour Papert and I analyzed this type of machine in the book <i>Perceptrons</i>, MIT Press, 1969, and showed that the simplest kinds of Perceptrons cannot do very much by themselves. However, they can do much more when arranged into societies so that some of them can then learn to recognize relations among the patterns recognized by the others. It seems quite likely that some types of brain cells use similar principles.</p>\n<h3>Picture-Frames</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-24.7.html\">24.7</a>) A type of frame whose terminals are controlled by direction-nemes. Picture-frames are particularly suited to representing certain kinds of spatial information.</p>\n<h3>Polyneme</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-19.5.html\">19.5</a>) An agent that arouses different activities, at the same time, in different agencies &mdash; as a result of learning from experience. Contrast with <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Isonome</a>.</p>\n<h3>Pronome</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-21.1.html\">21.1</a> ) A type of agent associated with a particular <em>role</em> or aspect of a representation &mdash; corresponding, for example, to the Actor, Trajectory, or Cause of some action. Pronome agents frequently control the attachments of terminals of frames to other frames; to do this, a pronome must possess some temporary memory.</p>\n<h3>Proto-specialist</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-16.3.html\">16.3</a>) One of the genetically constructed subsystems responsible for some of an animal's <em>instinctive</em> behavior. Large portions of our minds start out as almost separate proto-specialists, and we interpret their activity as manifesting different, primitive emotions. Later, as agencies become more interconnected and learn to exploit one another, these differences grow less distinct. This conception is based on the societylike theory proposed by Niko Tinbergen in The Study of Instinct, Oxford University Press, 1951.</p>\n<h3>Puzzle Principle</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-7.3.html\">7.3</a>) The idea that any problem can be solved by trial and error &mdash; provided one already has some way to recognize a solution when one is found. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Generate and Test</a>.</p>\n<h3>Realm, Mental</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-29.1.html\">29.1</a>) A division of the mind that deals with some distinct variety of concern by using distinct mechanisms and representations.</p>\n<h3>Recognizer</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-19.6.html\">19.6</a>) An agent that becomes active in response to a particular pattern of input signals.</p>\n<h3>Recursion Principle</h3>\n<p>( <a href=\"http://aurellem.org/society-of-mind/som-15.11.html\">15.11</a> ) The idea that no society, however large, can overcome every limitation &mdash; unless it has some way to reuse the same agents, over and over again, for different purposes. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Interruption</a>.</p>\n<h3>Reformulation</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-.html\">13.1</a>) Replacing one representation of something by another, different type of representation.</p>\n<h3>Representation</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-21.6.html\">21.6</a>) A structure that can be used as a substitute for something else, for a certain purpose, as one can use a map as a substitute for an actual city. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Functional Definition</a> and <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Model</a>.</p>\n<h3>Re-duplication Theory of Speech</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-22.10.html\">22.10</a>) My conjecture about what happens when a speaker explains an idea to a listener. A difference-enginelike process tries to construct a second copy of the idea's representation inside the speaker's mind. Each mental operation used in the course of that duplication process activates a corresponding grammar-tactic in the language- agency, and these lead to a stream of speech. This will result in communication to the extent that suitably matched <em>inverse grammar-tactics</em> construct, inside the listener's mind, an equivalent representation.</p>\n<h3>Script</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-13.5.html\">13.5</a>) A sequence of actions produced so automatically that it can be performed without disturbing the activities of many other agencies. The action script in section <a href=\"http://aurellem.org/society-of-mind/som-21.7.html\">21.7</a> accomplishes this by eliminating all the higher-level managers like Put and Get. A script-based skill tends to be inflexible because it lacks bureaucracy; one gains speed by removing higher-level anchor points but loses access to alternatives when things go wrong; script-based experts run the risk of becoming inarticulate. The book by Roger Schank and Robert Abelson, Scripts, Goals, Plans and Understanding, Erlbaum Associates, 1977, speculates about the human use of scripts.</p>\n<h3>Self</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-4.1.html\">4.1</a>) In this book, when written <em>Self,</em> the myth that each of us contains some special part that embodies the essence of the mind. When written as <em>self,</em> the word has the ordinary sense of a person's individuality. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Single-Agent Fallacy</a>.</p>\n<h3>Single-Agent Fallacy</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-4.1.html\">4.1</a>) The idea that a person's thought, will, decisions, and actions originate in some single center of control, instead of emerging from the activity of complex societies of processes.</p>\n<h3>Simulation</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-2.4.html\">2.4</a>) A situation in which one system mimics the behavior of another. In principle, a modern computer can be used to simulate any other kind of machine. This is important for psychology, because in the past, there was usually no way for scientists to confirm their expectations about the consequences of a complicated theory or mechanism. The theories in this book have not yet been simulated, partly because they are not specified clearly enough and partly because the older computers lacked enough capacity and speed to simulate enough agents. Such machines have recently become available; for an example, see W. Daniel Hillis's doctoral thesis, <em>The Connection Machine,</em> MIT Press, Cambridge, Mass., 1985.</p>\n<h3>Simulus</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-16.8.html\">16.8</a>) An illusion that a certain thing is present, caused by a process that evokes, at higher levels of the mind, a state resembling the state of mind that would be caused by that thing's actual presence. (A new word. )</p>\n<h3>Society</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-1.1.html\">1.1</a>) In this book, an organization of parts of a mind. I reserved the term <em>community</em> for referring to organizations of people because I did not want to suggest that a human mind resembles a human community in any particular way.</p>\n<h3>Society of More</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-10.2.html\">10.2</a>) The agents used by a mind to make comparisons of quantities.</p>\n<h3>Stage of Development</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-16.2.html\">16.2</a>) An episode in the growth of a mind. <a href=\"http://aurellem.org/society-of-mind/som-17.html\">Chapter 17</a> offers several reasons why complicated systems tend to grow in sequences of episodes, rather than through processes of steady change.</p>\n<h3>State of Mind</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-8.4.html\">8.4</a>) See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Mental State</a>.</p>\n<h3>Structural Definition</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-12.4.html\">12.4</a>) Describing something in terms of the relationships among its parts. Contrast with <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Functional Definition</a>.</p>\n<h3>Suppressor</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-27.2.html\">27.2</a>) A censorlike agent that works by disrupting a mental state that has already occurred. Suppressors are easier to construct than censors, and require less memory, but are much less efficient.</p>\n<h3>Time Blinking</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-23.3.html\">23.3</a>) Finding differences between two mental states by activating them in rapid succession and noticing which agents change their states. I suspect it is by using this method that our brains avoid the duplication problem mentioned in section <a href=\"http://aurellem.org/society-of-mind/som-23.2.html\">23.2</a>. Time blinking might be one of the synchronized activities of brain cells that gives rise to <em>brain waves.</em></p>\n<h3>Trajectory</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-21.6.html\">21.6</a>) Literally, the path or route of an action or activity. However, we use this word not only for a path in space, but, by analogy, for other realms of thought. See <a href=\"http://aurellem.org/society-of-mind/som-glossary.html\">Pronome</a>.</p>\n<h3>Trans-Frame</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-21.3.html\">21.3</a>) A particular type of frame that is centered around the trajectory between two situations, one for <em>before</em> and the other for <em>after.</em> The theories in this book about Trans-frames owe much to Roger Schank. See his book, <i>Conceptual Information Processing</i>, North-Holland, 1975.</p>\n<h3>Unconscious</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-17.10.html\">17.10</a>) A term often used, in common-sense psychology, to refer to areas of thought that are actively barred or censored against introspection. In this book we take <em>conscious</em> to mean aspects of our mental activity of which we are aware. But since there are very few such processes, we must consider virtually everything done by the mind to be unconscious.</p>\n<h3>Uniframe</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-12.3.html\">12.3</a>) A description designed to represent whichever common aspects of a group of things can be used to distinguish them from other things.</p>\n<h3>Will, Freedom of</h3>\n<p>(<a href=\"http://aurellem.org/society-of-mind/som-30.6.html\">30.6</a>) The myth that human volition is based upon some third alternative to either causality or chance.</p>",
    "text": "Because I thought this theory of the mind might interest not only specialists but everyone who thinks, I favored ordinary words over the technical language of psychology. This was rarely any sacrifice because so many psychological terms already stood for obsolete ideas. But since I also wished to speak to specialists, I tried to hide more technical ideas between the lines; I hope this second level does not show. However there still were certain points at which no ordinary words seemed satisfactory, and I had to invent new terms or assign new meanings to old ones.\nAccumulation\n(12.6) A type of learning based on collecting examples of an idea without attempting to describe what they have in common. Contrast with Uniframe.\nAgency\n(1.6) Any assembly of parts considered in terms of what it can accomplish as a unit, without regard to what each of its parts does by itself.\nAgent\n(1.4) Any part or process of the mind that by itself is simple enough to understand \u2014 even though the interactions among groups of such agents may produce phenomena that are much harder to understand.\nArtificial Intelligence\n(7.4) The field of research concerned with making machines do things that people consider to require intelligence. There is no clear boundary between psychology and Artificial Intelligence because the brain itself is a kind of machine. For an introduction to this field, I recommend Patrick Winston's textbook Artificial Intelligence, Addison-Wesley, 1984. For more connections with psychology, see Roger Schank and Kenneth Colby (eds.), Computer Models of Thought and Language, Freeman, 1973. Some influential early ideas about brains and machines can be seen in Warren McCulloch's Embodiments of Mind, MIT Press, Cambridge, Mass., 1966. See Intelligence.\nAttachment Learning\n(17.2) The specific theory, proposed in this book, that the presence of someone to whom we are emotionally attached has a special effect on how we learn, especially in infancy. Attachment learning tends to cause us to modify our goals \u2014 rather than merely improve our methods for achieving the goals we already have.\nB-Brain\n(6.4) Any part of the brain connected not to the outside world, but only to another part of the same brain. Like a manager, a B-brain can supervise an A-brain without understanding either how the A-brain works or the problems with which the A-brain is involved \u2014 for example, by recognizing patterns of activity that indicate the A-brain is confused, wasting time in repetitive activity, or focused on an unproductive level of detail.\nBlock-Arch\n(12.1) A scenario adapted from Patrick Winston's doctoral thesis, Learning Structural Descriptions by Examples, in The Psychology of Computer Vision, P. H. Winston (ed.), McGraw-Hill, 1975. The study of the world of children's building-blocks may at first seem childishly simple, but it has been one of the most productive areas of research about Artificial Intelligence, child psychology, and modern robotics engineering.\nCensor\n(27.2) An agent that inhibits or suppresses the operation of other agents. Censorlike agents are involved with how we learn from our mistakes. This idea played a prominent role in Freud's theories but has been virtually ignored by modern experimental psychologists \u2014 presumably because it is hard to study what people do not think. See Freud's 1905 book Jokes and Their Relation to the Unconscious. I suspect censorlike agents may constitute the larger part of human memory. The discussion of censors and jokes in chapter 27 is based on my essay Jokes and Their Relation to the Cognitive Unconscious, published in Cognitive Constraints on Communication, Representations and Processes, L. Vaina and J.K.K. Hintikka (eds.), . Reidel, 1981. See Suppressors.\nChallenger, Professor\n(4.4) A rival of mine, disguised as the treacherous archaeologist in Arthur Conan Doyle's novel The Lost World, who resembles Sherlock Holmes's nemesis, the mathematician Moriarty, except for being somewhat more honorable.\nClosing the Ring\n(19.10) A technique by which an agency can recall many details of a memory from being given only a few cues.\nCommon Sense\n(1.5) The mental skills that most people share. Commonsense thinking is actually more complex than many of the intellectual accomplishments that attract more attention and respect, because the mental skills we call expertise often engage large amounts of knowledge but usually employ only a few types of representations. In contrast, common sense involves many different kinds of representations and thus requires a larger range of different skills.\nComputer Science\n(6.8) A science still in its infancy. While other sciences study how particular types of objects interact, computer science studies how interactions work in general \u2014 that is, how societies of parts can accomplish what those parts cannot do separately. Although computer science began with the study of serial computers \u2014 that is, of machines that could do only one thing at a time \u2014 it has grown to the point of studying the sorts of interconnected networks of processes that must go on inside societies of mind. (For an introduction to the theory of single-process machines, see my book Computation: Finite and Infinite Machines, Prentice-Hall, 1967.)\nConsciousness\n(6.1) In this book, the word is used mainly for the myth that human minds are self-aware in the sense of perceiving what happens inside themselves. I maintain that human consciousness can never represent what is occurring at the present moment, but only a little of the recent past \u2014 partly because each agency has a limited capacity to represent what happened recently and partly because it takes time for agencies to communicate with one another. Consciousness is peculiarly hard to describe because each attempt to examine temporary memories distorts the very records it is trying to inspect. The description of consciousness in section 6.1 was adapted from my epilogue to Vernor Vinge's novel True Names, Bluejay Books, New York, 1984.\nContext\n(20.2) The effect upon one's state of mind of all the influences present at the time. At each moment, the context within which each agency works is determined by the activity of the nemes that reach that agency. See Neme.\nCross-Exclusion\n(16.4) An arrangement in which each of several agents is connected so as to inhibit all the others \u2014 so that only one of them can remain active at a time.\nCross-Realm Correspondence\n(29.4) A structure that has useful applications in two or more different mental realms. Such correspondences sometimes enable us to transfer knowledge and skill from one domain to another \u2014 without needing to accumulate experience in that other realm. This is the basis of certain important kinds of analogies and metaphors.\nCreativity\n(7.10) The myth that the production of novel ideas, artistic or otherwise, comes from some distinctive form of thought. I recommend the discussion of this subject in the chapter Variations on a Theme as the Crux of Creativity, in Douglas Hofstadter's Metamagical Themas, Basic Books, 1985.\nDefault Assumption\n(8.5, 12.12) The kind of assumption we make when we lack reasons to think otherwise. For example, we assume by default that an unfamiliar individual who belongs to a familiar class will think and act like a typical member of that class. Default assumptions are more than mere conveniences; they constitute our most productive way to make generalizations. Although such assumptions are frequently wrong, they usually do little harm because they are automatically displaced when more specific information becomes available. However, they can do incalculable harm when they are held too rigidly.\nDemon\n(27.1) An agent that constantly watches for a certain condition and intervenes when it occurs. Our discussion of demons is partly based on Eugene Charniak's doctoral thesis, Toward a Model of Children's Story Comprehension, MIT, 1972.\nDifference-Engine\n(7.8) An agency whose actions tend to make the present state of affairs more like some goal or desired state whose description is represented in that agency. This idea was developed by Allen Newell, C. J. Shaw, and Herbert A. Simon into an important theory about human problem solving. See G. Ernst and Allen Newell, GPS, A Case Study in Generality and Problem Solving, Academic Press, 1969.\nDirection-Neme\n(24.6) An agent associated with a particular direction or region in space. I suspect that bundles of direction-nemes are used inside our brains for representing not only spatial locations and directions, but also for representing many nonspatial concepts. Direction-nemes resemble isonomes in spatial realms but more resemble polynemes in other realms. See Interaction-Square and Frame-Array.\nDistributed Memory\n(20.9) A representation in which each fragment of information is stored, not by making a single, substantial change in one agent, but by making small changes in many different agents. Many theorists have been led to believe that the construction of distributed memory-systems must involve nondigital devices such as holograms; that this is not so was shown by P. J. Willshaw, 0. P. Buneman, and H. C. Longuet-Higgins in Non-Holographic Associative Memory, Nature, 222, 1969. See Memorizers.\nDuplication Problem\n(23.2) The question of how a mind could compare two similar ideas without possessing two identical agencies for representing both of them at the same time. This problem was never recognized in older theories of psychology, and I suspect it will be the downfall of most holistic theories of higher-level thought. See Time Blinking.\nEmotion\n(16.1) A term used for too many different purposes. There is a popular view that emotions are inherently more complex and harder to understand than other aspects of human thought. I maintain that infantile emotions are comparatively simple in character and that the complexity of adult emotions results from accumulating networks of mutual exploitations. In adults, these networks eventually become indescribably complicated, but no more so than the networks of our adult intellectual structures. Beyond a certain point, to distinguish between the emotional and intellectual structures of an adult is merely to describe the same structures from different points of view. See Proto-specialist.\nExploitation\n(4.5) The act of one agency making use of the activity of another agency, without understanding how it works. Exploitation is the most usual relationship among agencies because it is so difficult for them to understand one another.\nException Principle\n(12.9) The concept that it may not pay to change a well-established skill in order to accommodate an exception. The more one builds upon a certain foundation, the greater the disruption upon changing it. A system's growth will tend to cease, past the point at which the damage caused by any change outweighs the immediate gain. See Investment Principle.\nFrame\n(24.2) A representation based on a set of terminals to which other structures can be attached. Normally, each terminal is connected to a default assumption, which is easily displaced by more specific information. Other ideas about frames that are not discussed within this book were published in my chapter A Framework for Representing Knowledge, in Psychology of Computer Vision, P. H. Winston (ed.), McGraw-Hill, 1975. See Picture-Frames, Trans-frame.\nFrame-Array\n(25.2) A family of frames that share the same terminals. Information attached to any terminal of a frame-array automatically becomes available to all the frames of that array. This makes it easy to change perspective, not only in regard to a physical viewpoint, but in other mental realms as well. Frame-arrays are often controlled by bundles of direction-nemes.\nFunctional Autonomy\n(17.4) The idea that specific goals can lead to subgoals of broader character. For example, in order to please another individual, a child might develop more general goals of acquiring knowledge, power, or wealth \u2014 yet the very same subgoals might serve equally well an initial wish to injure that other individual. The term functional autonomy derives from Gordon Allport, who was one of my professors at Harvard.\nFunctional Definition\n(12.4) Specifying something in terms of how it might be used, rather than in terms of its parts and their relationships. See Structural Definition.\nGenerate and Test\n(7.3) Solving a problem by trial and error \u2014 that is, by proposing solutions recklessly, then rejecting those that do not work.\nGenius\n(7.10) An individual of prodigious mental accomplishment. Although even the most outstanding human prodigies rarely develop even twice as quickly as their peers, many people feel that their existence demands a special explanation. I suspect that the answer is to be found not in the superficial skills such people learn, but in the early accidents that lead them into learning better ways to learn.\nGoal\n(7.8) The representation in a difference-engine of an imagined final state of affairs. This definition of goal may at first seem too impersonal because it does not explain either the elation that comes with achieving a human goal or the frustration that accompanies failure. However, we should not expect to explain such complicated phenomena of adult psychology directly in terms of simple principles, since they also depend on many other aspects of our mental architecture. Basing our concept of goal on the difference-engine idea helps us to avoid the single-agent fallacy by permitting us to speak about a goal without needing to refer to the person who entertains that goal; a person's many agencies may each have different goals \u2014 without that person being aware of them.\nGrammar-Tactic\n(22.10) An operation involved with speech that corresponds to a step in a process of constructing a mental representation. Grammar-tactics are not the same as grammar rules, although these have a close relation. The difference is that grammar rules are both superficial and subjective \u2014 in the sense that they purport to describe regularities in one person's behavior as observed by someone else \u2014 while grammar-tactics are objective in the sense that they are defined to be the underlying processes that actually produce speech. Although it may be more difficult to discover just what those processes do, it is better to speculate on how language is produced and used than merely to describe its observed, external forms.\nHomunculus\n(5.2) Literally, a tiny person. In psychology, the unproductive and paradoxical idea that a person's behavior depends upon the behavior of another personlike entity located deeper inside that person.\nInteraction-Square\n(14.9) The idea of representing the interaction between two processes by linking pairs of examples to direction-nemes. We can use this same technique not only for representing spatial relationships, but for causal, temporal, and many other kinds of interactions. This makes the interaction-square idea a powerful scheme for representing cross-realm correspondences.\nInteraction\n(2.1) The effect of one part of a system upon another part. It is remarkable that in the history of science virtually all phenomena have eventually been explained in terms of interactions between parts taken two at a time. For example, Newton's law of gravity, which describes the mutual attraction of two particles, enables us to predict the motions of all the planets, stars, and galaxies \u2014 without any need to consider three or more objects at a time! One could conceive of a universe in which whenever three stars formed an equilateral triangle, one of them would instantly disappear \u2014 but virtually no three-part interactions have ever been observed in the physical world.\nInterruption\n(15.9) A term used in this book to refer to any process that can be suspended while the agency involved can do some other job \u2014 yet later return to where it left off. The ability to do this requires some sort of temporary memory. See Recursion Principle.\nIntelligence\n(7.1) A term frequently used to express the myth that some single entity or element is responsible for the quality of a person's ability to reason. I prefer to think of this word as representing not any particular power or phenomenon, but simply all the mental skills that, at any particular moment, we admire but don't yet understand.\nIntrospection\n(6.5) The myth that our minds possess the ability directly to perceive or apprehend their own operations.\nIntuition\n(12.10) The myth that the mind possesses some immediate (and hence inexplicable) abilities to solve problems or perceive truths. This belief is based on naive views of how we get ideas. For example, we often experience a moment of excitement or exhilaration at the moment of completing a complex and pro longed but nonconscious analysis of a problem. The myth of intuition wrongly attributes the solution to what happened in that final moment. As for being able directly to apprehend what is true, we simply forget how frequently our intuitions turn out wrong.\nInvestment Principle\n(14.6) The tendency of any well-developed skill to retard the growth of similar skills because the latter work less well in their early forms \u2014 and hence are used so infrequently that they never reach maturity. Because of this, we tend to invest most of our time and effort on elaborating a comparatively few techniques, rather than on accumulating many different ones. This can lead, at the same time, both to the formation of a coherent and effective personal style and to a decline in flexibility that may be wrongly attributed to aging. See Exception Principle.\nIsonome\n(22.1) A signal or pathway in the brain that has similar effects on several different agencies.\nK-Line\n(8.1) The theory that certain kinds of memories are based on turning on sets of agents that reactivate one's previous partial mental states. This idea was first described in my essay K-lines: A Theory of Memory, Cognitive Science, 4 (2), April 1980.\nLearning\n(7.5) An omnibus word for all the processes that lead to long-term changes in our minds.\nLevel-Band\n(8.5) The idea that a typical mental process tends to operate, at each moment, only within a certain range or portion of the structure of each agency. This makes it possible for one process to work on small details without disrupting other processes that are concerned with large-scale plans.\nLogical Thinking\n(18.1) The popular but unsound theory that much of human reasoning proceeds in accord with clear-cut rules that lead to foolproof conclusions. In my view, we employ logical reasoning only in special forms of adult thought, which are used mainly to summarize what has already been discovered. Most of our ordinary mental work \u2014 that is, our commonsense reasoning \u2014 is based more on thinking by analogy \u2014 that is, applying to our present circumstances our representations of seemingly similar previous experiences.\nMemorizer\n(19.5) An agent that can reset an agency into some previously useful state. See Recognizer and Distributed Memory.\nMemory\n(15.3) An omnibus term for a great many structures and processes that have ill-defined boundaries in both everyday and technical psychology; these include what we call re-membering, re-collecting, re-minding, and recognizing. This book suggests that what these share in common is their involvement with how we reproduce our former partial mental states.\nMental State\n(8.4) The condition of activity of a group of agents at a certain moment. In this book we have assumed that every agent, at any moment, is either completely aroused or completely quiescent; in other words, we ignore the possibility of different degrees of arousal. This kind of two-state or digital assumption is characteristic of computer science and, at first, may seem too simplistic. However, experience has shown that the so-called analog theories that are alleged to be more realistic quickly become so complicated that, in the end, the simpler two-state models actually lead to deeper understandings \u2014 at least about basic principles. See Partial Mental State.\nMetaphor\n(29.8) The myth that there is a clear distinction between representations that are realistic and those that are merely suggestive. In their book Metaphors We Live By, University of Chicago Press, 1980, Mark Johnson and George Lakoff demonstrate that metaphor is no mere special device of literary expression but permeates virtually every aspect of human thought.\nMicromemory\n(15.8) The smallest components of our short-term memory-systems.\nMicroneme\n(20.5) A neme involved with agents at a relatively low level. See Neme.\nModel\n(30.3) Any structure that a person can use to simulate or anticipate the behavior of something else.\nNeme\n(25.6) An agent whose output represents a fragment of an idea or state of mind. The context within which a typical agent works is largely determined by the activity of the nemes that reach it. I called nemes C-lines in Plain Talk About Neuro- developmental Epistemology, in Proceedings of the Fifth International Joint Conference on Artificial Intelligence, Cambridge, Mass., 1977; the description in section 20. 5 is also based on the idea of microfeature developed by David L.Waltz and Jordan Pollack in Massively Parallel Parsing, Cognitive Science, 9 (1).\nNome\n(25.6) An agent whose outputs affect an agency in some predetermined manner, such as a pronome, isonome, or paranome; an agent whose effect depends more on genetic architecture than on learning from experience. The suffix -nome was chosen to suggest an atom-like, unchanging quality.\nNoncompromise Principle\n(3.2) The idea that when two agencies conflict it may be better to ignore them both and yield control to yet another, independent agency.\nPapert's Principle\n(10.4) The hypothesis that many steps in mental growth are based less on the acquisition of new skills than on building new administrative systems for managing already established abilities.\nParanome\n(29.3) An agent that operates on agencies of several different mental realms at once, with similar effects on all of them.\nPartial Mental State\n(8.4) A description of the state of activity of some particular group of mental agents. This technical but simple idea makes it easy to understand how one can entertain and combine several ideas at the same time. See Mental State.\nPerceptron\n(19.7) A type of recognition machine that learns to weigh evidence. Invented by Frank Rosenblatt in the late 1950s, Perceptrons use singularly simple procedures for learning which weights to assign to various fragments of evidence. Seymour Papert and I analyzed this type of machine in the book Perceptrons, MIT Press, 1969, and showed that the simplest kinds of Perceptrons cannot do very much by themselves. However, they can do much more when arranged into societies so that some of them can then learn to recognize relations among the patterns recognized by the others. It seems quite likely that some types of brain cells use similar principles.\nPicture-Frames\n(24.7) A type of frame whose terminals are controlled by direction-nemes. Picture-frames are particularly suited to representing certain kinds of spatial information.\nPolyneme\n(19.5) An agent that arouses different activities, at the same time, in different agencies \u2014 as a result of learning from experience. Contrast with Isonome.\nPronome\n(21.1 ) A type of agent associated with a particular role or aspect of a representation \u2014 corresponding, for example, to the Actor, Trajectory, or Cause of some action. Pronome agents frequently control the attachments of terminals of frames to other frames; to do this, a pronome must possess some temporary memory.\nProto-specialist\n(16.3) One of the genetically constructed subsystems responsible for some of an animal's instinctive behavior. Large portions of our minds start out as almost separate proto-specialists, and we interpret their activity as manifesting different, primitive emotions. Later, as agencies become more interconnected and learn to exploit one another, these differences grow less distinct. This conception is based on the societylike theory proposed by Niko Tinbergen in The Study of Instinct, Oxford University Press, 1951.\nPuzzle Principle\n(7.3) The idea that any problem can be solved by trial and error \u2014 provided one already has some way to recognize a solution when one is found. See Generate and Test.\nRealm, Mental\n(29.1) A division of the mind that deals with some distinct variety of concern by using distinct mechanisms and representations.\nRecognizer\n(19.6) An agent that becomes active in response to a particular pattern of input signals.\nRecursion Principle\n( 15.11 ) The idea that no society, however large, can overcome every limitation \u2014 unless it has some way to reuse the same agents, over and over again, for different purposes. See Interruption.\nReformulation\n(13.1) Replacing one representation of something by another, different type of representation.\nRepresentation\n(21.6) A structure that can be used as a substitute for something else, for a certain purpose, as one can use a map as a substitute for an actual city. See Functional Definition and Model.\nRe-duplication Theory of Speech\n(22.10) My conjecture about what happens when a speaker explains an idea to a listener. A difference-enginelike process tries to construct a second copy of the idea's representation inside the speaker's mind. Each mental operation used in the course of that duplication process activates a corresponding grammar-tactic in the language- agency, and these lead to a stream of speech. This will result in communication to the extent that suitably matched inverse grammar-tactics construct, inside the listener's mind, an equivalent representation.\nScript\n(13.5) A sequence of actions produced so automatically that it can be performed without disturbing the activities of many other agencies. The action script in section 21.7 accomplishes this by eliminating all the higher-level managers like Put and Get. A script-based skill tends to be inflexible because it lacks bureaucracy; one gains speed by removing higher-level anchor points but loses access to alternatives when things go wrong; script-based experts run the risk of becoming inarticulate. The book by Roger Schank and Robert Abelson, Scripts, Goals, Plans and Understanding, Erlbaum Associates, 1977, speculates about the human use of scripts.\nSelf\n(4.1) In this book, when written Self, the myth that each of us contains some special part that embodies the essence of the mind. When written as self, the word has the ordinary sense of a person's individuality. See Single-Agent Fallacy.\nSingle-Agent Fallacy\n(4.1) The idea that a person's thought, will, decisions, and actions originate in some single center of control, instead of emerging from the activity of complex societies of processes.\nSimulation\n(2.4) A situation in which one system mimics the behavior of another. In principle, a modern computer can be used to simulate any other kind of machine. This is important for psychology, because in the past, there was usually no way for scientists to confirm their expectations about the consequences of a complicated theory or mechanism. The theories in this book have not yet been simulated, partly because they are not specified clearly enough and partly because the older computers lacked enough capacity and speed to simulate enough agents. Such machines have recently become available; for an example, see W. Daniel Hillis's doctoral thesis, The Connection Machine, MIT Press, Cambridge, Mass., 1985.\nSimulus\n(16.8) An illusion that a certain thing is present, caused by a process that evokes, at higher levels of the mind, a state resembling the state of mind that would be caused by that thing's actual presence. (A new word. )\nSociety\n(1.1) In this book, an organization of parts of a mind. I reserved the term community for referring to organizations of people because I did not want to suggest that a human mind resembles a human community in any particular way.\nSociety of More\n(10.2) The agents used by a mind to make comparisons of quantities.\nStage of Development\n(16.2) An episode in the growth of a mind. Chapter 17 offers several reasons why complicated systems tend to grow in sequences of episodes, rather than through processes of steady change.\nState of Mind\n(8.4) See Mental State.\nStructural Definition\n(12.4) Describing something in terms of the relationships among its parts. Contrast with Functional Definition.\nSuppressor\n(27.2) A censorlike agent that works by disrupting a mental state that has already occurred. Suppressors are easier to construct than censors, and require less memory, but are much less efficient.\nTime Blinking\n(23.3) Finding differences between two mental states by activating them in rapid succession and noticing which agents change their states. I suspect it is by using this method that our brains avoid the duplication problem mentioned in section 23.2. Time blinking might be one of the synchronized activities of brain cells that gives rise to brain waves.\nTrajectory\n(21.6) Literally, the path or route of an action or activity. However, we use this word not only for a path in space, but, by analogy, for other realms of thought. See Pronome.\nTrans-Frame\n(21.3) A particular type of frame that is centered around the trajectory between two situations, one for before and the other for after. The theories in this book about Trans-frames owe much to Roger Schank. See his book, Conceptual Information Processing, North-Holland, 1975.\nUnconscious\n(17.10) A term often used, in common-sense psychology, to refer to areas of thought that are actively barred or censored against introspection. In this book we take conscious to mean aspects of our mental activity of which we are aware. But since there are very few such processes, we must consider virtually everything done by the mind to be unconscious.\nUniframe\n(12.3) A description designed to represent whichever common aspects of a group of things can be used to distinguish them from other things.\nWill, Freedom of\n(30.6) The myth that human volition is based upon some third alternative to either causality or chance.",
    "type": "article",
    "title": "Glossary and bibliography",
    "tags": [
      {
        "score": 0.7322439551353455,
        "sentiment": 0,
        "count": 4,
        "label": "artificial intelligence",
        "uri": "https://diffbot.com/entity/X_lYDrjmAMlKKwXaDf958zg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.6978304982185364,
        "sentiment": 0,
        "count": 1,
        "label": "Glossary",
        "uri": "https://diffbot.com/entity/Bl_nvtVSTM2yIL4ROxZT4wA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.6767474412918091,
        "sentiment": 0,
        "count": 1,
        "label": "bibliography",
        "uri": "https://diffbot.com/entity/XCKduxEqtMKO90aRj3ZWQ-A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5864989757537842,
        "sentiment": 0.427,
        "count": 2,
        "label": "Patrick Winston",
        "uri": "https://diffbot.com/entity/P8URtEglOP1qs5-co31W3Mw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5829910039901733,
        "sentiment": 0,
        "count": 1,
        "label": "Warren Sturgis McCulloch",
        "uri": "https://diffbot.com/entity/PzUDb2uE8MKSbYUcPg_VwNQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      },
      {
        "score": 0.5767158269882202,
        "sentiment": 0,
        "count": 1,
        "label": "See Intelligence",
        "uri": "https://diffbot.com/entity/OzzuBIIMkPnqKYKJWV652iA",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      },
      {
        "score": 0.5718448162078857,
        "sentiment": 0,
        "count": 3,
        "label": "psychology",
        "uri": "https://diffbot.com/entity/XvmWcmJMyOt6Vha1szxrZXQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5495550036430359,
        "sentiment": 0,
        "count": 1,
        "label": "Cambridge",
        "uri": "https://diffbot.com/entity/AKtEQ-seQOfuiSDMTnvM9tA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PopulatedPlace",
          "http://dbpedia.org/ontology/Locality",
          "http://dbpedia.org/ontology/Settlement",
          "http://dbpedia.org/ontology/City"
        ]
      },
      {
        "score": 0.5402774214744568,
        "sentiment": 0,
        "count": 1,
        "label": "Common Sense",
        "uri": "https://diffbot.com/entity/XoeQIRczVOoCnN4_NrCDikA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.5345259308815002,
        "sentiment": 0,
        "count": 1,
        "label": "Jokes and Their Relation to the Unconscious",
        "uri": "https://diffbot.com/entity/XsOBxklnvP4OgUHaBP5tOrw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      }
    ],
    "docId": 177310613937,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 223048925582,
    "gburl": "http://aurellem.org/society-of-mind/som-glossary.html-diffbotxyz878577144",
    "lastCrawlTimeUTC": 1588759773,
    "timestamp": "Wed, 06 May 2020 10:09:33 GMT"
  },
  {
    "sentiment": -0.823,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-2038781808",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-25.5.html",
    "html": "<p>Imagine that you turned around and suddenly faced an absolutely unexpected scene. You'd be as shocked as though the world had changed before your eyes because so many of your expectations were not met. When we look about a familiar place, we know roughly what to expect. But what does expect mean?</p>\n<p>Whenever we become familiar with some particular environment like an office, home, or outdoor place, we represent it with a frame-array whose terminals have already been filled. Then, for each direction of motion inside that environment, our vision-systems activate the corresponding frames of that array. We also activate the corresponding frames even when we merely consider or imagine a certain body motion &mdash; and this amounts to <em>knowing what to expect.</em> In general, each frame of a spatial frame-array is controlled by some direction-neme. However, in surroundings that are either especially familiar or whose relationships we do not understand, we may learn to use more specific stimuli instead of using direction-nemes to switch the frames. For example, when you approach a familiar door, the frame for the room that you expect to find behind that door might be activated, not by your direction of motion, but by your recognition of that particular door. This could explain how a person can reside in the same home for decades, yet never learn which of its rooms share common walls.</p>\n<p>In any case, all this is oversimplified. Many of our frame-arrays must require more than nine direction views; they need machinery to modify the sizes and shapes of their objects; they must be adapted to three dimensions; and they must be able to represent what happens at intermediate moments during motion from one view to another. Furthermore, the control of frame selection cannot depend on a single, simple set of direction-nemes, for we must also compensate for the motions of our eyes, neck, body, and legs. Indeed, a major portion of our brain-machinery is involved with such calculations and corrections, and it takes a long time to learn to use all that machinery. The psychologist Piaget found that it takes ten years or more for children to refine their abilities to imagine how the same scene will appear from different viewpoints.</p>\n<p>This was the basis of Hogarth's complaint. The artist felt that many painters and sculptors never learned enough about spatial transformations. He felt that mental imagery is an acquired skill, and he scolded artists who gave too little time to <em>perfecting the ideas they have in their minds about the objects in nature.</em> Accordingly, Hogarth worked out ways to train people to better predict how viewpoints change appearances.</p>\n<p>[He who undertakes the acquisition of] perfect ideas of the distances, bearings, and oppositions of several material points and lines in even the most irregular figures, will gradually arrive at the knack of recalling them into his mind when the objects themselves are not before him &mdash; and will be of infinite service to those who invent and draw from fancy, as well as to enable those to be more correct who draw from the life.</p>",
    "text": "Imagine that you turned around and suddenly faced an absolutely unexpected scene. You'd be as shocked as though the world had changed before your eyes because so many of your expectations were not met. When we look about a familiar place, we know roughly what to expect. But what does expect mean?\nWhenever we become familiar with some particular environment like an office, home, or outdoor place, we represent it with a frame-array whose terminals have already been filled. Then, for each direction of motion inside that environment, our vision-systems activate the corresponding frames of that array. We also activate the corresponding frames even when we merely consider or imagine a certain body motion \u2014 and this amounts to knowing what to expect. In general, each frame of a spatial frame-array is controlled by some direction-neme. However, in surroundings that are either especially familiar or whose relationships we do not understand, we may learn to use more specific stimuli instead of using direction-nemes to switch the frames. For example, when you approach a familiar door, the frame for the room that you expect to find behind that door might be activated, not by your direction of motion, but by your recognition of that particular door. This could explain how a person can reside in the same home for decades, yet never learn which of its rooms share common walls.\nIn any case, all this is oversimplified. Many of our frame-arrays must require more than nine direction views; they need machinery to modify the sizes and shapes of their objects; they must be adapted to three dimensions; and they must be able to represent what happens at intermediate moments during motion from one view to another. Furthermore, the control of frame selection cannot depend on a single, simple set of direction-nemes, for we must also compensate for the motions of our eyes, neck, body, and legs. Indeed, a major portion of our brain-machinery is involved with such calculations and corrections, and it takes a long time to learn to use all that machinery. The psychologist Piaget found that it takes ten years or more for children to refine their abilities to imagine how the same scene will appear from different viewpoints.\nThis was the basis of Hogarth's complaint. The artist felt that many painters and sculptors never learned enough about spatial transformations. He felt that mental imagery is an acquired skill, and he scolded artists who gave too little time to perfecting the ideas they have in their minds about the objects in nature. Accordingly, Hogarth worked out ways to train people to better predict how viewpoints change appearances.\n[He who undertakes the acquisition of] perfect ideas of the distances, bearings, and oppositions of several material points and lines in even the most irregular figures, will gradually arrive at the knack of recalling them into his mind when the objects themselves are not before him \u2014 and will be of infinite service to those who invent and draw from fancy, as well as to enable those to be more correct who draw from the life.",
    "type": "article",
    "title": "25.5 expectations",
    "tags": [
      {
        "score": 0.5577977895736694,
        "sentiment": 0,
        "count": 2,
        "label": "William Hogarth",
        "uri": "https://diffbot.com/entity/PxdrQuIZoML-bOzOn1Wmezw",
        "rdfTypes": ["http://dbpedia.org/ontology/Person"]
      }
    ],
    "docId": 79655043457,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 98602172809,
    "gburl": "http://aurellem.org/society-of-mind/som-25.5.html-diffbotxyz117169557",
    "lastCrawlTimeUTC": 1588759683,
    "timestamp": "Wed, 06 May 2020 10:08:03 GMT"
  },
  {
    "sentiment": 0.986,
    "images": [
      {
        "naturalHeight": 115,
        "width": 382,
        "diffbotUri": "image|3|1153019139",
        "url": "http://aurellem.org/society-of-mind/illus/ch10/10-7.png",
        "naturalWidth": 382,
        "primary": true,
        "height": 115
      },
      {
        "naturalHeight": 133,
        "width": 364,
        "diffbotUri": "image|3|1153942660",
        "url": "http://aurellem.org/society-of-mind/illus/ch10/10-8.png",
        "naturalWidth": 364,
        "height": 133
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|19943745",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-10.5.html",
    "html": "<p>Think how many meanings <em>more</em> must have! We seem to use a different one for every sort of thing we know.</p>\n<p>More red. More loud. More swift. More old. More tall. More soft. More cruel. More alive. More glad. More wealthy.</p>\n<p>Each usage has a distinct sense, involving different agencies. How could all these ways to make comparisons get grouped into just one society? Here's a Society-of-More a child might use to deal with that egg cup problem.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch10/10-7.png\"/></figure>\n<p>This society has two main divisions. In its Appearance division, a Spatial subdivision considers both the increased extent occupied by the spread-out eggs and also their thinned-out appearance or reduced density. In the case of those spread-out eggs, these conflict &mdash; and the Spatial agency withdraws. Then, if the child can count, Numerical decides; otherwise the History division applies some agents that use memories of recent happenings. If some of the eggs were rolled away, Confined would say that their amount is no longer the same; if the eggs were merely moved around, Reversible would claim that their amount cannot have changed.</p>\n<p>To solve the water jar problem, the Society-of-More would need other kinds of lower-level agents:</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch10/10-8.png\"/></figure>\n<p>You might complain that even if we needed these hordes of lower-level agencies to make comparisons, this system has too many middle-level managers. But those mountains of bureaucracy are more than worth their cost. Each higher-level agent embodies a form of <em>higher-order</em> knowledge that helps us organize ourselves by telling us when and how to use the things we know. Without a many layered management, we couldn't use the knowledge in our low-level agencies; they'd all keep getting in one another's way.</p>",
    "text": "Think how many meanings more must have! We seem to use a different one for every sort of thing we know.\nMore red. More loud. More swift. More old. More tall. More soft. More cruel. More alive. More glad. More wealthy.\nEach usage has a distinct sense, involving different agencies. How could all these ways to make comparisons get grouped into just one society? Here's a Society-of-More a child might use to deal with that egg cup problem.\nThis society has two main divisions. In its Appearance division, a Spatial subdivision considers both the increased extent occupied by the spread-out eggs and also their thinned-out appearance or reduced density. In the case of those spread-out eggs, these conflict \u2014 and the Spatial agency withdraws. Then, if the child can count, Numerical decides; otherwise the History division applies some agents that use memories of recent happenings. If some of the eggs were rolled away, Confined would say that their amount is no longer the same; if the eggs were merely moved around, Reversible would claim that their amount cannot have changed.\nTo solve the water jar problem, the Society-of-More would need other kinds of lower-level agents:\nYou might complain that even if we needed these hordes of lower-level agencies to make comparisons, this system has too many middle-level managers. But those mountains of bureaucracy are more than worth their cost. Each higher-level agent embodies a form of higher-order knowledge that helps us organize ourselves by telling us when and how to use the things we know. Without a many layered management, we couldn't use the knowledge in our low-level agencies; they'd all keep getting in one another's way.",
    "type": "article",
    "title": "10.5 the society-of-more",
    "tags": [
      {
        "score": 0.694105327129364,
        "sentiment": 0,
        "count": 2,
        "label": "society",
        "uri": "https://diffbot.com/entity/X5-r2onDFMwqrJxIepxIeQw"
      },
      {
        "score": 0.559246838092804,
        "sentiment": -0.134,
        "count": 2,
        "label": "Spatial",
        "uri": "https://diffbot.com/entity/C_wxbBiqjPmy1E01UUmtzjg",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      },
      {
        "score": 0.5202550292015076,
        "sentiment": -0.193,
        "count": 2,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 84838662567,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 106164306312,
    "gburl": "http://aurellem.org/society-of-mind/som-10.5.html-diffbotxyz464012265",
    "lastCrawlTimeUTC": 1588759652,
    "timestamp": "Wed, 06 May 2020 10:07:32 GMT"
  },
  {
    "date": "Wed, 16 Oct 2019 00:00:00 GMT",
    "sentiment": 0.814,
    "humanLanguage": "en",
    "estimatedDate": "Wed, 16 Oct 2019 00:00:00 GMT",
    "diffbotUri": "article|3|1363373632",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-16.10.html",
    "html": "<blockquote> Since emotions are few and reasons are many (said the robot, Giskard), the behavior of a crowd can be more easily predicted than the behavior of one person can. &mdash;Isaac Asimov </blockquote>\n<p>What are emotions, anyway? Our culture sees this question as a deep and ancient mystery. How could the idea of society of mind contribute to what our ancestors have said? Common-sense psychology has not even reached a consensus on which emotions exist.</p>\n<table> <tbody><tr> <td> Restlessness </td> <td> Fear </td> <td> Gladness </td> <td> Jealousy </td> <td> Sorrow </td> </tr> <tr> <td> Curiosity </td> <td> Hate </td> <td> Enthusiasm </td> <td> Ambition </td> <td> Thirst </td> </tr> <tr> <td> Infatuation </td> <td> Anger </td> <td> Admiration </td> <td> Laziness </td> <td> Disgust </td> </tr> <tr> <td> Impatience </td> <td> Love </td> <td> Boredom </td> <td> Contempt </td> <td> Hunger </td> </tr> <tr> <td> Excitement </td> <td> Greed </td> <td> Reverence </td> <td> Anxiety </td> <td> Lust </td> </tr> </tbody></table>\n<p>If there exists anger, what constitutes rage? How does fear relate to fright, terror, dread, dismay, and all such other awful things? How does love relate to reverence or to attachment or infatuation? Are these just various degrees of intensity and direction, or are they genuinely different entities that happen to be neighbors in an uncharted universe of affections? Are hate and love quite separate things, or, similarly, courage and cowardice &mdash; or are these merely pairs of extremes, each just the absence of its peer? What are emotions, anyway, and what are all the other things we label moods, feelings, passions, needs, or sensibilities? We find it hard to agree on the meanings of words like these, presumably because few of them actually correspond to clearly distinct mental processes. Instead, when we learn such words, we each attach to them variously different and personal accumulations of conceptions in our minds.</p>\n<p>Infants' early emotion signs clearly signify their needs. We later learn to use such signals in more exploitative ways. Thus you can learn to use affection or anger as a social coin in trade for various accommodations; for example, one can pretend to be angry or pleased, or even offer &mdash; that is, threaten or promise &mdash; to become angry or affectionate in certain circumstances. Our culture is ambivalent about such matters; on one side we're taught that emotions should be natural and spontaneous; on the other side we're told that we must learn to regulate them. We recognize in deeds (though not in words) that feeling may be easier to understand and modify than other parts of intellect. We censure those who fail to learn to control their emotions but merely pity those whose problem-solving capabilities are poor; we blame for <em>lack of self-control,</em> but not for <em>weakness of intelligence.</em></p>\n<p>Our earliest emotions are built-in processes in which inborn proto-specialists control what happens in our brains. Soon we learn to overrule those schemes, as our surroundings teach us what we ought to feel. Parents, teachers, friends, and finally our self-ideals impose upon us new rules for how to use the remnants of those early states: they teach us how and when to feel and show each kind of emotion sign. By the time we're adults, these systems have become too complicated to understand. By the time we've passed through all those stages of development, our grown-up minds have been rebuilt too many times to remember or understand much of how it felt to be an infant.</p>",
    "text": "Since emotions are few and reasons are many (said the robot, Giskard), the behavior of a crowd can be more easily predicted than the behavior of one person can. \u2014Isaac Asimov\nWhat are emotions, anyway? Our culture sees this question as a deep and ancient mystery. How could the idea of society of mind contribute to what our ancestors have said? Common-sense psychology has not even reached a consensus on which emotions exist.\nRestlessness\nFear\nGladness\nJealousy\nSorrow\nCuriosity\nHate\nEnthusiasm\nAmbition\nThirst\nInfatuation\nAnger\nAdmiration\nLaziness\nDisgust\nImpatience\nLove\nBoredom\nContempt\nHunger\nExcitement\nGreed\nReverence\nAnxiety\nLust\nIf there exists anger, what constitutes rage? How does fear relate to fright, terror, dread, dismay, and all such other awful things? How does love relate to reverence or to attachment or infatuation? Are these just various degrees of intensity and direction, or are they genuinely different entities that happen to be neighbors in an uncharted universe of affections? Are hate and love quite separate things, or, similarly, courage and cowardice \u2014 or are these merely pairs of extremes, each just the absence of its peer? What are emotions, anyway, and what are all the other things we label moods, feelings, passions, needs, or sensibilities? We find it hard to agree on the meanings of words like these, presumably because few of them actually correspond to clearly distinct mental processes. Instead, when we learn such words, we each attach to them variously different and personal accumulations of conceptions in our minds.\nInfants' early emotion signs clearly signify their needs. We later learn to use such signals in more exploitative ways. Thus you can learn to use affection or anger as a social coin in trade for various accommodations; for example, one can pretend to be angry or pleased, or even offer \u2014 that is, threaten or promise \u2014 to become angry or affectionate in certain circumstances. Our culture is ambivalent about such matters; on one side we're taught that emotions should be natural and spontaneous; on the other side we're told that we must learn to regulate them. We recognize in deeds (though not in words) that feeling may be easier to understand and modify than other parts of intellect. We censure those who fail to learn to control their emotions but merely pity those whose problem-solving capabilities are poor; we blame for lack of self-control, but not for weakness of intelligence.\nOur earliest emotions are built-in processes in which inborn proto-specialists control what happens in our brains. Soon we learn to overrule those schemes, as our surroundings teach us what we ought to feel. Parents, teachers, friends, and finally our self-ideals impose upon us new rules for how to use the remnants of those early states: they teach us how and when to feel and show each kind of emotion sign. By the time we're adults, these systems have become too complicated to understand. By the time we've passed through all those stages of development, our grown-up minds have been rebuilt too many times to remember or understand much of how it felt to be an infant.",
    "type": "article",
    "title": "16.10 adult emotions",
    "tags": [
      {
        "score": 0.6827964782714844,
        "sentiment": 0,
        "count": 2,
        "label": "Our Culture, What's Left of It",
        "uri": "https://diffbot.com/entity/Xab8dv2XTNgu0zljCjl6fGg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      },
      {
        "score": 0.5308483242988586,
        "sentiment": 0,
        "count": 1,
        "label": "Society of Mind",
        "uri": "https://diffbot.com/entity/X7xnOHKxmOA-YF6VzL-F-NA",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      }
    ],
    "docId": 105365504430,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 175644066225,
    "gburl": "http://aurellem.org/society-of-mind/som-16.10.html-diffbotxyz3420482396",
    "lastCrawlTimeUTC": 1588759713,
    "timestamp": "Wed, 06 May 2020 10:08:33 GMT"
  },
  {
    "sentiment": 0,
    "images": [
      {
        "naturalHeight": 138,
        "width": 336,
        "diffbotUri": "image|3|-1067130587",
        "url": "http://aurellem.org/society-of-mind/illus/ch13/13-9.png",
        "naturalWidth": 336,
        "primary": true,
        "height": 138
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-328070776",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-13.4.html",
    "html": "<p>That body-head drawing seems very wrong to most adults, yet it seems to please many children. Does it really look like a person to those children? That seems like a simple question, but it is not &mdash; for we must remember that a child is not a single agent and that various other agencies inside a child's mind may not be satisfied at all. At the moment, those other agencies are not in control and have little effect. Yet if some creature came on the scene that really looked like that, most children would be terrified. It does not make much sense to speak of what a person <em>really</em> sees, because we have so many different agencies.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch13/13-9.png\"/></figure>\n<p>What happened in the intervening years to make the older children draw the body separately? This could come about without even making any change in the list of features and relations we used in the previous section. It would need only a small change in step 2 of our drawing procedure:</p>\n<p>This ensures that every feature mentioned in the list will be represented only once in the drawing, even if two such features look alike. Of course, this requires some ability to count each feature only once, and never twice. How interesting that in order to make mature, realistic drawings, the child could exploit the same kind of ability it must acquire in order to count things properly!</p>\n<p>To be sure, we could explain the child's progress in other ways. That new and <em>more realistic</em> picture could come from adding a neck to the feature list, for that would demand a separate body and head. It might suffice simply to impose an additional constraint or relationship: that the head be above the body. One might argue that the younger child never had a clear concept of a separate and distinct body feature in the first place; after all, there are many things that you can do with your arms and legs or with your head &mdash; but your body only gets in the way.</p>\n<p>In any case, after mastering the art of making these body-head drawings, many children seem to progress rather slowly in the art of making personal portraits, and these types of <em>childish</em> drawings often persist for some years. I suspect that after children learn to make recognizable figures, they usually move on to face the problems of representing much more complicated scenes. As they do this, we should continue to appreciate how well children deal with the problems they set for themselves. They may not meet our own grown-up expectations, but they often solve their own versions of the problems we pose.</p>",
    "text": "That body-head drawing seems very wrong to most adults, yet it seems to please many children. Does it really look like a person to those children? That seems like a simple question, but it is not \u2014 for we must remember that a child is not a single agent and that various other agencies inside a child's mind may not be satisfied at all. At the moment, those other agencies are not in control and have little effect. Yet if some creature came on the scene that really looked like that, most children would be terrified. It does not make much sense to speak of what a person really sees, because we have so many different agencies.\nWhat happened in the intervening years to make the older children draw the body separately? This could come about without even making any change in the list of features and relations we used in the previous section. It would need only a small change in step 2 of our drawing procedure:\nThis ensures that every feature mentioned in the list will be represented only once in the drawing, even if two such features look alike. Of course, this requires some ability to count each feature only once, and never twice. How interesting that in order to make mature, realistic drawings, the child could exploit the same kind of ability it must acquire in order to count things properly!\nTo be sure, we could explain the child's progress in other ways. That new and more realistic picture could come from adding a neck to the feature list, for that would demand a separate body and head. It might suffice simply to impose an additional constraint or relationship: that the head be above the body. One might argue that the younger child never had a clear concept of a separate and distinct body feature in the first place; after all, there are many things that you can do with your arms and legs or with your head \u2014 but your body only gets in the way.\nIn any case, after mastering the art of making these body-head drawings, many children seem to progress rather slowly in the art of making personal portraits, and these types of childish drawings often persist for some years. I suspect that after children learn to make recognizable figures, they usually move on to face the problems of representing much more complicated scenes. As they do this, we should continue to appreciate how well children deal with the problems they set for themselves. They may not meet our own grown-up expectations, but they often solve their own versions of the problems we pose.",
    "type": "article",
    "title": "13.4 children's drawing-frames",
    "tags": [
      {
        "score": 0.7763718962669373,
        "sentiment": -0.461,
        "count": 12,
        "label": "child",
        "uri": "https://diffbot.com/entity/XFfVrHIuyPp2bjICzGxdT9g",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.7552162408828735,
        "sentiment": -0.44,
        "count": 1,
        "label": "child art",
        "uri": "https://diffbot.com/entity/XK-QYuIbUM16O_1METjhOiQ",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      },
      {
        "score": 0.5474502444267273,
        "sentiment": -0.294,
        "count": 1,
        "label": "drawing",
        "uri": "https://diffbot.com/entity/XfPGhEDU3OSmuKXt27BTSAw"
      }
    ],
    "docId": 210730860939,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 52800569754,
    "gburl": "http://aurellem.org/society-of-mind/som-13.4.html-diffbotxyz649541792",
    "lastCrawlTimeUTC": 1588759741,
    "timestamp": "Wed, 06 May 2020 10:09:01 GMT"
  },
  {
    "sentiment": -0.645,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1530270003",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.5.html",
    "html": "<p>Many things that we regard as physical are actually psychological. To see why this is so, let's try to say what we mean by <em>chair.</em> At first it seems enough to say:</p>\n<p><em>A chair is a thing with legs and a back and seat.</em></p>\n<p>But when we look more carefully at what we recognize as chairs, we find that many of them do not fit this description because they don't divide into those separate parts. When all is done, there's little we can find in common to all chairs &mdash; except for their intended use.</p>\n<p><em>A chair is something you can sit upon.</em></p>\n<p>But that, too, seems inadequate: it makes it seem as though a chair were as insubstantial as a wish. The solution is that we need to combine at least two different kinds of descriptions. On one side, we need structural descriptions for recognizing chairs when we see them. On the other side we need functional descriptions in order to know what we can do with chairs. We can capture more of what we mean by interweaving both ideas. But it's not enough merely to propose a vague association, because in order for it to have some use, we need more intimate details about how those chair parts actually help a person to sit. To catch the proper meaning, we need connections between parts of the chair structure and the requirements of the human body that those parts are supposed to serve. Our network needs details like these:</p>\n<p>Without such knowledge, we might just crawl under the chair or try to wear it on our head. But with that knowledge we can do amazing things, like applying the concept of a chair to see how we could sit on a box, even though it has no legs or back!</p>\n<p>Uniframes that include structures like this can be powerful. For example, such knowledge about relations between structure, comfort, and posture could be used to understand when a box could serve as a chair: that is, only when it is of suitable height for a person who does not require a backrest or room to bend the knees. To be sure, such clever reasoning requires special mental skills with which to redescribe or <em>reformulate</em> the descriptions of both box and chair so that they <em>match</em> despite their differences. Until we learn to make old descriptions fit new circumstances, our old knowledge can be applied only to the circumstances in which it was learned. And that would scarcely ever work, since circumstances never repeat themselves perfectly.</p>",
    "text": "Many things that we regard as physical are actually psychological. To see why this is so, let's try to say what we mean by chair. At first it seems enough to say:\nA chair is a thing with legs and a back and seat.\nBut when we look more carefully at what we recognize as chairs, we find that many of them do not fit this description because they don't divide into those separate parts. When all is done, there's little we can find in common to all chairs \u2014 except for their intended use.\nA chair is something you can sit upon.\nBut that, too, seems inadequate: it makes it seem as though a chair were as insubstantial as a wish. The solution is that we need to combine at least two different kinds of descriptions. On one side, we need structural descriptions for recognizing chairs when we see them. On the other side we need functional descriptions in order to know what we can do with chairs. We can capture more of what we mean by interweaving both ideas. But it's not enough merely to propose a vague association, because in order for it to have some use, we need more intimate details about how those chair parts actually help a person to sit. To catch the proper meaning, we need connections between parts of the chair structure and the requirements of the human body that those parts are supposed to serve. Our network needs details like these:\nWithout such knowledge, we might just crawl under the chair or try to wear it on our head. But with that knowledge we can do amazing things, like applying the concept of a chair to see how we could sit on a box, even though it has no legs or back!\nUniframes that include structures like this can be powerful. For example, such knowledge about relations between structure, comfort, and posture could be used to understand when a box could serve as a chair: that is, only when it is of suitable height for a person who does not require a backrest or room to bend the knees. To be sure, such clever reasoning requires special mental skills with which to redescribe or reformulate the descriptions of both box and chair so that they match despite their differences. Until we learn to make old descriptions fit new circumstances, our old knowledge can be applied only to the circumstances in which it was learned. And that would scarcely ever work, since circumstances never repeat themselves perfectly.",
    "type": "article",
    "title": "12.5 the function of structures",
    "docId": 44456411523,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 6236635550,
    "gburl": "http://aurellem.org/society-of-mind/som-12.5.html-diffbotxyz1225782369",
    "lastCrawlTimeUTC": 1588759608,
    "timestamp": "Wed, 06 May 2020 10:06:48 GMT"
  },
  {
    "sentiment": 0.291,
    "humanLanguage": "en",
    "diffbotUri": "article|3|1387531838",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-7.1.html",
    "html": "<p>Many people insist on having some definition of <em>intelligence.</em></p>\n<p>CRITIC: How can we be sure that things like plants and stones, or storms and streams, are not intelligent in ways that we have not yet conceived?</p>\n<p>It doesn't seem a good idea to use the same word for different things, unless one has in mind important ways in which they are the same. Plants and streams don't seem very good at solving the kinds of problems we regard as needing intelligence.</p>\n<p>CRITIC: What's so special about solving problems? And why don't you define <em>intelligence</em> precisely, so that we can agree on what we're discussing?</p>\n<p>That isn't a good idea, either. An author's job is using words the ways other people do, not telling others how to use them. In the few places the word <em>intelligence</em> appears in this book, it merely means what people usually mean &mdash; the ability to solve hard problems.</p>\n<p>CRITIC: Then you should define what you mean by a <em>hard</em> problem. We know it took a lot of human intelligence to build the pyramids &mdash; yet little coral reef animals build impressive structures on even larger scales. So don't you have to consider them intelligent? Isn't it hard to build gigantic coral reefs?</p>\n<p>Yes, but it is only an illusion that animals can <em>solve</em> those problems! No individual bird discovers a way to fly. Instead, each bird exploits a solution that evolved from countless reptile years of evolution. Similarly, although a person might find it very hard to design an oriole's nest or a beaver's dam, no oriole or beaver ever figures out such things at all. Those animals don't <em>solve</em> such problems themselves; they only exploit procedures available within their complicated gene-built brains.</p>\n<p>CRITIC: Then wouldn't you be forced to say that evolution itself must be intelligent, since it solved those problems of flying and building reefs and nests?</p>\n<p>No, because people also use the word <em>intelligence</em> to emphasize swiftness and efficiency. Evolution's time rate is so slow that we don't see it as intelligent, even though it finally produces wonderful things we ourselves cannot yet make. Anyway, it isn't wise to treat an old, vague word like <em>intelligence</em> as though it must define any definite thing. Instead of trying to say what such a word <em>means,</em> it is better simply to try to explain how we use it.</p>\n<p>Our minds contain processes that enable us to solve problems we consider difficult. <em>Intelligence</em> is our name for whichever of those processes we don't yet understand.</p>\n<p>Some people dislike this <em>definition</em> because its meaning is doomed to keep changing as we learn more about psychology. But in my view that's exactly how it ought to be, because the very concept of intelligence is like a stage magician's trick. Like the concept of <em>the unexplored regions of Africa,</em> it disappears as soon as we discover it.</p>",
    "text": "Many people insist on having some definition of intelligence.\nCRITIC: How can we be sure that things like plants and stones, or storms and streams, are not intelligent in ways that we have not yet conceived?\nIt doesn't seem a good idea to use the same word for different things, unless one has in mind important ways in which they are the same. Plants and streams don't seem very good at solving the kinds of problems we regard as needing intelligence.\nCRITIC: What's so special about solving problems? And why don't you define intelligence precisely, so that we can agree on what we're discussing?\nThat isn't a good idea, either. An author's job is using words the ways other people do, not telling others how to use them. In the few places the word intelligence appears in this book, it merely means what people usually mean \u2014 the ability to solve hard problems.\nCRITIC: Then you should define what you mean by a hard problem. We know it took a lot of human intelligence to build the pyramids \u2014 yet little coral reef animals build impressive structures on even larger scales. So don't you have to consider them intelligent? Isn't it hard to build gigantic coral reefs?\nYes, but it is only an illusion that animals can solve those problems! No individual bird discovers a way to fly. Instead, each bird exploits a solution that evolved from countless reptile years of evolution. Similarly, although a person might find it very hard to design an oriole's nest or a beaver's dam, no oriole or beaver ever figures out such things at all. Those animals don't solve such problems themselves; they only exploit procedures available within their complicated gene-built brains.\nCRITIC: Then wouldn't you be forced to say that evolution itself must be intelligent, since it solved those problems of flying and building reefs and nests?\nNo, because people also use the word intelligence to emphasize swiftness and efficiency. Evolution's time rate is so slow that we don't see it as intelligent, even though it finally produces wonderful things we ourselves cannot yet make. Anyway, it isn't wise to treat an old, vague word like intelligence as though it must define any definite thing. Instead of trying to say what such a word means, it is better simply to try to explain how we use it.\nOur minds contain processes that enable us to solve problems we consider difficult. Intelligence is our name for whichever of those processes we don't yet understand.\nSome people dislike this definition because its meaning is doomed to keep changing as we learn more about psychology. But in my view that's exactly how it ought to be, because the very concept of intelligence is like a stage magician's trick. Like the concept of the unexplored regions of Africa, it disappears as soon as we discover it.",
    "type": "article",
    "title": "7.1 intelligence",
    "tags": [
      {
        "score": 0.7128631472587585,
        "sentiment": 0.832,
        "count": 1,
        "label": "United States Intelligence Community",
        "uri": "https://diffbot.com/entity/OoJSMz4rCODSf_A6waKqC5Q",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Non-ProfitOrganisation"
        ]
      },
      {
        "score": 0.7051301002502441,
        "sentiment": 0,
        "count": 4,
        "label": "critic",
        "uri": "https://diffbot.com/entity/XIWaV9PFrOFmQ7JSXCYdLRQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 82907955614,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 135849951636,
    "gburl": "http://aurellem.org/society-of-mind/som-7.1.html-diffbotxyz4186809884",
    "lastCrawlTimeUTC": 1588759537,
    "timestamp": "Wed, 06 May 2020 10:05:37 GMT"
  },
  {
    "sentiment": 0.8,
    "images": [
      {
        "naturalHeight": 206,
        "width": 411,
        "diffbotUri": "image|3|1788944770",
        "url": "http://aurellem.org/society-of-mind/illus/ch21/21-6.png",
        "naturalWidth": 411,
        "primary": true,
        "height": 206
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-837251602",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-21.6.html",
    "html": "<p>Our first Trans-frame scheme was connected to only four pronomes &mdash; Origin, Destination, Difference, and Trajectory. These are just enough to link together a simple chain of reasoning. But what about all the other roles that things can play &mdash; like Actors, Recipients, Vehicles, Goals, Obstacles, and Instruments? In order to keep track of these, we surely need some other pronomes, too. So now we shall imagine a larger kind of Trans-frame scheme that engages, all at once, a much larger constellation of different pronome roles.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch21/21-6.png\"/></figure>\n<p>Why propose this particular structure instead of some other arrangement? Because I suspect that Trans-like structures have a special prominence in how we think. One reason is that some sort of bridgelike scheme seems indispensable for making those all- important connections between structures and functions. Without that bridge trajectory, it might be hard to connect what we learn about things to what we learn about using them. Also, in order to use chainlike thinking skills, we need to be able to represent what we know in ways that provide connection points for roles like Origins, Destinations, and Differences. All these requirements suggest the use of bridgelike frames.</p>\n<p>One might wonder if we need to use any <em>standard</em> representations at all. The answer is that we do, indeed, if only because of the Investment principle. No matter what kind of representations we adopt, there's nothing we could do with them until we also learn effective skills and memory-scripts that work with them. And since such complex skills take time to grow, we'd never have enough time to learn new sets of representations for every different new idea. No powerful skills would ever emerge without some reasonably uniform schemes for representing knowledge.</p>\n<p>Trans-like representation-schemes have been very useful in Artificial Intelligence research projects. They have been useful, among other things, for making theories of problem solving, for making clever computer programs to simulate <em>expertise</em> in various specialties, and for making programs that understand languagelike expressions to limited degrees. In the next few sections we'll see how to use them for making several different kinds of <em>chains of reasoning.</em></p>",
    "text": "Our first Trans-frame scheme was connected to only four pronomes \u2014 Origin, Destination, Difference, and Trajectory. These are just enough to link together a simple chain of reasoning. But what about all the other roles that things can play \u2014 like Actors, Recipients, Vehicles, Goals, Obstacles, and Instruments? In order to keep track of these, we surely need some other pronomes, too. So now we shall imagine a larger kind of Trans-frame scheme that engages, all at once, a much larger constellation of different pronome roles.\nWhy propose this particular structure instead of some other arrangement? Because I suspect that Trans-like structures have a special prominence in how we think. One reason is that some sort of bridgelike scheme seems indispensable for making those all- important connections between structures and functions. Without that bridge trajectory, it might be hard to connect what we learn about things to what we learn about using them. Also, in order to use chainlike thinking skills, we need to be able to represent what we know in ways that provide connection points for roles like Origins, Destinations, and Differences. All these requirements suggest the use of bridgelike frames.\nOne might wonder if we need to use any standard representations at all. The answer is that we do, indeed, if only because of the Investment principle. No matter what kind of representations we adopt, there's nothing we could do with them until we also learn effective skills and memory-scripts that work with them. And since such complex skills take time to grow, we'd never have enough time to learn new sets of representations for every different new idea. No powerful skills would ever emerge without some reasonably uniform schemes for representing knowledge.\nTrans-like representation-schemes have been very useful in Artificial Intelligence research projects. They have been useful, among other things, for making theories of problem solving, for making clever computer programs to simulate expertise in various specialties, and for making programs that understand languagelike expressions to limited degrees. In the next few sections we'll see how to use them for making several different kinds of chains of reasoning.",
    "type": "article",
    "title": "21.6 trans-frame pronomes",
    "tags": [
      {
        "score": 0.6141912341117859,
        "sentiment": 0,
        "count": 1,
        "label": "artificial intelligence",
        "uri": "https://diffbot.com/entity/X_lYDrjmAMlKKwXaDf958zg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.601997971534729,
        "sentiment": 0.806,
        "count": 1,
        "label": "Origin Systems",
        "uri": "https://diffbot.com/entity/Ccb_oHREFPvC2aSvyOr652A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5850575566291809,
        "sentiment": 0.847,
        "count": 1,
        "label": "Destination",
        "uri": "https://diffbot.com/entity/Cfsi5hk5iNX2ll5YomlNYNw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Group",
          "http://dbpedia.org/ontology/Band"
        ]
      },
      {
        "score": 0.5732855200767517,
        "sentiment": 0,
        "count": 2,
        "label": "Psychology of reasoning",
        "uri": "https://diffbot.com/entity/XVm-Vdw8-OJeBqGziWNLIqA"
      },
      {
        "score": 0.5035525560379028,
        "sentiment": 0,
        "count": 1,
        "label": "musical instrument",
        "uri": "https://diffbot.com/entity/X_BYz5L3MNwqwDVtVymhTdw"
      }
    ],
    "docId": 138076045704,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 265029468555,
    "gburl": "http://aurellem.org/society-of-mind/som-21.6.html-diffbotxyz3285257036",
    "lastCrawlTimeUTC": 1588759560,
    "timestamp": "Wed, 06 May 2020 10:06:00 GMT"
  },
  {
    "sentiment": -0.702,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-755472588",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-24.4.html",
    "html": "<p>When someone says, <em>John threw a ball,</em> you probably unconsciously assume a certain set of features and qualities of the ball, like color, size, and weight. These are your assumptions by default, the kind we talked about when we first introduced the idea of level-bands. Your assumptions about that ball might be derived from some ball you owned long ago &mdash; or, possibly, your newest one. It is our theory that such optional details are usually attached too weakly to hold against the sharp insistence of reality, so that other stimuli will find them easy to detach or otherwise adapt. This is why default assumptions make weak images, and why we aren't too amazed when they turn out wrong. It is no surprise that frames share so many properties of K-lines, since the terminals of frames themselves will lie in level-bands near the K-lines whose fringes represent our expectations and default assumptions.</p>\n<p>But why use default assumptions at all, instead of simply seeing what's really there? Because unless we make assumptions, the world would simply make no sense. It would be as useless to perceive how things <em>actually look</em> as it would be to watch the random dots on untuned television screens. What really matters is being able to see what things look like. This is why our brains need special machinery for representing what we see in terms of distinct <em>objects.</em> The very idea of an object embodies making many assumptions that <em>go without saying</em> &mdash; for example, that it has substance and boundaries, that it existed before we saw it, and that it will remain afterward &mdash; in short, that it will act like other typical objects. Thus, though we never see every side of an object at once, we always assume that its unseen sides exist. I suspect that the larger part of what we know &mdash; or think we know &mdash; is represented by default assumptions, because there is so little that we know with perfect certainty.</p>\n<p>We use default assumptions in personal relations, too. Why do so many people give such credence to astrology, to classify friends by the months of their births? Perhaps it seems a forward step, to class all persons into just twelve types &mdash; to those who once supposed that there were less. And how does the writer's craft evoke such lifelike characters? It's ridiculous to think that people could be well portrayed in so few words. Instead, our story writers use phrases that activate great networks of assumptions that already lie in the minds of their readers. It takes great skill to create those illusions &mdash; to activate unknown processes in unknown readers' minds and to shape them to one's purposes. Indeed, in doing so, a writer can make things clearer than reality. For although words are merely catalysts for starting mental processes, so, too, are real things: we can't sense what they really are, only what they remind us of. As Proust went on to say:</p>\n<p>Each reader reads only what is already inside himself. A book is only a sort of optical instrument which the writer offers to let the reader discover in himself what he would not have found without the aid of the book.</p>",
    "text": "When someone says, John threw a ball, you probably unconsciously assume a certain set of features and qualities of the ball, like color, size, and weight. These are your assumptions by default, the kind we talked about when we first introduced the idea of level-bands. Your assumptions about that ball might be derived from some ball you owned long ago \u2014 or, possibly, your newest one. It is our theory that such optional details are usually attached too weakly to hold against the sharp insistence of reality, so that other stimuli will find them easy to detach or otherwise adapt. This is why default assumptions make weak images, and why we aren't too amazed when they turn out wrong. It is no surprise that frames share so many properties of K-lines, since the terminals of frames themselves will lie in level-bands near the K-lines whose fringes represent our expectations and default assumptions.\nBut why use default assumptions at all, instead of simply seeing what's really there? Because unless we make assumptions, the world would simply make no sense. It would be as useless to perceive how things actually look as it would be to watch the random dots on untuned television screens. What really matters is being able to see what things look like. This is why our brains need special machinery for representing what we see in terms of distinct objects. The very idea of an object embodies making many assumptions that go without saying \u2014 for example, that it has substance and boundaries, that it existed before we saw it, and that it will remain afterward \u2014 in short, that it will act like other typical objects. Thus, though we never see every side of an object at once, we always assume that its unseen sides exist. I suspect that the larger part of what we know \u2014 or think we know \u2014 is represented by default assumptions, because there is so little that we know with perfect certainty.\nWe use default assumptions in personal relations, too. Why do so many people give such credence to astrology, to classify friends by the months of their births? Perhaps it seems a forward step, to class all persons into just twelve types \u2014 to those who once supposed that there were less. And how does the writer's craft evoke such lifelike characters? It's ridiculous to think that people could be well portrayed in so few words. Instead, our story writers use phrases that activate great networks of assumptions that already lie in the minds of their readers. It takes great skill to create those illusions \u2014 to activate unknown processes in unknown readers' minds and to shape them to one's purposes. Indeed, in doing so, a writer can make things clearer than reality. For although words are merely catalysts for starting mental processes, so, too, are real things: we can't sense what they really are, only what they remind us of. As Proust went on to say:\nEach reader reads only what is already inside himself. A book is only a sort of optical instrument which the writer offers to let the reader discover in himself what he would not have found without the aid of the book.",
    "type": "article",
    "title": "24.4 default assumptions",
    "tags": [
      {
        "score": 0.7904117107391357,
        "sentiment": -0.775,
        "count": 6,
        "label": "default",
        "uri": "https://diffbot.com/entity/XXI5-w-afMpq0Q__GOebiOQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5479701161384583,
        "sentiment": 0,
        "count": 1,
        "label": "John the Apostle",
        "uri": "https://diffbot.com/entity/PMMsc4KA4PjuDLjx_O0LE0w",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.5304394364356995,
        "sentiment": 0,
        "count": 2,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.516743540763855,
        "sentiment": 0.302,
        "count": 1,
        "label": "What Really Matters",
        "uri": "https://diffbot.com/entity/XdFFtD86pPVakEct9cff21Q",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 170390716837,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 150033564055,
    "gburl": "http://aurellem.org/society-of-mind/som-24.4.html-diffbotxyz3931962444",
    "lastCrawlTimeUTC": 1588759493,
    "timestamp": "Wed, 06 May 2020 10:04:53 GMT"
  },
  {
    "sentiment": -0.853,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-594412945",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-26.6.html",
    "html": "<p>Part of what a sentence means depends upon its separate words, and part depends on how those words are arranged.</p>\n<p>Round squares steal honestly. Honestly steal squares round.</p>\n<p>What makes these seem so different in character, when both use the very same words? I'll argue that this is because your language-agency, immediately upon hearing the first word-string, knows exactly what to do with it because it fits a well-established sentence-frame. The second string fits no familiar form at all. But how do we fit those sentence-frames? We'll come to that presently, but for the moment, let's simply assume that our young listener has somehow come to classify words into various types, like nouns, adjectives, verbs, and adverbs. (We'll ignore the fact that children go through other stages before they use words as adults do.) Then our first string of words has this form:</p>\n<p>Adjective Noun Verb Adverb Now we'll suppose our listener has learned a specific recognition- frame that is activated on hearing this string of particular types of words. This frame then executes a special process script that makes the following assignments to the terminals of a Trans-frame. The neme for <em>steal</em> is assigned to the Trans-frame's Action terminal, while the neme for <em>squares</em> is attached to the Actor terminal. The frame then activates scripts that modify the action <em>steal</em> by applying to it the neme for <em>honestly</em> and modify the object <em>squares</em> by applying to it the neme for <em>round.</em> Up to this point, everything works smoothly: the language-agency has found a use for every word. We have special names for the strings of words that we process with such fluency: we call them <em>phrases</em> or <em>sentences.</em></p>\n<p>A word-string seems <em>grammatical</em> if all its words fit quickly and easily into frames that connect suitably to one another.</p>\n<p>However, at this point some serious conflicts start to appear within some other agencies because of certain incompatibilities. The frame for <em>steal</em> requires its Actor to be animate. A square can't steal, because it's not alive! Besides, the frame for <em>steal</em> expects an act that's reprehensible, and that clashes with the modifier for <em>honestly.</em> If that weren't bad enough, our agency for describing shape can't tolerate the polynemes for <em>round</em> and <em>square</em> when both are activated at the same time. It doesn't matter that our sentence is grammatical: so much turmoil is set up that most of its meaning cancels out and we regard it as <em>nonsense.</em> But it is important to recognize that the distinction between sense and nonsense is only partly a matter of grammar, for consider what happens when you hear these three words:</p>\n<p>thief -- -- careless -- -- prison --</p>\n<p>Although these do not establish any single well-formed grammar- frame, they activate some word-sense nemes that skip past all our grammar-forms to fit a familiar story-frame, a moral tale about a thief who's caught and reaps a just reward. Ungrammatical expressions can frequently be meaningful when they lead to clear and stable mental states. Grammar is the servant of language, not the master.</p>",
    "text": "Part of what a sentence means depends upon its separate words, and part depends on how those words are arranged.\nRound squares steal honestly. Honestly steal squares round.\nWhat makes these seem so different in character, when both use the very same words? I'll argue that this is because your language-agency, immediately upon hearing the first word-string, knows exactly what to do with it because it fits a well-established sentence-frame. The second string fits no familiar form at all. But how do we fit those sentence-frames? We'll come to that presently, but for the moment, let's simply assume that our young listener has somehow come to classify words into various types, like nouns, adjectives, verbs, and adverbs. (We'll ignore the fact that children go through other stages before they use words as adults do.) Then our first string of words has this form:\nAdjective Noun Verb Adverb Now we'll suppose our listener has learned a specific recognition- frame that is activated on hearing this string of particular types of words. This frame then executes a special process script that makes the following assignments to the terminals of a Trans-frame. The neme for steal is assigned to the Trans-frame's Action terminal, while the neme for squares is attached to the Actor terminal. The frame then activates scripts that modify the action steal by applying to it the neme for honestly and modify the object squares by applying to it the neme for round. Up to this point, everything works smoothly: the language-agency has found a use for every word. We have special names for the strings of words that we process with such fluency: we call them phrases or sentences.\nA word-string seems grammatical if all its words fit quickly and easily into frames that connect suitably to one another.\nHowever, at this point some serious conflicts start to appear within some other agencies because of certain incompatibilities. The frame for steal requires its Actor to be animate. A square can't steal, because it's not alive! Besides, the frame for steal expects an act that's reprehensible, and that clashes with the modifier for honestly. If that weren't bad enough, our agency for describing shape can't tolerate the polynemes for round and square when both are activated at the same time. It doesn't matter that our sentence is grammatical: so much turmoil is set up that most of its meaning cancels out and we regard it as nonsense. But it is important to recognize that the distinction between sense and nonsense is only partly a matter of grammar, for consider what happens when you hear these three words:\nthief -- -- careless -- -- prison --\nAlthough these do not establish any single well-formed grammar- frame, they activate some word-sense nemes that skip past all our grammar-forms to fit a familiar story-frame, a moral tale about a thief who's caught and reaps a just reward. Ungrammatical expressions can frequently be meaningful when they lead to clear and stable mental states. Grammar is the servant of language, not the master.",
    "type": "article",
    "title": "26.6 sentence and nonsense",
    "tags": [
      {
        "score": 0.6220982670783997,
        "sentiment": -0.416,
        "count": 3,
        "label": "literary nonsense",
        "uri": "https://diffbot.com/entity/XbrjlOAduMG2De4T9ckGcBw",
        "rdfTypes": ["http://dbpedia.org/ontology/Work"]
      },
      {
        "score": 0.5149577856063843,
        "sentiment": -0.367,
        "count": 2,
        "label": "actor",
        "uri": "https://diffbot.com/entity/X9qXzfDyuMNOCw-kCz5CNyQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 192175980981,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 23599399335,
    "gburl": "http://aurellem.org/society-of-mind/som-26.6.html-diffbotxyz2940331766",
    "lastCrawlTimeUTC": 1588759519,
    "timestamp": "Wed, 06 May 2020 10:05:19 GMT"
  },
  {
    "sentiment": 0.997,
    "humanLanguage": "en",
    "diffbotUri": "article|3|2059363163",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-17.html",
    "html": "<blockquote> To the child, Nature gives various means of rectifying any mistakes he may commit respecting the salutary or hurtful qualities of the objects which surround him. On every occasion his judgments are corrected by experience; want and pain are the necessary consequences arising from false judgment; gratification and pleasure are produced by judging aright. Under such masters, we cannot fail to become well informed; and we soon learn to reason justly, when want and pain are the necessary consequences of a contrary conduct.<br><br> In the study and practice of the sciences it is quite different; the false judgments we form neither affect our existence nor our welfare; and we are not forced by any physical necessity to correct them. Imagination, on the contrary, which is ever wandering beyond the bounds of truth, joined to self-love and that self-confidence we are so apt to indulge, prompt us to draw conclusions which are not immediately derived from facts&hellip; &mdash;A. Lavoisier </blockquote>",
    "text": "To the child, Nature gives various means of rectifying any mistakes he may commit respecting the salutary or hurtful qualities of the objects which surround him. On every occasion his judgments are corrected by experience; want and pain are the necessary consequences arising from false judgment; gratification and pleasure are produced by judging aright. Under such masters, we cannot fail to become well informed; and we soon learn to reason justly, when want and pain are the necessary consequences of a contrary conduct.\nIn the study and practice of the sciences it is quite different; the false judgments we form neither affect our existence nor our welfare; and we are not forced by any physical necessity to correct them. Imagination, on the contrary, which is ever wandering beyond the bounds of truth, joined to self-love and that self-confidence we are so apt to indulge, prompt us to draw conclusions which are not immediately derived from facts\u2026 \u2014A. Lavoisier",
    "type": "article",
    "title": "17 development",
    "tags": [
      {
        "score": 0.5919936299324036,
        "sentiment": 0.909,
        "count": 1,
        "label": "software development",
        "uri": "https://diffbot.com/entity/XMHzColFvMMKEKx6t-BCv5A"
      },
      {
        "score": 0.5156311988830566,
        "sentiment": 0.97,
        "count": 1,
        "label": "nature",
        "uri": "https://diffbot.com/entity/Xw07HHhZvNxSqarZzowd-Og"
      }
    ],
    "docId": 260469539220,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 1651097986,
    "gburl": "http://aurellem.org/society-of-mind/som-17.html-diffbotxyz1965354327",
    "lastCrawlTimeUTC": 1588759586,
    "timestamp": "Wed, 06 May 2020 10:06:26 GMT"
  },
  {
    "sentiment": 0.678,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-838911870",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-6.10.html",
    "html": "<p>There is no singularly real world of thought; each mind evolves its own internal universe. The worlds of thought that we appear to like the best are those where goals and actions seem to mesh in regions large enough to spend our lives in &mdash; and thus become a Buddhist, or Republican, or poet, or topologist. Some mental starting points grow into great, coherent continents. In certain parts of mathematics, science, and philosophy, a relatively few but clear ideas may lead into an endless realm of complex yet consistent new structures. Yet even in mathematics, a handful of seemingly innocent rules can lead to complications far beyond our grasp. Thus we feel we understand perfectly the rules of addition and multiplication &mdash; yet when we mix them together, we encounter problems about prime numbers that have remained unsolved for centuries.</p>\n<p>Minds also make up pleasant worlds of practical affairs &mdash; which work because we make them work, by putting things in order there. In the physical realm, we keep our books and clothes in self-made shelves and cabinets &mdash; thus building artificial boundaries to keep our things from interacting very much. Similarly, in mental realms, we make up countless artificial schemes to force things to seem orderly, by specifying legal codes, grammar rules and traffic laws. When growing up in such a world, it all seems right and natural &mdash; and only scholars and historians recall the mass of precedents and failed experiments it took to make it work so well. These <em>natural</em> worlds are actually more complex than the technical worlds of philosophy. They're far too vast to comprehend &mdash; except where we impose on them the rules we make.</p>\n<p>There is also a different and more sinister way to make the world seem orderly, in which the mind has merely found a way to simplify itself. This is what we must suspect whenever some idea seems to explain too much. Perhaps no problem was actually solved at all; instead, the mind has merely found some secondary pathway in the brain, through which one can mechanically dislodge each doubt and difference from its rightful place! This may be what happens in some of those experiences that leave a person with a sense of revelation &mdash; in a state in which no doubts remain, or with a vision of astounding clarity &mdash; yet unable to recount any details. Some accident of mental stress has temporarily suppressed the capacity to question, doubt, or probe. One remembers that no questions went unanswered but forgets that none were asked! One can acquire certainty only by amputating inquiry.</p>\n<p>When victims of these incidents become compelled to recapture them, their lives and personalities are sometimes permanently changed. Then others, seeing the radiance in their eyes and hearing of the glory to be found, are drawn to follow them. But to offer hospitality to paradox is like leaning toward a precipice. You can find out what it is like by falling in, but you may not be able to fall out again. Once contradiction finds a home, few minds can spurn the sense-destroying force of slogans such as <em>all is one.</em></p>",
    "text": "There is no singularly real world of thought; each mind evolves its own internal universe. The worlds of thought that we appear to like the best are those where goals and actions seem to mesh in regions large enough to spend our lives in \u2014 and thus become a Buddhist, or Republican, or poet, or topologist. Some mental starting points grow into great, coherent continents. In certain parts of mathematics, science, and philosophy, a relatively few but clear ideas may lead into an endless realm of complex yet consistent new structures. Yet even in mathematics, a handful of seemingly innocent rules can lead to complications far beyond our grasp. Thus we feel we understand perfectly the rules of addition and multiplication \u2014 yet when we mix them together, we encounter problems about prime numbers that have remained unsolved for centuries.\nMinds also make up pleasant worlds of practical affairs \u2014 which work because we make them work, by putting things in order there. In the physical realm, we keep our books and clothes in self-made shelves and cabinets \u2014 thus building artificial boundaries to keep our things from interacting very much. Similarly, in mental realms, we make up countless artificial schemes to force things to seem orderly, by specifying legal codes, grammar rules and traffic laws. When growing up in such a world, it all seems right and natural \u2014 and only scholars and historians recall the mass of precedents and failed experiments it took to make it work so well. These natural worlds are actually more complex than the technical worlds of philosophy. They're far too vast to comprehend \u2014 except where we impose on them the rules we make.\nThere is also a different and more sinister way to make the world seem orderly, in which the mind has merely found a way to simplify itself. This is what we must suspect whenever some idea seems to explain too much. Perhaps no problem was actually solved at all; instead, the mind has merely found some secondary pathway in the brain, through which one can mechanically dislodge each doubt and difference from its rightful place! This may be what happens in some of those experiences that leave a person with a sense of revelation \u2014 in a state in which no doubts remain, or with a vision of astounding clarity \u2014 yet unable to recount any details. Some accident of mental stress has temporarily suppressed the capacity to question, doubt, or probe. One remembers that no questions went unanswered but forgets that none were asked! One can acquire certainty only by amputating inquiry.\nWhen victims of these incidents become compelled to recapture them, their lives and personalities are sometimes permanently changed. Then others, seeing the radiance in their eyes and hearing of the glory to be found, are drawn to follow them. But to offer hospitality to paradox is like leaning toward a precipice. You can find out what it is like by falling in, but you may not be able to fall out again. Once contradiction finds a home, few minds can spurn the sense-destroying force of slogans such as all is one.",
    "type": "article",
    "title": "6.10 worlds out of mind",
    "tags": [
      {
        "score": 0.5847358703613281,
        "sentiment": 0,
        "count": 2,
        "label": "mathematics",
        "uri": "https://diffbot.com/entity/XsLHF-RZyOomEmARsR2YOgw",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5772516131401062,
        "sentiment": 0,
        "count": 1,
        "label": "Republican Party",
        "uri": "https://diffbot.com/entity/BQux7TYFDMgO6n_OByeSXzg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/PoliticalParty",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5763054490089417,
        "sentiment": 0,
        "count": 2,
        "label": "philosophy",
        "uri": "https://diffbot.com/entity/XL-OhgLG4MhK7hLV7pshwOQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5239855051040649,
        "sentiment": 0,
        "count": 1,
        "label": "Buddhism",
        "uri": "https://diffbot.com/entity/XfoOKxu8ZPhWH12YEGVaDuA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/Ideology",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 9517318581,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 261758239107,
    "gburl": "http://aurellem.org/society-of-mind/som-6.10.html-diffbotxyz3623130081",
    "lastCrawlTimeUTC": 1588759379,
    "timestamp": "Wed, 06 May 2020 10:02:59 GMT"
  },
  {
    "sentiment": 0,
    "humanLanguage": "en",
    "diffbotUri": "article|3|758270497",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-29.5.html",
    "html": "<p>What makes our minds form many separate mental realms, instead of attempting, as scientists do, to see all aspects of the world in a unified way? Because, at least in everyday life, the latter simply isn't practical. Consider how different the rules are within the physical and social realms. If you want your furniture inside a different room, you normally would push it there. But when you want to move your party guests, it would be rude to push them there. Contrast the principles of physics and geometry with those we use in the social realm. In the physical realm, the rules seem very orderly:</p>\n<p>-- A stationary object stays where it is unless another object pushes it. --- A moving object continues in its course until some external force makes it stop. ----All unsupported objects start to fall.</p>\n<p>-----No two things can occupy the same location. -----Etc.</p>\n<p>These principles seem clear to us &mdash; but infants cannot appreciate them until they've built up ways to represent ingredients like <em>thing,</em> <em>shape,</em> <em>place,</em> <em>move,</em> and <em>near.</em> It takes each child many years to develop these abilities.</p>\n<p>Our comprehensions of social acts are based on different principles. When an ordinary object moves around, we usually see an obvious cause; most likely, another object pushed it. But when we see a person move, we rarely see the cause at all &mdash; because it's buried in a brain. In predicting how a person will react to an expression or gesture, we have little use for physical properties like color, shape, or place. Instead, we employ almost entirely different conceptions. To guess the outcome of a social interaction, we have to be able to represent each person's mental state &mdash; and to do that, we must develop concepts about traits, dispositions, motives, and plans. The concepts that serve so well for physical objects are of little help within the social realm &mdash; and vice versa.</p>\n<p>When normal children start to talk, among the early aspects of their speech are words that distinguish animate things. Frequently, a child will use a single expression for all kinds of animals, and for everything else that can move by itself &mdash; for example, an automobile. According to our view of things, this surely is no accident.</p>\n<p>To adults, the laws that govern the physical world seem simpler and more orderly than those that apply to human events. Does this mean that for infants, too, it should be easier first to master the physical world and later to proceed toward social and psychological understanding? No. Paradoxically, the social realm is initially the easier! Imagine that an infant wants a certain toy and that there's a sympathetic person near. The easiest thing is to make a request &mdash; that is, a gesture, smile, or cry &mdash; and this will probably achieve the goal. It would be far more difficult for the infant to coordinate all the complicated machinery for planning and executing the trajectory for propelling the object from where it is to where the infant wishes it to be. From the point of view of a physically helpless infant, the social realm is by far the simpler one.</p>",
    "text": "What makes our minds form many separate mental realms, instead of attempting, as scientists do, to see all aspects of the world in a unified way? Because, at least in everyday life, the latter simply isn't practical. Consider how different the rules are within the physical and social realms. If you want your furniture inside a different room, you normally would push it there. But when you want to move your party guests, it would be rude to push them there. Contrast the principles of physics and geometry with those we use in the social realm. In the physical realm, the rules seem very orderly:\n-- A stationary object stays where it is unless another object pushes it. --- A moving object continues in its course until some external force makes it stop. ----All unsupported objects start to fall.\n-----No two things can occupy the same location. -----Etc.\nThese principles seem clear to us \u2014 but infants cannot appreciate them until they've built up ways to represent ingredients like thing, shape, place, move, and near. It takes each child many years to develop these abilities.\nOur comprehensions of social acts are based on different principles. When an ordinary object moves around, we usually see an obvious cause; most likely, another object pushed it. But when we see a person move, we rarely see the cause at all \u2014 because it's buried in a brain. In predicting how a person will react to an expression or gesture, we have little use for physical properties like color, shape, or place. Instead, we employ almost entirely different conceptions. To guess the outcome of a social interaction, we have to be able to represent each person's mental state \u2014 and to do that, we must develop concepts about traits, dispositions, motives, and plans. The concepts that serve so well for physical objects are of little help within the social realm \u2014 and vice versa.\nWhen normal children start to talk, among the early aspects of their speech are words that distinguish animate things. Frequently, a child will use a single expression for all kinds of animals, and for everything else that can move by itself \u2014 for example, an automobile. According to our view of things, this surely is no accident.\nTo adults, the laws that govern the physical world seem simpler and more orderly than those that apply to human events. Does this mean that for infants, too, it should be easier first to master the physical world and later to proceed toward social and psychological understanding? No. Paradoxically, the social realm is initially the easier! Imagine that an infant wants a certain toy and that there's a sympathetic person near. The easiest thing is to make a request \u2014 that is, a gesture, smile, or cry \u2014 and this will probably achieve the goal. It would be far more difficult for the infant to coordinate all the complicated machinery for planning and executing the trajectory for propelling the object from where it is to where the infant wishes it to be. From the point of view of a physically helpless infant, the social realm is by far the simpler one.",
    "type": "article",
    "title": "29.5 the problem of unity",
    "tags": [
      {
        "score": 0.6233044862747192,
        "sentiment": 0.712,
        "count": 7,
        "label": "social change",
        "uri": "https://diffbot.com/entity/XcjlvlDYXMTKRsmzWSdKLTA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.556311309337616,
        "sentiment": 0,
        "count": 1,
        "label": "scientist",
        "uri": "https://diffbot.com/entity/XujaIGUOaNJ2JsSIPQozUng",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5269602537155151,
        "sentiment": 0,
        "count": 4,
        "label": "baby",
        "uri": "https://diffbot.com/entity/X4MZ_yLLbO0ueNL-zoshUhg",
        "rdfTypes": [
          "http://dbpedia.org/ontology/TopicalConcept",
          "http://dbpedia.org/ontology/AcademicSubject",
          "http://dbpedia.org/ontology/Activity",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5059663653373718,
        "sentiment": 0.383,
        "count": 2,
        "label": "nurse assistant",
        "uri": "https://diffbot.com/entity/XdMFIB2kmMXC-Di6prNElJA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 100547920310,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 68001808794,
    "gburl": "http://aurellem.org/society-of-mind/som-29.5.html-diffbotxyz811115626",
    "lastCrawlTimeUTC": 1588759416,
    "timestamp": "Wed, 06 May 2020 10:03:36 GMT"
  },
  {
    "sentiment": 0.169,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-510404213",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-2.6.html",
    "html": "<p>It's mainly when our other systems start to fail that we engage the special agencies involved with what we call <em>consciousness.</em> Accordingly, we're more aware of simple processes that don't work well than of complex ones that work flawlessly. This means that we cannot trust our offhand judgments about which of the things we do are simple, and which require complicated machinery. Most times, each portion of the mind can only sense how quietly the other portions do their jobs.</p>",
    "text": "It's mainly when our other systems start to fail that we engage the special agencies involved with what we call consciousness. Accordingly, we're more aware of simple processes that don't work well than of complex ones that work flawlessly. This means that we cannot trust our offhand judgments about which of the things we do are simple, and which require complicated machinery. Most times, each portion of the mind can only sense how quietly the other portions do their jobs.",
    "type": "article",
    "title": "2.6 confusion",
    "docId": 121154765192,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 124502049165,
    "gburl": "http://aurellem.org/society-of-mind/som-2.6.html-diffbotxyz1347996834",
    "lastCrawlTimeUTC": 1588759439,
    "timestamp": "Wed, 06 May 2020 10:03:59 GMT"
  },
  {
    "sentiment": -0.859,
    "images": [
      {
        "naturalHeight": 106,
        "width": 308,
        "diffbotUri": "image|3|-1220774432",
        "url": "http://aurellem.org/society-of-mind/illus/ch19/19-4.png",
        "naturalWidth": 308,
        "primary": true,
        "height": 106
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-136556747",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-19.6.html",
    "html": "<p>When we see an apple, how do we know it as an apple? How do we recognize a friend &mdash; or even know when we're seeing a person? How do we recognize things? The simplest way to recognize something is to verify that it has certain properties. To recognize an apple, in many circumstances, it might suffice to look for something that is red AND round AND apple-sized. In order to do that, we need a kind of agent that detects when all three conditions occur at once. The simplest form of this would be an agent that becomes active whenever all three of its inputs are active.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch19/19-4.png\"/></figure>\n<p>We can use AND-agents to do many kinds of recognition, but the idea also has serious limitations. If you tried to recognize a chair that way, you would usually fail, if you insisted on finding <em>four legs AND a seat AND a back.</em> You scarcely ever see four legs of a chair at the same time, since at least one leg is usually out of view. Besides, if someone's sitting in the chair, you often cannot see the seat at all. In real life, no recognition-scheme will always work if it's based on absolutely perfect evidence. A more judicious scheme would not demand that every feature of a chair be seen; instead, it would only <em>weigh the evidence</em> that a chair is present. For example, we could make a chair-agent that becomes active whenever five or more of our six chair features are in sight:</p>\n<p>This scheme, too, will make mistakes. It will miss the chair if too many features are out of sight. It will mistake other objects for chairs if those features are present but in the wrong arrangement &mdash; for example, if all four legs are attached to the same side of a <em>seat.</em> Indeed, it usually won't suffice merely to verify that all the required parts are there &mdash; one also has to verify their dimensions and relationships; otherwise our recognizer would not distinguish a chair from a couch or even from a bunch of boards and sticks. Failure to verify relationships is the basis of a certain type of nonsense joke:</p>\n<p>What has eight legs and flies? --- A string quartet on a foreign tour.</p>",
    "text": "When we see an apple, how do we know it as an apple? How do we recognize a friend \u2014 or even know when we're seeing a person? How do we recognize things? The simplest way to recognize something is to verify that it has certain properties. To recognize an apple, in many circumstances, it might suffice to look for something that is red AND round AND apple-sized. In order to do that, we need a kind of agent that detects when all three conditions occur at once. The simplest form of this would be an agent that becomes active whenever all three of its inputs are active.\nWe can use AND-agents to do many kinds of recognition, but the idea also has serious limitations. If you tried to recognize a chair that way, you would usually fail, if you insisted on finding four legs AND a seat AND a back. You scarcely ever see four legs of a chair at the same time, since at least one leg is usually out of view. Besides, if someone's sitting in the chair, you often cannot see the seat at all. In real life, no recognition-scheme will always work if it's based on absolutely perfect evidence. A more judicious scheme would not demand that every feature of a chair be seen; instead, it would only weigh the evidence that a chair is present. For example, we could make a chair-agent that becomes active whenever five or more of our six chair features are in sight:\nThis scheme, too, will make mistakes. It will miss the chair if too many features are out of sight. It will mistake other objects for chairs if those features are present but in the wrong arrangement \u2014 for example, if all four legs are attached to the same side of a seat. Indeed, it usually won't suffice merely to verify that all the required parts are there \u2014 one also has to verify their dimensions and relationships; otherwise our recognizer would not distinguish a chair from a couch or even from a bunch of boards and sticks. Failure to verify relationships is the basis of a certain type of nonsense joke:\nWhat has eight legs and flies? --- A string quartet on a foreign tour.",
    "type": "article",
    "title": "19.6 recognizers",
    "docId": 16640426414,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 9058058627,
    "gburl": "http://aurellem.org/society-of-mind/som-19.6.html-diffbotxyz2845887636",
    "lastCrawlTimeUTC": 1588759348,
    "timestamp": "Wed, 06 May 2020 10:02:28 GMT"
  },
  {
    "sentiment": -0.808,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1343756392",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-16.5.html",
    "html": "<p>Few of the schemes we've discussed would actually work if they were built exactly as they were described. Most of them would soon break down because virtually all their agents would become engaged into unconstrained activity. Suppose each typical agent tends to arouse several others. Then each of those would turn on several more &mdash; and each of those would turn on yet others; the activity would spread faster than a forest fire. But all that activity would accomplish nothing, since all those agents would interfere with each other and none of them could gain control of the resources they need. Indeed, this is more or less what happens in an attack of epilepsy.</p>\n<p>Similar problems arise in every biological system. Each simple principle or mechanism must be controlled to operate within some limited range. Even little groups of genes embody schemes to regulate the quantities of proteins they cause to be manufactured inside every cell. We find the same pattern repeated on every scale. Every biological tissue, organ, and system is regulated by several kinds of control mechanisms, and wherever these fail we find disease. What normally protects our brains from such avalanches of activity? The cross- exclusion scheme is probably the most usual way to regulate the levels of activities within our agencies. But there are also several other frequently encountered schemes to prevent explosions.</p>\n<p>Conservation: Force all activities to depend upon some substance or other kind of quantity of which only a certain amount is available. For example, we controlled our Snarc machine by setting a limit on the total electric current available to all the agents; this permitted only a few of them to be active at any particular moment. Negative Feedback: Supply a <em>summary</em> device that estimates the total activity in the agency and then broadcasts to that agency an <em>inhibitory</em> signal whose strength is in proportion to that total. This will tend to damp down incipient avalanches.</p>\n<p>Censors and Suppressors: The <em>conservation</em> and <em>feedback</em> schemes tend to be indiscriminate. Later we'll discuss methods that are more sensitive and versatile in learning to recognize &mdash; and then to avoid &mdash; specific patterns of activity that have led to trouble in the past.</p>\n<p>These methods are simple enough to be applied inside small societies, but they are not versatile enough to solve all the management difficulties that can arise in the more complex societies we need for learning to solve harder problems. Fortunately, systems built upon larger scales may be able to apply their own enhanced abilities to managing themselves &mdash; by formulating and solving their own self-regulation problems. In the next few sections we'll see how such capacities could grow in the course of several stages of development. Not all of this need happen independently inside each separate child's mind, because that child's family and cultural community can develop self-regulation schemes of great complexity. All human communities seem to work out policies for how their members ought to think, in forms that are thought of as common sense or as law, religion, or philosophy.</p>",
    "text": "Few of the schemes we've discussed would actually work if they were built exactly as they were described. Most of them would soon break down because virtually all their agents would become engaged into unconstrained activity. Suppose each typical agent tends to arouse several others. Then each of those would turn on several more \u2014 and each of those would turn on yet others; the activity would spread faster than a forest fire. But all that activity would accomplish nothing, since all those agents would interfere with each other and none of them could gain control of the resources they need. Indeed, this is more or less what happens in an attack of epilepsy.\nSimilar problems arise in every biological system. Each simple principle or mechanism must be controlled to operate within some limited range. Even little groups of genes embody schemes to regulate the quantities of proteins they cause to be manufactured inside every cell. We find the same pattern repeated on every scale. Every biological tissue, organ, and system is regulated by several kinds of control mechanisms, and wherever these fail we find disease. What normally protects our brains from such avalanches of activity? The cross- exclusion scheme is probably the most usual way to regulate the levels of activities within our agencies. But there are also several other frequently encountered schemes to prevent explosions.\nConservation: Force all activities to depend upon some substance or other kind of quantity of which only a certain amount is available. For example, we controlled our Snarc machine by setting a limit on the total electric current available to all the agents; this permitted only a few of them to be active at any particular moment. Negative Feedback: Supply a summary device that estimates the total activity in the agency and then broadcasts to that agency an inhibitory signal whose strength is in proportion to that total. This will tend to damp down incipient avalanches.\nCensors and Suppressors: The conservation and feedback schemes tend to be indiscriminate. Later we'll discuss methods that are more sensitive and versatile in learning to recognize \u2014 and then to avoid \u2014 specific patterns of activity that have led to trouble in the past.\nThese methods are simple enough to be applied inside small societies, but they are not versatile enough to solve all the management difficulties that can arise in the more complex societies we need for learning to solve harder problems. Fortunately, systems built upon larger scales may be able to apply their own enhanced abilities to managing themselves \u2014 by formulating and solving their own self-regulation problems. In the next few sections we'll see how such capacities could grow in the course of several stages of development. Not all of this need happen independently inside each separate child's mind, because that child's family and cultural community can develop self-regulation schemes of great complexity. All human communities seem to work out policies for how their members ought to think, in forms that are thought of as common sense or as law, religion, or philosophy.",
    "type": "article",
    "title": "16.5 avalanche effects",
    "tags": [
      {
        "score": 0.7525576949119568,
        "sentiment": 0,
        "count": 1,
        "label": "avalanche",
        "uri": "https://diffbot.com/entity/XBoohY1NLMoCuwCIp0FoAVg"
      },
      {
        "score": 0.5453913807868958,
        "sentiment": 0.16,
        "count": 2,
        "label": "conservation",
        "uri": "https://diffbot.com/entity/XpG2KXYdhM4GyO2TOjYJQxg"
      },
      {
        "score": 0.5312113761901855,
        "sentiment": -0.794,
        "count": 4,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.5194088220596313,
        "sentiment": -0.37,
        "count": 1,
        "label": "Negative Feedback",
        "uri": "https://diffbot.com/entity/OOV-ORtjBPbW96guUoKgdbA",
        "rdfTypes": ["http://dbpedia.org/ontology/Organisation"]
      }
    ],
    "docId": 29352149377,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 37320655294,
    "gburl": "http://aurellem.org/society-of-mind/som-16.5.html-diffbotxyz3169763652",
    "lastCrawlTimeUTC": 1588759264,
    "timestamp": "Wed, 06 May 2020 10:01:04 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|-262990619",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-13.3.html",
    "html": "<p>We normally assume that children see the same as we do and only lack our tricky muscle skills. But that doesn't explain why so many children produce this particular kind of drawing, nor why they seem so satisfied with them. In any case, this phenomenon makes it seem very unlikely that a child has a realistic, picturelike <em>image</em> in mind.</p>\n<p>Now let's consider a different idea. We'll suppose that the child does not have anything like a picture in mind, but only some network of relationships that various <em>features</em> must satisfy. For example, a child's <em>person-drawing</em> feature-network might consist of the following features and relations:</p>\n<p>To convert this description into an actual drawing, the child must employ some sort of <em>drawing procedure.</em> Here's one in which the process simply works its way down the feature list, like a little computer program:</p>\n<p>When the child starts to draw, the first item on the list is <em>large closed figure.</em> Since there isn't any such thing yet, the child draws one: that's the head. Next the eyes and mouth get drawn. But then, when it comes to drawing the body feature, step 2 of the procedure finds that a <em>large closed figure</em> has already been drawn. Accordingly, nothing new is required, and the procedure simply advances to step 3. As a result, the child goes on to attach the arms and legs to the feature that has been assigned to both the body and the head.</p>\n<p>An adult would never make such a <em>mistake,</em> since once some feature has been assigned to represent a head, that feature is thereafter regarded as <em>used up</em> or <em>occupied</em> and cannot represent anything else. But the child has less capacity or inclination for <em>keeping track.</em> Accordingly, since that <em>large closed figure</em> satisfies the description's requirements for both the head and the body &mdash; albeit at different moments of time &mdash; there is no cause for discontent. The little artist has satisfied all the conditions required by its description!</p>",
    "text": "We normally assume that children see the same as we do and only lack our tricky muscle skills. But that doesn't explain why so many children produce this particular kind of drawing, nor why they seem so satisfied with them. In any case, this phenomenon makes it seem very unlikely that a child has a realistic, picturelike image in mind.\nNow let's consider a different idea. We'll suppose that the child does not have anything like a picture in mind, but only some network of relationships that various features must satisfy. For example, a child's person-drawing feature-network might consist of the following features and relations:\nTo convert this description into an actual drawing, the child must employ some sort of drawing procedure. Here's one in which the process simply works its way down the feature list, like a little computer program:\nWhen the child starts to draw, the first item on the list is large closed figure. Since there isn't any such thing yet, the child draws one: that's the head. Next the eyes and mouth get drawn. But then, when it comes to drawing the body feature, step 2 of the procedure finds that a large closed figure has already been drawn. Accordingly, nothing new is required, and the procedure simply advances to step 3. As a result, the child goes on to attach the arms and legs to the feature that has been assigned to both the body and the head.\nAn adult would never make such a mistake, since once some feature has been assigned to represent a head, that feature is thereafter regarded as used up or occupied and cannot represent anything else. But the child has less capacity or inclination for keeping track. Accordingly, since that large closed figure satisfies the description's requirements for both the head and the body \u2014 albeit at different moments of time \u2014 there is no cause for discontent. The little artist has satisfied all the conditions required by its description!",
    "type": "article",
    "title": "13.3 seeing and believing",
    "docId": 29668344221,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 10715562429,
    "gburl": "http://aurellem.org/society-of-mind/som-13.3.html-diffbotxyz1576395397",
    "lastCrawlTimeUTC": 1588759289,
    "timestamp": "Wed, 06 May 2020 10:01:29 GMT"
  },
  {
    "date": "Tue, 10 Dec 2019 00:00:00 GMT",
    "sentiment": 0.715,
    "humanLanguage": "en",
    "estimatedDate": "Tue, 10 Dec 2019 00:00:00 GMT",
    "diffbotUri": "article|3|-1838389644",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-12.10.html",
    "html": "<p>Why can one build a tower or arch of stone or brick, but not of water, air, or sand? To answer that amounts to asking, <em>How do towers work?</em> But asking that seems quite perverse, because the answer seems so obvious: <em>Each block holds the next one up, and that's all there is to it!</em> As we've said before:</p>\n<p>An idea will seem self-evident &mdash; once you've forgotten learning it!</p>\n<p>We often use words like <em>insight</em> or <em>intuition</em> to talk about understandings that seem especially immediate. But it is bad psychology to assume that what seems <em>obvious</em> is therefore simple or self-evident. Many such things are done for us by huge, silent systems in our mind, built over long forgotten years of childhood. We rarely think about the giant engines we've developed for understanding space, which work so quietly that they leave no traces in our consciousness. How towers work is something everyone has known for so long that it seems odd to mention it:</p>\n<p>A tower's height depends only upon the heights of its parts! None of the other properties of the blocks matter &mdash; neither what they cost, nor where they've been, nor what you think of them. Only lifting counts &mdash; so we can build a tower by thinking only about actions that increase its height.</p>\n<p>This makes tower building easy, by letting us separate the basic building plan from all the small details. To build a tower of a certain height, just find enough <em>pieces of height</em> and stack them up by lifting actions. But towers have to stay up, too. So the next problem is to find actions we can take to make our tower stable. Here we can use a second, wonderful principle:</p>\n<p>A tower is stable if each block is properly centered on the last. Because of this, we can build a tower by first lifting each block vertically and then adjusting it horizontally.</p>\n<p>Notice that this second kind of action &mdash; adjusting for stability &mdash; requires only horizontal movements, which do not affect the tower's height at all. This explains why towers are so easy to build. To increase a tower's height, you need only vertical lifting actions. The second-rank goal, stability, requires only horizontal sliding motions, which don't interact with height at all &mdash; provided the blocks have horizontal surfaces. This lets us achieve our tower-building goal simply by doing <em>first things first.</em></p>\n<p>To adults it is no mystery that height and width are independent of each other. But this is not so evident in infancy: to understand the world of space and time, each child must make many such discoveries. Still, the division into Lifting and Sliding has a special importance; there are an infinity of ways to move around inside the world: how could a person ever learn them all? The answer: We don't need to learn them all, because we can learn to deal with each dimension separately. Lifting has a special significance because it isolates the vertical dimension from the others and relates it to ideas about balancing. The complementary operations of Sliding can then be divided into two remaining dimensions: either to push and pull or to move from side to side. One way to Lift and two ways to Slide makes three &mdash; and that is just enough to move around in a three- dimensional world!</p>",
    "text": "Why can one build a tower or arch of stone or brick, but not of water, air, or sand? To answer that amounts to asking, How do towers work? But asking that seems quite perverse, because the answer seems so obvious: Each block holds the next one up, and that's all there is to it! As we've said before:\nAn idea will seem self-evident \u2014 once you've forgotten learning it!\nWe often use words like insight or intuition to talk about understandings that seem especially immediate. But it is bad psychology to assume that what seems obvious is therefore simple or self-evident. Many such things are done for us by huge, silent systems in our mind, built over long forgotten years of childhood. We rarely think about the giant engines we've developed for understanding space, which work so quietly that they leave no traces in our consciousness. How towers work is something everyone has known for so long that it seems odd to mention it:\nA tower's height depends only upon the heights of its parts! None of the other properties of the blocks matter \u2014 neither what they cost, nor where they've been, nor what you think of them. Only lifting counts \u2014 so we can build a tower by thinking only about actions that increase its height.\nThis makes tower building easy, by letting us separate the basic building plan from all the small details. To build a tower of a certain height, just find enough pieces of height and stack them up by lifting actions. But towers have to stay up, too. So the next problem is to find actions we can take to make our tower stable. Here we can use a second, wonderful principle:\nA tower is stable if each block is properly centered on the last. Because of this, we can build a tower by first lifting each block vertically and then adjusting it horizontally.\nNotice that this second kind of action \u2014 adjusting for stability \u2014 requires only horizontal movements, which do not affect the tower's height at all. This explains why towers are so easy to build. To increase a tower's height, you need only vertical lifting actions. The second-rank goal, stability, requires only horizontal sliding motions, which don't interact with height at all \u2014 provided the blocks have horizontal surfaces. This lets us achieve our tower-building goal simply by doing first things first.\nTo adults it is no mystery that height and width are independent of each other. But this is not so evident in infancy: to understand the world of space and time, each child must make many such discoveries. Still, the division into Lifting and Sliding has a special importance; there are an infinity of ways to move around inside the world: how could a person ever learn them all? The answer: We don't need to learn them all, because we can learn to deal with each dimension separately. Lifting has a special significance because it isolates the vertical dimension from the others and relates it to ideas about balancing. The complementary operations of Sliding can then be divided into two remaining dimensions: either to push and pull or to move from side to side. One way to Lift and two ways to Slide makes three \u2014 and that is just enough to move around in a three- dimensional world!",
    "type": "article",
    "title": "12.10 how towers work",
    "tags": [
      {
        "score": 0.5503236651420593,
        "sentiment": 0.434,
        "count": 1,
        "label": "First Things First",
        "uri": "https://diffbot.com/entity/X793VWZptP2yJe0LamrwQGQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/WrittenWork"
        ]
      }
    ],
    "docId": 209466048933,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 142387462545,
    "gburl": "http://aurellem.org/society-of-mind/som-12.10.html-diffbotxyz1874200211",
    "lastCrawlTimeUTC": 1588759315,
    "timestamp": "Wed, 06 May 2020 10:01:55 GMT"
  },
  {
    "sentiment": 0.964,
    "images": [
      {
        "naturalHeight": 192,
        "width": 400,
        "diffbotUri": "image|3|-433975519",
        "url": "http://aurellem.org/society-of-mind/illus/ch24/24-5.png",
        "naturalWidth": 400,
        "primary": true,
        "height": 192
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|-1945827705",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-24.8.html",
    "html": "<p>Now that we've seen how <em>picture-frames</em> could represent memories of spatial arrangements, let's ask how we actually build such frames. We'll use the same technique that we used to build Trans-frames, except for one small change. To make a picture-frame, we'll simply replace the pronomes of our Trans-frame scheme by a set of nine direction-nemes! The diagram below also includes an agent to serve for turning on the frame itself.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch24/24-5.png\"/></figure>\n<p>To apply the picture-frames idea to how our vision-systems work, imagine that you're looking at some real-world scene. Your eyes move in various directions, controlled in some way by direction-nemes. Now suppose that every time you move your eyes, the same direction-nemes also activate the K-lines attached to the corresponding terminals of a certain vision-frame. Suppose, also, that those K-lines are ready to form new memories. Then each time you look in a different direction, your vision-system will describe what you see &mdash; and the corresponding K-line will record what you see when you look in that direction!</p>\n<p>Now suppose that the same frame is activated at some later date &mdash; but this time by means of memory and not from looking at some scene. Then, as any of your agencies conceives of looking in a certain direction, the thought itself will involve the activation of the corresponding direction-neme; then, before you have a chance to think of anything else, the corresponding K-line will be aroused. This creates a most remarkable effect:</p>\n<p>Whichever way your <em>mind's eye</em> looks, you'll seem to see the corresponding aspect of the scene. You will experience an almost perfect <em>simulus</em> of being there!</p>\n<p>How <em>real</em> could such a recollection seem? In principle, it could even seem as real as vision itself &mdash; since it could make you seem to sense not only how an object looks, but also how it tastes and feels. Shortly we'll see how this could yield not merely the sense of seeing a scene, but also the sense of being able to move around inside it.</p>",
    "text": "Now that we've seen how picture-frames could represent memories of spatial arrangements, let's ask how we actually build such frames. We'll use the same technique that we used to build Trans-frames, except for one small change. To make a picture-frame, we'll simply replace the pronomes of our Trans-frame scheme by a set of nine direction-nemes! The diagram below also includes an agent to serve for turning on the frame itself.\nTo apply the picture-frames idea to how our vision-systems work, imagine that you're looking at some real-world scene. Your eyes move in various directions, controlled in some way by direction-nemes. Now suppose that every time you move your eyes, the same direction-nemes also activate the K-lines attached to the corresponding terminals of a certain vision-frame. Suppose, also, that those K-lines are ready to form new memories. Then each time you look in a different direction, your vision-system will describe what you see \u2014 and the corresponding K-line will record what you see when you look in that direction!\nNow suppose that the same frame is activated at some later date \u2014 but this time by means of memory and not from looking at some scene. Then, as any of your agencies conceives of looking in a certain direction, the thought itself will involve the activation of the corresponding direction-neme; then, before you have a chance to think of anything else, the corresponding K-line will be aroused. This creates a most remarkable effect:\nWhichever way your mind's eye looks, you'll seem to see the corresponding aspect of the scene. You will experience an almost perfect simulus of being there!\nHow real could such a recollection seem? In principle, it could even seem as real as vision itself \u2014 since it could make you seem to sense not only how an object looks, but also how it tastes and feels. Shortly we'll see how this could yield not merely the sense of seeing a scene, but also the sense of being able to move around inside it.",
    "type": "article",
    "title": "24.8 how picture-frames work",
    "tags": [
      {
        "score": 0.6295510530471802,
        "sentiment": 0.24,
        "count": 2,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5935177803039551,
        "sentiment": 0.622,
        "count": 2,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      }
    ],
    "docId": 241342464410,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 239720219041,
    "gburl": "http://aurellem.org/society-of-mind/som-24.8.html-diffbotxyz1058943790",
    "lastCrawlTimeUTC": 1588759240,
    "timestamp": "Wed, 06 May 2020 10:00:40 GMT"
  },
  {
    "author": "Marvin Minsky",
    "humanLanguage": "unknown",
    "authorUrl": "http://web.media.mit.edu/~minsky",
    "diffbotUri": "article|3|-234103372",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind",
    "text": "",
    "type": "article",
    "title": "The Society of Mind",
    "resolvedPageUrl": "http://aurellem.org/society-of-mind/",
    "authors": [
      { "name": "Marvin\n    Minsky", "link": "web.media.mit.edu/~minsky" }
    ],
    "docId": 22133932418,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 58896449928,
    "gburl": "http://aurellem.org/society-of-mind-diffbotxyz3296497013",
    "lastCrawlTimeUTC": 1588759120,
    "timestamp": "Wed, 06 May 2020 09:58:40 GMT"
  },
  {
    "sentiment": 0,
    "images": [
      {
        "naturalHeight": 145,
        "width": 458,
        "diffbotUri": "image|3|-1611436203",
        "url": "http://aurellem.org/society-of-mind/illus/ch8/8-7.png",
        "naturalWidth": 458,
        "primary": true,
        "height": 145
      }
    ],
    "humanLanguage": "en",
    "diffbotUri": "article|3|336023812",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-8.8.html",
    "html": "<p>Yesterday, you watched Jack fly his kite. How do you remember that today? One answer would be, <em>Remembering it is much like seeing it again.</em> But yesterday, when you recognized that kite, you didn't really see it as something wholly new. The fact that you recognized it as a <em>kite</em> yesterday means that you already saw that kite in terms of even older memories.</p>\n<p>This suggests two ways to make new memories of what you saw a moment ago. One scheme is shown to the left below: you simply connect a new K-line to all the agents that were recently active in your mind. The other way to make that memory is shown in the diagram to the right below: instead of attaching the new K-line to that whole multitude of separate agents, connect it only to whichever of your older K-lines were active recently. This will lead to a similar result since those K-lines were involved in arousing many of the agents that were active recently. This second scheme has two advantages: it is more economical, and it leads to forming memories as organized societies.</p>\n<figure><img src=\"http://aurellem.org/society-of-mind/illus/ch8/8-7.png\"/></figure>\n<p>Consider that when you realized Jack was flying a kite, this must have involved the use of K-lines &mdash; for <em>Jack</em> and <em>Fly</em> and <em>Kite</em> &mdash; that had been formed at earlier times and were aroused by the sight of Jack flying his kite. When those three K-lines were activated, each of them in turn activated hundreds or thousands of other agents. (Your state of mind, when seeing that scene, resulted from combinations both of agents aroused directly by your senses and of agents aroused indirectly by your recognitions.) Now, our left-hand memory-scheme would need an enormous number of connections to link all those agents to the new K-line. But our right-hand scheme would obtain much the same effect by attaching the new K-line to only three old K-lines! Yet when you reactivate that K-line at some later date, it will, in turn, arouse the same K-lines for Jack, Fly, Kite, and whichever other recognitions were involved. As a result, you will reexperience many of the same recognitions as before. To that extent, you will feel and act as though you were back in the same situation again.</p>\n<p>To be sure, these two types of memories would not produce exactly the same results. Our trick of connecting new K-lines to old ones will not recapture so many of the scene's precise, perceptual details.</p>\n<p>Instead, the kinds of mental states that this <em>hierarchical</em> type of memory produces will be based more on stereotypes and default assumptions than on actual perceptions. Specifically, you will tend to remember only what you recognized at the time. So something is lost &mdash; but there's a gain in exchange. These <em>K-line memory-trees</em> lose certain kinds of details, but they retain more traces of the origins of our ideas. These memory-trees might not serve quite so well if the original circumstances were exactly repeated. But that never happens, anyway &mdash; and the structured memories will be much more easily adapted to new situations.</p>",
    "text": "Yesterday, you watched Jack fly his kite. How do you remember that today? One answer would be, Remembering it is much like seeing it again. But yesterday, when you recognized that kite, you didn't really see it as something wholly new. The fact that you recognized it as a kite yesterday means that you already saw that kite in terms of even older memories.\nThis suggests two ways to make new memories of what you saw a moment ago. One scheme is shown to the left below: you simply connect a new K-line to all the agents that were recently active in your mind. The other way to make that memory is shown in the diagram to the right below: instead of attaching the new K-line to that whole multitude of separate agents, connect it only to whichever of your older K-lines were active recently. This will lead to a similar result since those K-lines were involved in arousing many of the agents that were active recently. This second scheme has two advantages: it is more economical, and it leads to forming memories as organized societies.\nConsider that when you realized Jack was flying a kite, this must have involved the use of K-lines \u2014 for Jack and Fly and Kite \u2014 that had been formed at earlier times and were aroused by the sight of Jack flying his kite. When those three K-lines were activated, each of them in turn activated hundreds or thousands of other agents. (Your state of mind, when seeing that scene, resulted from combinations both of agents aroused directly by your senses and of agents aroused indirectly by your recognitions.) Now, our left-hand memory-scheme would need an enormous number of connections to link all those agents to the new K-line. But our right-hand scheme would obtain much the same effect by attaching the new K-line to only three old K-lines! Yet when you reactivate that K-line at some later date, it will, in turn, arouse the same K-lines for Jack, Fly, Kite, and whichever other recognitions were involved. As a result, you will reexperience many of the same recognitions as before. To that extent, you will feel and act as though you were back in the same situation again.\nTo be sure, these two types of memories would not produce exactly the same results. Our trick of connecting new K-lines to old ones will not recapture so many of the scene's precise, perceptual details.\nInstead, the kinds of mental states that this hierarchical type of memory produces will be based more on stereotypes and default assumptions than on actual perceptions. Specifically, you will tend to remember only what you recognized at the time. So something is lost \u2014 but there's a gain in exchange. These K-line memory-trees lose certain kinds of details, but they retain more traces of the origins of our ideas. These memory-trees might not serve quite so well if the original circumstances were exactly repeated. But that never happens, anyway \u2014 and the structured memories will be much more easily adapted to new situations.",
    "type": "article",
    "title": "8.8 societies of memories",
    "tags": [
      {
        "score": 0.7518931031227112,
        "sentiment": 0.946,
        "count": 5,
        "label": "Jack Shephard",
        "uri": "https://diffbot.com/entity/XPb-cc-yAPbK9HeP1VhSLpQ",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Person",
          "http://dbpedia.org/ontology/FictionalCharacter"
        ]
      },
      {
        "score": 0.6993591785430908,
        "sentiment": 0.743,
        "count": 6,
        "label": "Internet Relay Chat daemon",
        "uri": "https://diffbot.com/entity/XgUVWbakXPb2PAnim7w2rHA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/Software"
        ]
      },
      {
        "score": 0.6964015960693359,
        "sentiment": 0,
        "count": 2,
        "label": "society",
        "uri": "https://diffbot.com/entity/X5-r2onDFMwqrJxIepxIeQw"
      },
      {
        "score": 0.645287036895752,
        "sentiment": 0.798,
        "count": 7,
        "label": "K-Lines",
        "uri": "https://diffbot.com/entity/BW36jAuvYMT67pm-5fuJw8A",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Organisation",
          "http://dbpedia.org/ontology/Company"
        ]
      },
      {
        "score": 0.5983539819717407,
        "sentiment": 0,
        "count": 1,
        "label": "Make Believe",
        "uri": "https://diffbot.com/entity/XUP3zc1dsNjmTWrqn_qDTkA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/Work",
          "http://dbpedia.org/ontology/MusicalWork",
          "http://dbpedia.org/ontology/Album"
        ]
      }
    ],
    "docId": 119108731327,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 183790584209,
    "gburl": "http://aurellem.org/society-of-mind/som-8.8.html-diffbotxyz166129071",
    "lastCrawlTimeUTC": 1588759178,
    "timestamp": "Wed, 06 May 2020 09:59:38 GMT"
  },
  {
    "sentiment": 0.988,
    "humanLanguage": "en",
    "diffbotUri": "article|3|-630474781",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-1.1.html",
    "html": "<p>Good theories of the mind must span at least three different scales of time: slow, for the billion years in which our brains have evolved; fast, for the fleeting weeks and months of infancy and childhood; and in between, the centuries of growth of our ideas through history.</p>\n<p>To explain the mind, we have to show how minds are built from mindless stuff, from parts that are much smaller and simpler than anything we'd consider smart. Unless we can explain the mind in terms of things that have no thoughts or feelings of their own, we'll only have gone around in a circle. But what could those simpler particles be &mdash; the <em>agents</em> that compose our minds? This is the subject of our book, and knowing this, let's see our task. There are many questions to answer.</p>\n<p><strong>Function:</strong> How do agents work?<br> <strong>Embodiment:</strong> What are they made of?<br> <strong>Interaction:</strong> How do they communicate?<br> <strong>Origins:</strong> Where do the first agents come from?<br> <strong>Heredity:</strong> Are we all born with the same agents?<br> <strong>Learning:</strong> How do we make new agents and change old ones?<br> <strong>Character:</strong> What are the most important kinds of agents?<br> <strong>Authority:</strong> What happens when agents disagree?<br> <strong>Intention:</strong> How could such networks want or wish?<br> <strong>Competence:</strong> How can groups of agents do what separate agents cannot do?<br> <strong>Selfness:</strong> What gives them unity or personality?<br> <strong>Meaning:</strong> How could they understand anything?<br> <strong>Sensibility:</strong> How could they have feelings and emotions?<br> <strong>Awareness:</strong> How could they be conscious or self-aware?<br></p>\n<p>How could a theory of the mind explain so many things, when every separate question seems too hard to answer by itself? These questions all seem difficult, indeed, when we sever each one's connections to the other ones. But once we see the mind as a society of agents, each answer will illuminate the rest.</p>",
    "text": "Good theories of the mind must span at least three different scales of time: slow, for the billion years in which our brains have evolved; fast, for the fleeting weeks and months of infancy and childhood; and in between, the centuries of growth of our ideas through history.\nTo explain the mind, we have to show how minds are built from mindless stuff, from parts that are much smaller and simpler than anything we'd consider smart. Unless we can explain the mind in terms of things that have no thoughts or feelings of their own, we'll only have gone around in a circle. But what could those simpler particles be \u2014 the agents that compose our minds? This is the subject of our book, and knowing this, let's see our task. There are many questions to answer.\nFunction: How do agents work? Embodiment: What are they made of? Interaction: How do they communicate? Origins: Where do the first agents come from? Heredity: Are we all born with the same agents? Learning: How do we make new agents and change old ones? Character: What are the most important kinds of agents? Authority: What happens when agents disagree? Intention: How could such networks want or wish? Competence: How can groups of agents do what separate agents cannot do? Selfness: What gives them unity or personality? Meaning: How could they understand anything? Sensibility: How could they have feelings and emotions? Awareness: How could they be conscious or self-aware?\nHow could a theory of the mind explain so many things, when every separate question seems too hard to answer by itself? These questions all seem difficult, indeed, when we sever each one's connections to the other ones. But once we see the mind as a society of agents, each answer will illuminate the rest.",
    "type": "article",
    "title": "1.1 The agents of the mind",
    "tags": [
      {
        "score": 0.7576692700386047,
        "sentiment": 0.592,
        "count": 7,
        "label": "legal agent",
        "uri": "https://diffbot.com/entity/XCCtuC9CcPHqNAb5XVOYDpA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      },
      {
        "score": 0.7509942650794983,
        "sentiment": 0.957,
        "count": 4,
        "label": "talent agent",
        "uri": "https://diffbot.com/entity/XFpChMVxtMty13AhRN5XFaA",
        "rdfTypes": [
          "http://dbpedia.org/ontology/PersonFunction",
          "http://dbpedia.org/ontology/Profession",
          "http://dbpedia.org/ontology/Skill"
        ]
      }
    ],
    "docId": 149550088610,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 197977260445,
    "gburl": "http://aurellem.org/society-of-mind/som-1.1.html-diffbotxyz3218370834",
    "lastCrawlTimeUTC": 1588759200,
    "timestamp": "Wed, 06 May 2020 10:00:00 GMT"
  },
  {
    "humanLanguage": "en",
    "diffbotUri": "article|3|1130930411",
    "siteName": "aurellem.org",
    "pageUrl": "http://aurellem.org/society-of-mind/som-prologue.html",
    "html": "<blockquote> Everything should be made as simple as possible, but not simpler. &mdash;Albert Einstein</blockquote>\n<p>This book tries to explain how minds work. How can intelligence emerge from nonintelligence? To answer that, we'll show that you can build a mind from many little parts, each mindless by itself.</p>\n<p>I'll call <em>Society of Mind</em> this scheme in which each mind is made of many smaller processes. These we'll call agents. Each mental agent by itself can only do some simple thing that needs no mind or thought at all. Yet when we join these agents in societies &mdash; in certain very special ways &mdash; this leads to true intelligence.</p>\n<p>There's nothing very technical in this book. It, too, is a society &mdash; of many small ideas. Each by itself is only common sense, yet when we join enough of them we can explain the strangest mysteries of mind. One trouble is that these ideas have lots of cross-connections. My explanations rarely go in neat, straight lines from start to end. I wish I could have lined them up so that you could climb straight to the top, by mental stair-steps, one by one. Instead they're tied in tangled webs.</p>\n<p>Perhaps the fault is actually mine, for failing to find a tidy base of neatly ordered principles. But I'm inclined to lay the blame upon the nature of the mind: much of its power seems to stem from just the messy ways its agents cross-connect. If so, that complication can't be helped; it's only what we must expect from evolution's countless tricks.</p>\n<p>What can we do when things are hard to describe? We start by sketching out the roughest shapes to serve as scaffolds for the rest; it doesn't matter very much if some of those forms turn out partially wrong. Next, draw details to give these skeletons more lifelike flesh. Last, in the final filling-in, discard whichever first ideas no longer fit.</p>\n<p>That's what we do in real life, with puzzles that seem very hard. It's much the same for shattered pots as for the cogs of great machines. Until you've seen some of the rest, you can't make sense of any part.</p>",
    "text": "Everything should be made as simple as possible, but not simpler. \u2014Albert Einstein\nThis book tries to explain how minds work. How can intelligence emerge from nonintelligence? To answer that, we'll show that you can build a mind from many little parts, each mindless by itself.\nI'll call Society of Mind this scheme in which each mind is made of many smaller processes. These we'll call agents. Each mental agent by itself can only do some simple thing that needs no mind or thought at all. Yet when we join these agents in societies \u2014 in certain very special ways \u2014 this leads to true intelligence.\nThere's nothing very technical in this book. It, too, is a society \u2014 of many small ideas. Each by itself is only common sense, yet when we join enough of them we can explain the strangest mysteries of mind. One trouble is that these ideas have lots of cross-connections. My explanations rarely go in neat, straight lines from start to end. I wish I could have lined them up so that you could climb straight to the top, by mental stair-steps, one by one. Instead they're tied in tangled webs.\nPerhaps the fault is actually mine, for failing to find a tidy base of neatly ordered principles. But I'm inclined to lay the blame upon the nature of the mind: much of its power seems to stem from just the messy ways its agents cross-connect. If so, that complication can't be helped; it's only what we must expect from evolution's countless tricks.\nWhat can we do when things are hard to describe? We start by sketching out the roughest shapes to serve as scaffolds for the rest; it doesn't matter very much if some of those forms turn out partially wrong. Next, draw details to give these skeletons more lifelike flesh. Last, in the final filling-in, discard whichever first ideas no longer fit.\nThat's what we do in real life, with puzzles that seem very hard. It's much the same for shattered pots as for the cogs of great machines. Until you've seen some of the rest, you can't make sense of any part.",
    "type": "article",
    "title": "1 Prologue",
    "docId": 183916282294,
    "fromSeedUrl": "http://aurellem.org/society-of-mind",
    "seedUrlHash32": 3061883208,
    "parentUrlDocId": 210148704694,
    "gburl": "http://aurellem.org/society-of-mind/som-prologue.html-diffbotxyz2178800720",
    "lastCrawlTimeUTC": 1588759157,
    "timestamp": "Wed, 06 May 2020 09:59:17 GMT"
  }
]
